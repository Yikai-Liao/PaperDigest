name: Paper Recommendation Pipeline

on:
  workflow_dispatch:
    inputs:
      matrix_count:
        description: '并行处理任务数量'
        required: true
        default: 4
        type: number
  schedule:
    # 东八区(UTC+8)早8点 = UTC 0点
    - cron: '0 0 * * *'

jobs:
  prepare_data:
    name: 预测并下载 PDF 文件
    runs-on: ubuntu-latest
    outputs:
      batch_list: ${{ steps.split_dirs.outputs.batch_list }}
    steps:
      - name: 检出代码
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: requirements.txt

      - name: 安装依赖
        run: |
          pip install -r requirements.txt

      - name: Hugging Face 登录
        env:
          HUGGINGFACE_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
        run: |
          huggingface-cli login --token $HUGGINGFACE_TOKEN

      - name: 运行预测模型
        run: python script/fit_predict.py

      - name: 上传预测结果
        uses: actions/upload-artifact@v4
        with:
          name: predictions
          path: predictions.parquet
        
      - name: 下载 PDF 文件
        run: python script/download_pdf.py

      - name: 拆分 PDF 目录
        id: split_dirs
        run: |
          # 创建任务分割目录
          mkdir -p pdf_batches
          
          # 获取总 PDF 文件数
          total_files=$(find pdfs -name "*.pdf" | wc -l)
          
          # 处理 matrix_count，确保为整数
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            input_matrix_count=${{ github.event.inputs.matrix_count }}
          else
            input_matrix_count=4
          fi
          matrix_count=$(echo $input_matrix_count | awk '{print int($1)}')
          
          # 确保 matrix_count 不超过文件总数
          if [ $matrix_count -gt $total_files ]; then
            matrix_count=$total_files
          fi
          
          echo "总文件数: $total_files"
          echo "任务数量: $matrix_count"
          
          # 计算每个批次应包含的文件数
          files_per_batch=$(( (total_files + matrix_count - 1) / matrix_count ))
          echo "每个任务处理文件数: $files_per_batch"
          
          # 创建目录结构
          for i in $(seq 1 $matrix_count); do
            mkdir -p "pdf_batches/batch_$i"
          done
          
          # 将 PDF 文件分配到不同批次目录
          counter=0
          batch_num=1
          
          # 根据文件名排序，确保相同的分配
          find pdfs -name "*.pdf" | sort | while read pdf_file; do
            # 复制到相应批次目录
            cp "$pdf_file" "pdf_batches/batch_$batch_num/"
            
            counter=$((counter + 1))
            if [ $counter -ge $files_per_batch ] && [ $batch_num -lt $matrix_count ]; then
              counter=0
              batch_num=$((batch_num + 1))
            fi
          done
          
          # 统计每个批次包含的文件数
          echo "批次分配情况:"
          total_batches=0
          for i in $(seq 1 $matrix_count); do
            file_count=$(find "pdf_batches/batch_$i" -name "*.pdf" | wc -l)
            echo "batch_$i: $file_count 个文件"
            if [ $file_count -gt 0 ]; then
              total_batches=$((total_batches + 1))
            fi
          done
          
          # 生成批次列表供后续 matrix 使用
          seq 1 $total_batches | sed 's/^/batch_/' > pdf_batches/batch_list.txt
          batch_list=$(cat pdf_batches/batch_list.txt | tr '\n' ',' | sed 's/,$//')
          echo "batch_list=$batch_list" >> $GITHUB_OUTPUT

          # 创建 JSON 格式的数组
          echo -n "batch_list=[" >> $GITHUB_OUTPUT
          for i in $(seq 1 $total_batches); do
            if [ $i -eq $total_batches ]; then
              echo -n "\"batch_$i\"" >> $GITHUB_OUTPUT
            else
              echo -n "\"batch_$i\"," >> $GITHUB_OUTPUT
            fi
          done
          echo "]" >> $GITHUB_OUTPUT

          # 确保输出不为空
          if [ $total_batches -eq 0 ]; then
            echo "batch_list=[\"batch_1\"]" >> $GITHUB_OUTPUT
          fi
          
          echo "实际使用批次数: $total_batches"

      - name: 上传 PDF 批次
        uses: actions/upload-artifact@v4
        with:
          name: pdf-batches
          path: pdf_batches/
          retention-days: 1

  process_pdfs:
    name: 处理 PDF 批次 ${{ matrix.batch }}
    needs: prepare_data
    runs-on: ubuntu-latest
    strategy:
      matrix:
        batch: ${{ fromJson(needs.prepare_data.outputs.batch_list) }}
      fail-fast: false

    steps:
      - name: 检出代码
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: requirements.txt

      - name: 下载 PDF 批次
        uses: actions/download-artifact@v4
        with:
          name: pdf-batches
          path: pdf_batches

      - name: 缓存 Hugging Face 模型
        uses: actions/cache@v4
        id: huggingface-cache
        with:
          path: /home/runner/.cache/huggingface
          key: ${{ runner.os }}-huggingface-marker

      - name: 安装 marker-pdf
        run: pip install marker-pdf

      - name: 处理 PDF 文件
        run: |
          # 创建输出目录
          mkdir -p extracted_mds
          
          # 获取当前批次
          BATCH_DIR="pdf_batches/${{ matrix.batch }}"
          echo "处理批次目录: $BATCH_DIR"
          
          # 检查目录中的文件数量
          pdf_count=$(find "$BATCH_DIR" -name "*.pdf" | wc -l)
          echo "该批次包含 $pdf_count 个 PDF 文件"
          
          if [ $pdf_count -gt 0 ]; then
            # 使用 marker 处理整个目录
            marker "$BATCH_DIR" --disable_image_extraction --output_dir ./extracted_mds --workers 2
            
            # 统计处理结果
            md_count=$(find ./extracted_mds -name "*.md" | wc -l)
            echo "成功提取了 $md_count 个 Markdown 文件"
          else
            echo "该批次没有 PDF 文件，跳过处理"
          fi

      - name: 上传处理结果
        uses: actions/upload-artifact@v4
        with:
          name: extracted-${{ matrix.batch }}
          path: extracted_mds/
          retention-days: 1

      
      - name: AI 摘要
        env:
          SUMMARY_API_KEY: ${{ secrets.SUMMARY_API_KEY }}
        run: |
          pip install -r requirements.txt
          python script/summarize.py ./extracted_mds/

      - name: 上传 AI 摘要结果
        uses: actions/upload-artifact@v4
        with:
          name: ai-summary-${{ matrix.batch }}
          path: ./extracted_mds/**/*.json

  merge_results:
    name: 合并所有文件
    runs-on: ubuntu-latest
    needs: process_pdfs
    steps:
      - name: 检出代码
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: requirements.txt
        
      - name: 下载所有 artifact
        uses: actions/download-artifact@v4
        with:
          path: all_artifacts

      - name: 合并 Markdown 文件到一个文件夹
        run: |
          # 创建目标文件夹
          mkdir -p merged_mds
          
          # 找到所有 .md 文件并复制到 merged_mds/
          find all_artifacts -name "*.md" -type f -exec cp {} merged_mds/ \;
          
          # 检查合并的文件数量
          total_md=$(find merged_mds -name "*.md" | wc -l)
          echo "成功合并了 $total_md 个 Markdown 文件"

      - name: 上传合并后的 Markdown 文件
        uses: actions/upload-artifact@v4
        with:
          name: merged-markdowns
          path: merged_mds/
          retention-days: 7

      - name: 合并 JSON 文件到一个文件夹
        run: |
          # 创建目标文件夹
          mkdir -p merged_jsons
          
          # 找到所有 .json 文件并复制到 merged_jsons/
          find all_artifacts -name "*.json" -type f -exec cp {} merged_jsons/ \;
          rm -rf merged_jsons/**/*_meta.json
          
          # 检查合并的文件数量
          total_json=$(find merged_jsons -name "*.json" | wc -l)
          echo "成功合并了 $total_json 个 JSON 文件"

      - name: 下载预测结果
        uses: actions/download-artifact@v4
        with:
          name: predictions

      - name: 更新 json 元数据
        run: |
          pip install -r requirements.txt
          python script/update_metadata.py -j merged_jsons -p predictions.parquet

      - name: 上传合并后的 JSON 文件
        uses: actions/upload-artifact@v4
        with:
          name: merged-jsons
          path: merged_jsons/
          retention-days: 7

      - name: 合并 JSON 文件到 Parquet
        run: |
          python script/json2parquet.py merged_jsons main.parquet

      - name: 上传合并后的 Parquet 文件
        uses: actions/upload-artifact@v4
        with:
          name: merged-parquet
          path: main.parquet
          retention-days: 7
      
      - name: 上传 Parquet 到 Hugging Face
        env:
          HUGGINGFACE_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
        run: |
          # 上传到 Hugging Face
          python script/upload2hg.py main.parquet main.parquet lyk/PaperDigestDataBase

  trigger_build:
    name: 触发构建Workflow
    runs-on: ubuntu-latest
    needs: merge_results
    permissions: write-all
    steps:
      - name: 触发构建Workflow
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: build_page.yaml
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: main