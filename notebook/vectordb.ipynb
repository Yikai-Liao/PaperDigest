{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2cf6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8caf962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_parquet('/home/lyk/Downloads/2025.parquet').drop('conan_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3da8425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=arxiv)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = client.get_or_create_collection(name=\"arxiv\",    \n",
    "    configuration={\n",
    "        \"hnsw\": {\n",
    "            \"space\": \"cosine\", # Cohere models often use cosine space\n",
    "        },\n",
    "})\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02c4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_seperator = \"\\t\"\n",
    "\n",
    "index = df['id'].to_list()\n",
    "embedding = df['jasper_v1'].to_numpy()\n",
    "metadata = df.drop('id', 'jasper_v1','license').with_columns(\n",
    "    pl.col('authors').list.join(safe_seperator), \n",
    "    pl.col('categories').list.join(safe_seperator)\n",
    ").to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3219d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = 4096\n",
    "# for i in range(0, len(index), batch):\n",
    "#     print(f\"upsert {i} to {i+batch}\")\n",
    "#     database.upsert(\n",
    "#         ids = index[i:i+batch],\n",
    "#         embeddings = embedding[i:i+batch],\n",
    "#         metadatas = metadata[i:i+batch],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5bffdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['2504.18116'],\n",
       " 'embeddings': array([[ 0.0195315 ,  0.00181597,  0.05034558, ...,  0.03292076,\n",
       "         -0.03156114,  0.05090722]], shape=(1, 1024)),\n",
       " 'documents': None,\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'embeddings'],\n",
       " 'data': None,\n",
       " 'metadatas': [{'abstract': 'Large language models (LLMs) have demonstrated strong capabilities in programming and mathematical reasoning tasks, but are constrained by limited high-quality training data. Synthetic data can be leveraged to enhance fine-tuning outcomes, but several factors influence this process, including model size, synthetic data volume, pruning strategy, and number of fine-tuning rounds. We explore these axes and investigate which conditions enable model self-improvement. We introduce the Think, Prune, Train process, a scalable framework that iteratively fine-tunes models on their own reasoning traces, using ground-truth pruning to ensure high-quality training data. This approach yields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6% (from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B attains 91%, even surpassing GPT-4o, demonstrating the effectiveness of self-generated reasoning and systematic data selection for improving LLM capabilities.',\n",
       "   'title': 'Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models',\n",
       "   'date': '2025-04-28',\n",
       "   'created': '2025-04-25',\n",
       "   'authors': 'Caia Costello\\tSimon Guo\\tAnna Goldie\\tAzalia Mirhoseini',\n",
       "   'updated': '2025-04-28',\n",
       "   'categories': 'cs.LG'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.get('2504.18116', include = [\"metadatas\", \"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d8fabb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArXiv ID: 2502.18008\n",
      "Title: NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms\n",
      "Abstract: We introduce NotaGen, a symbolic music generation model aiming to explore the potential of producing high-quality classical sheet music. Inspired by the success of Large Language Models (LLMs), NotaGen adopts pre-training, fine-tuning, and reinforcement learning paradigms (henceforth referred to as the LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC notation, and then fine-tuned on approximately 9K high-quality classical compositions conditioned on \"period-composer-instrumentation\" prompts. For reinforcement learning, we propose the CLaMP-DPO method, which further enhances generation quality and controllability without requiring human annotations or predefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in symbolic music generation models with different architectures and encoding schemes. Furthermore, subjective A/B tests show that NotaGen outperforms baseline models against human compositions, greatly advancing musical aesthetics in symbolic music generation.\n"
     ]
    }
   ],
   "source": [
    "# def query(\n",
    "#         query_embeddings: Optional[OneOrMany[Embedding]] = None,\n",
    "#         n_results: int = 10,\n",
    "#         where: Optional[Where] = None,\n",
    "#         where_document: Optional[WhereDocument] = None,\n",
    "#         include: Include = [\"metadatas\", \"documents\",\n",
    "#                             \"distances\"]) -> QueryResult\n",
    "\n",
    "arxiv_id = '2502.18008'\n",
    "data = database.get(arxiv_id, include = [\"metadatas\", \"embeddings\"])\n",
    "emb = data['embeddings'][0]\n",
    "meta = data['metadatas'][0]\n",
    "print(f\"ArXiv ID: {arxiv_id}\")\n",
    "print(f\"Title: {meta['title']}\")\n",
    "print(f\"Abstract: {meta['abstract']}\")\n",
    "results = database.query(\n",
    "    query_embeddings=[emb],\n",
    "    n_results=20,\n",
    "    include=[\"metadatas\", \"distances\"]   \n",
    ")\n",
    "\n",
    "docs = [\n",
    "    f'Title: {results['metadatas'][0][i][\"title\"]}\\n'\n",
    "    f'Abstract: {results['metadatas'][0][i][\"abstract\"]}\\n'\n",
    "    for i in range(len(results[\"ids\"][0]))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77439d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['2502.18008',\n",
       "   '2502.10467',\n",
       "   '2502.14893',\n",
       "   '2412.16526',\n",
       "   '2410.08435',\n",
       "   '2501.08809',\n",
       "   '2504.09219',\n",
       "   '2501.17011',\n",
       "   '2505.03314',\n",
       "   '2503.19611',\n",
       "   '2503.00084',\n",
       "   '2504.13535',\n",
       "   '2309.13259',\n",
       "   '2502.13128',\n",
       "   '2504.05690',\n",
       "   '2502.04522',\n",
       "   '2504.16839',\n",
       "   '2503.08147',\n",
       "   '2408.15176',\n",
       "   '2503.17654']],\n",
       " 'embeddings': None,\n",
       " 'documents': None,\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'updated': '2025-03-24',\n",
       "    'authors': 'Yashan Wang\\tShangda Wu\\tJianhuai Hu\\tXingjian Du\\tYueqi Peng\\tYongxin Huang\\tShuai Fan\\tXiaobing Li\\tFeng Yu\\tMaosong Sun',\n",
       "    'created': '2025-03-21',\n",
       "    'title': 'NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms',\n",
       "    'categories': 'cs.SD\\tcs.AI\\teess.AS',\n",
       "    'abstract': 'We introduce NotaGen, a symbolic music generation model aiming to explore the potential of producing high-quality classical sheet music. Inspired by the success of Large Language Models (LLMs), NotaGen adopts pre-training, fine-tuning, and reinforcement learning paradigms (henceforth referred to as the LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC notation, and then fine-tuned on approximately 9K high-quality classical compositions conditioned on \"period-composer-instrumentation\" prompts. For reinforcement learning, we propose the CLaMP-DPO method, which further enhances generation quality and controllability without requiring human annotations or predefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in symbolic music generation models with different architectures and encoding schemes. Furthermore, subjective A/B tests show that NotaGen outperforms baseline models against human compositions, greatly advancing musical aesthetics in symbolic music generation.',\n",
       "    'date': '2025-03-24'},\n",
       "   {'date': '2025-02-18',\n",
       "    'authors': 'Shao-Chien Lu\\tChen-Chen Yeh\\tHui-Lin Cho\\tChun-Chieh Hsu\\tTsai-Ling Hsu\\tCheng-Han Wu\\tTimothy K. Shih\\tYu-Cheng Lin',\n",
       "    'created': '2025-02-12',\n",
       "    'abstract': \"The field of music generation using Large Language Models (LLMs) is evolving rapidly, yet existing music notation systems, such as MIDI, ABC Notation, and MusicXML, remain too complex for effective fine-tuning of LLMs. These formats are difficult for both machines and humans to interpret due to their variability and intricate structure. To address these challenges, we introduce YNote, a simplified music notation system that uses only four characters to represent a note and its pitch. YNote's fixed format ensures consistency, making it easy to read and more suitable for fine-tuning LLMs. In our experiments, we fine-tuned GPT-2 (124M) on a YNote-encoded dataset and achieved BLEU and ROUGE scores of 0.883 and 0.766, respectively. With just two notes as prompts, the model was able to generate coherent and stylistically relevant music. We believe YNote offers a practical alternative to existing music notations for machine learning applications and has the potential to significantly enhance the quality of music generation using LLMs.\",\n",
       "    'title': 'YNote: A Novel Music Notation for Fine-Tuning LLMs in Music Generation',\n",
       "    'categories': 'cs.SD\\tcs.AI\\teess.AS',\n",
       "    'updated': '2025-02-18'},\n",
       "   {'authors': 'Mingni Tang\\tJiajia Li\\tLu Yang\\tZhiqiang Zhang\\tJinghao Tian\\tZuchao Li\\tLefei Zhang\\tPing Wang',\n",
       "    'categories': 'cs.CV\\tcs.AI\\tcs.LG\\tcs.SD\\teess.AS',\n",
       "    'created': '2025-02-17',\n",
       "    'updated': '2025-02-24',\n",
       "    'abstract': 'Symbolic music is represented in two distinct forms: two-dimensional, visually intuitive score images, and one-dimensional, standardized text annotation sequences. While large language models have shown extraordinary potential in music, current research has primarily focused on unimodal symbol sequence text. Existing general-domain visual language models still lack the ability of music notation understanding. Recognizing this gap, we propose NOTA, the first large-scale comprehensive multimodal music notation dataset. It consists of 1,019,237 records, from 3 regions of the world, and contains 3 tasks. Based on the dataset, we trained NotaGPT, a music notation visual large language model. Specifically, we involve a pre-alignment training phase for cross-modal alignment between the musical notes depicted in music score images and their textual representation in ABC notation. Subsequent training phases focus on foundational music information extraction, followed by training on music notation analysis. Experimental results demonstrate that our NotaGPT-7B achieves significant improvement on music understanding, showcasing the effectiveness of NOTA and the training pipeline. Our datasets are open-sourced at https://huggingface.co/datasets/MYTH-Lab/NOTA-dataset.',\n",
       "    'date': '2025-02-24',\n",
       "    'title': 'NOTA: Multimodal Music Notation Understanding for Visual Large Language Model'},\n",
       "   {'title': 'Text2midi: Generating Symbolic Music from Captions',\n",
       "    'updated': '2025-01-03',\n",
       "    'date': '2025-01-03',\n",
       "    'abstract': 'This paper introduces text2midi, an end-to-end model to generate MIDI files from textual descriptions. Leveraging the growing popularity of multimodal generative approaches, text2midi capitalizes on the extensive availability of textual data and the success of large language models (LLMs). Our end-to-end system harnesses the power of LLMs to generate symbolic music in the form of MIDI files. Specifically, we utilize a pretrained LLM encoder to process captions, which then condition an autoregressive transformer decoder to produce MIDI sequences that accurately reflect the provided descriptions. This intuitive and user-friendly method significantly streamlines the music creation process by allowing users to generate music pieces using text prompts. We conduct comprehensive empirical evaluations, incorporating both automated and human studies, that show our model generates MIDI files of high quality that are indeed controllable by text captions that may include music theory terms such as chords, keys, and tempo. We release the code and music samples on our demo page (https://github.com/AMAAI-Lab/Text2midi) for users to interact with text2midi.',\n",
       "    'created': '2024-12-31',\n",
       "    'categories': 'cs.SD\\tcs.AI\\tcs.CL\\teess.AS',\n",
       "    'authors': 'Keshav Bhandari\\tAbhinaba Roy\\tKyra Wang\\tGeeta Puri\\tSimon Colton\\tDorien Herremans'},\n",
       "   {'title': 'Efficient Fine-Grained Guidance for Diffusion-Based Symbolic Music Generation',\n",
       "    'authors': 'Tingyu Zhu\\tHaoyu Liu\\tZiyu Wang\\tZhimin Jiang\\tZeyu Zheng',\n",
       "    'categories': 'cs.SD\\tcs.AI\\tcs.LG\\tcs.MM\\teess.AS',\n",
       "    'created': '2025-02-02',\n",
       "    'abstract': \"Developing generative models to create or conditionally create symbolic music presents unique challenges due to the combination of limited data availability and the need for high precision in note pitch. To address these challenges, we introduce an efficient Fine-Grained Guidance (FGG) approach within diffusion models. FGG guides the diffusion models to generate music that aligns more closely with the control and intent of expert composers, which is critical to improve the accuracy, listenability, and quality of generated music. This approach empowers diffusion models to excel in advanced applications such as improvisation, and interactive music creation. We derive theoretical characterizations for both the challenges in symbolic music generation and the effects of the FGG approach. We provide numerical experiments and subjective evaluation to demonstrate the effectiveness of our approach. We have published a demo page to showcase performances, as one of the first in the symbolic music literature's demo pages that enables real-time interactive generation.\",\n",
       "    'date': '2025-02-04',\n",
       "    'updated': '2025-02-04'},\n",
       "   {'title': 'XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework',\n",
       "    'authors': 'Sida Tian\\tCan Zhang\\tWei Yuan\\tWei Tan\\tWenjie Zhu',\n",
       "    'updated': '2025-01-16',\n",
       "    'date': '2025-01-16',\n",
       "    'abstract': 'In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is https://xmusic-project.github.io.',\n",
       "    'created': '2025-01-15',\n",
       "    'categories': 'cs.SD\\tcs.AI\\teess.AS'},\n",
       "   {'created': '2025-04-12',\n",
       "    'abstract': 'In recent years, text-to-audio systems have achieved remarkable success, enabling the generation of complete audio segments directly from text descriptions. While these systems also facilitate music creation, the element of human creativity and deliberate expression is often limited. In contrast, the present work allows composers, arrangers, and performers to create the basic building blocks for music creation: audio of individual musical notes for use in electronic instruments and DAWs. Through text prompts, the user can specify the timbre characteristics of the audio. We introduce a system that combines a latent diffusion model and multi-modal contrastive learning to generate musical timbres conditioned on text descriptions. By jointly generating the magnitude and phase of the spectrogram, our method eliminates the need for subsequently running a phase retrieval algorithm, as related methods do.   Audio examples, source code, and a web app are available at https://wxuanyuan.github.io/Musical-Note-Generation/',\n",
       "    'title': 'Generation of Musical Timbres using a Text-Guided Diffusion Model',\n",
       "    'authors': 'Weixuan Yuan\\tQadeer Khan\\tVladimir Golkov',\n",
       "    'updated': '2025-04-15',\n",
       "    'date': '2025-04-15',\n",
       "    'categories': 'cs.SD\\teess.AS'},\n",
       "   {'updated': '2025-02-05',\n",
       "    'date': '2025-02-05',\n",
       "    'authors': 'Philippe Pasquier\\tJeff Ens\\tNathan Fradet\\tPaul Triana\\tDavide Rizzotti\\tJean-Baptiste Rolland\\tMaryam Safi',\n",
       "    'categories': 'cs.SD\\tcs.LG\\tcs.MM\\teess.AS',\n",
       "    'abstract': 'We present and release MIDI-GPT, a generative system based on the Transformer architecture that is designed for computer-assisted music composition workflows. MIDI-GPT supports the infilling of musical material at the track and bar level, and can condition generation on attributes including: instrument type, musical style, note density, polyphony level, and note duration. In order to integrate these features, we employ an alternative representation for musical material, creating a time-ordered sequence of musical events for each track and concatenating several tracks into a single sequence, rather than using a single time-ordered sequence where the musical events corresponding to different tracks are interleaved. We also propose a variation of our representation allowing for expressiveness. We present experimental results that demonstrate that MIDI-GPT is able to consistently avoid duplicating the musical material it was trained on, generate music that is stylistically similar to the training dataset, and that attribute controls allow enforcing various constraints on the generated material. We also outline several real-world applications of MIDI-GPT, including collaborations with industry partners that explore the integration and evaluation of MIDI-GPT into commercial products, as well as several artistic works produced using it.',\n",
       "    'title': 'MIDI-GPT: A Controllable Generative Model for Computer-Assisted Multitrack Music Composition',\n",
       "    'created': '2025-02-04'},\n",
       "   {'updated': '2025-05-07',\n",
       "    'title': 'Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation',\n",
       "    'categories': 'cs.SD\\tcs.AI\\teess.AS',\n",
       "    'authors': 'Jincheng Zhang\\tGyörgy Fazekas\\tCharalampos Saitis',\n",
       "    'abstract': 'The recent surge in the popularity of diffusion models for image synthesis has attracted new attention to their potential for generation tasks in other domains. However, their applications to symbolic music generation remain largely under-explored because symbolic music is typically represented as sequences of discrete events and standard diffusion models are not well-suited for discrete data. We represent symbolic music as image-like pianorolls, facilitating the use of diffusion models for the generation of symbolic music. Moreover, this study introduces a novel diffusion model that incorporates our proposed Transformer-Mamba block and learnable wavelet transform. Classifier-free guidance is utilised to generate symbolic music with target chords. Our evaluation shows that our method achieves compelling results in terms of music quality and controllability, outperforming the strong baseline in pianoroll generation. Our code is available at https://github.com/jinchengzhanggg/proffusion.',\n",
       "    'date': '2025-05-07',\n",
       "    'created': '2025-05-06'},\n",
       "   {'created': '2025-03-25',\n",
       "    'title': 'Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation',\n",
       "    'updated': '2025-03-26',\n",
       "    'authors': 'Max W. Y. Lam\\tYijin Xing\\tWeiya You\\tJingcheng Wu\\tZongyu Yin\\tFuqiang Jiang\\tHangyu Liu\\tFeng Liu\\tXingda Li\\tWei-Tsung Lu\\tHanyu Chen\\tTong Feng\\tTianwei Zhao\\tChien-Hung Liu\\tXuchen Song\\tYang Li\\tYahui Zhou',\n",
       "    'categories': 'cs.SD\\tcs.AI\\tcs.MM\\teess.AS\\teess.SP',\n",
       "    'abstract': 'Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of \"musical thoughts\", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models.   Our samples are available at https://MusiCoT.github.io/.',\n",
       "    'date': '2025-03-26'},\n",
       "   {'date': '2025-03-04',\n",
       "    'categories': 'cs.SD\\tcs.AI\\tcs.CL\\teess.AS',\n",
       "    'abstract': 'We introduce InspireMusic, a framework integrated super resolution and large language model for high-fidelity long-form music generation. A unified framework generates high-fidelity music, songs, and audio, which incorporates an autoregressive transformer with a super-resolution flow-matching model. This framework enables the controllable generation of high-fidelity long-form music at a higher sampling rate from both text and audio prompts. Our model differs from previous approaches, as we utilize an audio tokenizer with one codebook that contains richer semantic information, thereby reducing training costs and enhancing efficiency. This combination enables us to achieve high-quality audio generation with long-form coherence of up to $8$ minutes. Then, an autoregressive transformer model based on Qwen 2.5 predicts audio tokens. Next, we employ a super-resolution flow-matching model to generate high-sampling rate audio with fine-grained details learned from an acoustic codec model. Comprehensive experiments show that the InspireMusic-1.5B-Long model has a comparable performance to recent top-tier open-source systems, including MusicGen and Stable Audio 2.0, on subjective and objective evaluations. The code and pre-trained models are released at https://github.com/FunAudioLLM/InspireMusic.',\n",
       "    'updated': '2025-03-04',\n",
       "    'created': '2025-02-28',\n",
       "    'title': 'InspireMusic: Integrating Super Resolution and Large Language Model for High-Fidelity Long-Form Music Generation',\n",
       "    'authors': 'Chong Zhang\\tYukun Ma\\tQian Chen\\tWen Wang\\tShengkui Zhao\\tZexu Pan\\tHao Wang\\tChongjia Ni\\tTrung Hieu Nguyen\\tKun Zhou\\tYidi Jiang\\tChaohong Tan\\tZhifu Gao\\tZhihao Du\\tBin Ma'},\n",
       "   {'date': '2025-04-21',\n",
       "    'updated': '2025-04-21',\n",
       "    'title': 'MusFlow: Multimodal Music Generation via Conditional Flow Matching',\n",
       "    'abstract': \"Music generation aims to create music segments that align with human aesthetics based on diverse conditional information. Despite advancements in generating music from specific textual descriptions (e.g., style, genre, instruments), the practical application is still hindered by ordinary users' limited expertise or time to write accurate prompts. To bridge this application gap, this paper introduces MusFlow, a novel multimodal music generation model using Conditional Flow Matching. We employ multiple Multi-Layer Perceptrons (MLPs) to align multimodal conditional information into the audio's CLAP embedding space. Conditional flow matching is trained to reconstruct the compressed Mel-spectrogram in the pretrained VAE latent space guided by aligned feature embedding. MusFlow can generate music from images, story texts, and music captions. To collect data for model training, inspired by multi-agent collaboration, we construct an intelligent data annotation workflow centered around a fine-tuned Qwen2-VL model. Using this workflow, we build a new multimodal music dataset, MMusSet, with each sample containing a quadruple of image, story text, music caption, and music piece. We conduct four sets of experiments: image-to-music, story-to-music, caption-to-music, and multimodal music generation. Experimental results demonstrate that MusFlow can generate high-quality music pieces whether the input conditions are unimodal or multimodal. We hope this work can advance the application of music generation in multimedia field, making music creation more accessible. Our generated samples, code and dataset are available at musflow.github.io.\",\n",
       "    'authors': 'Jiahao Song\\tYuzhao Wang',\n",
       "    'created': '2025-04-18',\n",
       "    'categories': 'cs.SD\\tcs.MM\\teess.AS'},\n",
       "   {'authors': 'Monan Zhou\\tXiaobing Li\\tFeng Yu\\tWei Li',\n",
       "    'title': 'EMelodyGen: Emotion-Conditioned Melody Generation in ABC Notation with the Musical Feature Template',\n",
       "    'categories': 'cs.IR\\tcs.AI\\tcs.SD\\teess.AS',\n",
       "    'abstract': 'The EMelodyGen system focuses on emotional melody generation in ABC notation controlled by the musical feature template. Owing to the scarcity of well-structured and emotionally labeled sheet music, we designed a template for controlling emotional melody generation by statistical correlations between musical features and emotion labels derived from small-scale emotional symbolic music datasets and music psychology conclusions. We then automatically annotated a large, well-structured sheet music collection with rough emotional labels by the template, converted them into ABC notation, and reduced label imbalance by data augmentation, resulting in a dataset named Rough4Q. Our system backbone pre-trained on Rough4Q can achieve up to 99% music21 parsing rate and melodies generated by our template can lead to a 91% alignment on emotional expressions in blind listening tests. Ablation studies further validated the effectiveness of the feature controls in the template. Available code and demos are at https://github.com/monetjoe/EMelodyGen.',\n",
       "    'updated': '2025-04-23',\n",
       "    'created': '2025-04-22',\n",
       "    'date': '2025-04-23'},\n",
       "   {'title': 'SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation',\n",
       "    'authors': 'Zihan Liu\\tShuangrui Ding\\tZhixiong Zhang\\tXiaoyi Dong\\tPan Zhang\\tYuhang Zang\\tYuhang Cao\\tDahua Lin\\tJiaqi Wang',\n",
       "    'categories': 'cs.SD\\tcs.AI',\n",
       "    'created': '2025-02-18',\n",
       "    'abstract': 'Text-to-song generation, the task of creating vocals and accompaniment from textual inputs, poses significant challenges due to domain complexity and data scarcity. Existing approaches often employ multi-stage generation procedures, resulting in cumbersome training and inference pipelines. In this paper, we propose SongGen, a fully open-source, single-stage auto-regressive transformer designed for controllable song generation. The proposed model facilitates fine-grained control over diverse musical attributes, including lyrics and textual descriptions of instrumentation, genre, mood, and timbre, while also offering an optional three-second reference clip for voice cloning. Within a unified auto-regressive framework, SongGen supports two output modes: mixed mode, which generates a mixture of vocals and accompaniment directly, and dual-track mode, which synthesizes them separately for greater flexibility in downstream applications. We explore diverse token pattern strategies for each mode, leading to notable improvements and valuable insights. Furthermore, we design an automated data preprocessing pipeline with effective quality control. To foster community engagement and future research, we will release our model weights, training code, annotated data, and preprocessing pipeline. The generated samples are showcased on our project page at https://liuzh-19.github.io/SongGen/ , and the code will be available at https://github.com/LiuZH-19/SongGen .',\n",
       "    'updated': '2025-02-19',\n",
       "    'date': '2025-02-19'},\n",
       "   {'date': '2025-04-10',\n",
       "    'created': '2025-04-09',\n",
       "    'categories': 'cs.SD\\teess.AS',\n",
       "    'abstract': \"Recent advances in generative models have made it possible to create high-quality, coherent music, with some systems delivering production-level output. Yet, most existing models focus solely on generating music from scratch, limiting their usefulness for musicians who want to integrate such models into a human, iterative composition workflow. In this paper we introduce STAGE, our STemmed Accompaniment GEneration model, fine-tuned from the state-of-the-art MusicGen to generate single-stem instrumental accompaniments conditioned on a given mixture. Inspired by instruction-tuning methods for language models, we extend the transformer's embedding matrix with a context token, enabling the model to attend to a musical context through prefix-based conditioning. Compared to the baselines, STAGE yields accompaniments that exhibit stronger coherence with the input mixture, higher audio quality, and closer alignment with textual prompts. Moreover, by conditioning on a metronome-like track, our framework naturally supports tempo-constrained generation, achieving state-of-the-art alignment with the target rhythmic structure--all without requiring any additional tempo-specific module. As a result, STAGE offers a practical, versatile tool for interactive music creation that can be readily adopted by musicians in real-world workflows.\",\n",
       "    'updated': '2025-04-10',\n",
       "    'title': 'STAGE: Stemmed Accompaniment Generation through Prefix-Based Conditioning',\n",
       "    'authors': 'Giorgio Strano\\tChiara Ballanti\\tDonato Crisostomi\\tMichele Mancusi\\tLuca Cosmo\\tEmanuele Rodolà'},\n",
       "   {'categories': 'cs.SD\\tcs.AI\\teess.AS',\n",
       "    'authors': 'Keshav Bhandari\\tSungkyun Chang\\tTongyu Lu\\tFareza R. Enus\\tLouis B. Bradshaw\\tDorien Herremans\\tSimon Colton',\n",
       "    'date': '2025-02-10',\n",
       "    'abstract': \"Deep learning has enabled remarkable advances in style transfer across various domains, offering new possibilities for creative content generation. However, in the realm of symbolic music, generating controllable and expressive performance-level style transfers for complete musical works remains challenging due to limited datasets, especially for genres such as jazz, and the lack of unified models that can handle multiple music generation tasks. This paper presents ImprovNet, a transformer-based architecture that generates expressive and controllable musical improvisations through a self-supervised corruption-refinement training strategy. ImprovNet unifies multiple capabilities within a single model: it can perform cross-genre and intra-genre improvisations, harmonize melodies with genre-specific styles, and execute short prompt continuation and infilling tasks. The model's iterative generation framework allows users to control the degree of style transfer and structural similarity to the original composition. Objective and subjective evaluations demonstrate ImprovNet's effectiveness in generating musically coherent improvisations while maintaining structural relationships with the original pieces. The model outperforms Anticipatory Music Transformer in short continuation and infilling tasks and successfully achieves recognizable genre conversion, with 79\\\\% of participants correctly identifying jazz-style improvisations. Our code and demo page can be found at https://github.com/keshavbhandari/improvnet.\",\n",
       "    'title': 'ImprovNet: Generating Controllable Musical Improvisations with Iterative Corruption Refinement',\n",
       "    'updated': '2025-02-10',\n",
       "    'created': '2025-02-06'},\n",
       "   {'created': '2025-04-23',\n",
       "    'updated': '2025-04-24',\n",
       "    'title': 'SMART: Tuning a symbolic music generation system with an audio domain aesthetic reward',\n",
       "    'date': '2025-04-24',\n",
       "    'categories': 'cs.SD',\n",
       "    'abstract': 'Recent work has proposed training machine learning models to predict aesthetic ratings for music audio. Our work explores whether such models can be used to finetune a symbolic music generation system with reinforcement learning, and what effect this has on the system outputs. To test this, we use group relative policy optimization to finetune a piano MIDI model with Meta Audiobox Aesthetics ratings of audio-rendered outputs as the reward. We find that this optimization has effects on multiple low-level features of the generated outputs, and improves the average subjective ratings in a preliminary listening study with $14$ participants. We also find that over-optimization dramatically reduces diversity of model outputs.',\n",
       "    'authors': 'Nicolas Jonason\\tLuca Casini\\tBob L. T. Sturm'},\n",
       "   {'authors': 'Zhifeng Xie\\tQile He\\tYoujia Zhu\\tQiwei He\\tMengtian Li',\n",
       "    'updated': '2025-03-12',\n",
       "    'date': '2025-03-12',\n",
       "    'created': '2025-03-11',\n",
       "    'categories': 'cs.CV\\tcs.MM\\tcs.SD\\teess.AS',\n",
       "    'abstract': 'In this work, we implement music production for silent film clips using LLM-driven method. Given the strong professional demands of film music production, we propose the FilmComposer, simulating the actual workflows of professional musicians. FilmComposer is the first to combine large generative models with a multi-agent approach, leveraging the advantages of both waveform music and symbolic music generation. Additionally, FilmComposer is the first to focus on the three core elements of music production for film-audio quality, musicality, and musical development-and introduces various controls, such as rhythm, semantics, and visuals, to enhance these key aspects. Specifically, FilmComposer consists of the visual processing module, rhythm-controllable MusicGen, and multi-agent assessment, arrangement and mix. In addition, our framework can seamlessly integrate into the actual music production pipeline and allows user intervention in every step, providing strong interactivity and a high degree of creative freedom. Furthermore, we propose MusicPro-7k which includes 7,418 film clips, music, description, rhythm spots and main melody, considering the lack of a professional and high-quality film music dataset. Finally, both the standard metrics and the new specialized metrics we propose demonstrate that the music generated by our model achieves state-of-the-art performance in terms of quality, consistency with video, diversity, musicality, and musical development. Project page: https://apple-jun.github.io/FilmComposer.github.io/',\n",
       "    'title': 'FilmComposer: LLM-Driven Music Production for Silent Film Clips'},\n",
       "   {'categories': 'cs.SD\\tcs.CL\\teess.AS',\n",
       "    'title': 'Unifying Multitrack Music Arrangement via Reconstruction Fine-Tuning and Efficient Tokenization',\n",
       "    'created': '2025-03-06',\n",
       "    'updated': '2025-03-07',\n",
       "    'authors': 'Longshen Ou\\tJingwei Zhao\\tZiyu Wang\\tGus Xia\\tYe Wang',\n",
       "    'date': '2025-03-07',\n",
       "    'abstract': 'Automatic music arrangement streamlines the creation of musical variants for composers and arrangers, reducing reliance on extensive music expertise. However, existing methods suffer from inefficient tokenization, underutilization of pre-trained music language models (LMs), and suboptimal fidelity and coherence in generated arrangements. This paper introduces an efficient multitrack music tokenizer for unconditional and conditional symbolic music generation, along with a unified sequence-to-sequence reconstruction fine-tuning objective for pre-trained music LMs that balances task-specific needs with coherence constraints. Our approach achieves state-of-the-art results on band arrangement, piano reduction, and drum arrangement, surpassing task-specific models in both objective metrics and perceptual quality. Additionally, we demonstrate that generative pretraining significantly contributes to the performance across these arrangement tasks, especially when handling long segments with complex alignment.'},\n",
       "   {'abstract': 'Recent advances in symbolic music generation primarily rely on deep learning models such as Transformers, GANs, and diffusion models. While these approaches achieve high-quality results, they require substantial computational resources, limiting their scalability. We introduce LZMidi, a lightweight symbolic music generation framework based on a Lempel-Ziv (LZ78)-induced sequential probability assignment (SPA). By leveraging the discrete and sequential structure of MIDI data, our approach enables efficient music generation on standard CPUs with minimal training and inference costs. Theoretically, we establish universal convergence guarantees for our approach, underscoring its reliability and robustness. Compared to state-of-the-art diffusion models, LZMidi achieves competitive Frechet Audio Distance (FAD), Wasserstein Distance (WD), and Kullback-Leibler (KL) scores, while significantly reducing computational overhead - up to 30x faster training and 300x faster generation. Our results position LZMidi as a significant advancement in compression-based learning, highlighting how universal compression techniques can efficiently model and generate structured sequential data, such as symbolic music, with practical scalability and theoretical rigor.',\n",
       "    'categories': 'cs.SD\\tcs.IT\\tmath.IT',\n",
       "    'created': '2025-03-22',\n",
       "    'title': 'LZMidi: Compression-Based Symbolic Music Generation',\n",
       "    'date': '2025-03-25',\n",
       "    'updated': '2025-03-25',\n",
       "    'authors': 'Connor Ding\\tAbhiram Gorle\\tSagnik Bhattacharya\\tDivija Hasteer\\tNaomi Sagan\\tTsachy Weissman'}]],\n",
       " 'distances': [[0.0,\n",
       "   0.3128470778465271,\n",
       "   0.3215706944465637,\n",
       "   0.3800410330295563,\n",
       "   0.3926222026348114,\n",
       "   0.4246169328689575,\n",
       "   0.4391179084777832,\n",
       "   0.4395812153816223,\n",
       "   0.44093114137649536,\n",
       "   0.4409639537334442,\n",
       "   0.464139461517334,\n",
       "   0.465199738740921,\n",
       "   0.4707429111003876,\n",
       "   0.47274842858314514,\n",
       "   0.47296637296676636,\n",
       "   0.4732849597930908,\n",
       "   0.48297762870788574,\n",
       "   0.48351192474365234,\n",
       "   0.483794629573822,\n",
       "   0.48445940017700195]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a752132e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from FlagEmbedding import FlagReranker,FlagLLMReranker\n",
    "from FlagEmbedding import LightWeightFlagLLMReranker\n",
    "reranker = LightWeightFlagLLMReranker('BAAI/bge-reranker-v2.5-gemma2-lightweight', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "97bcc160",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = f\"Which paper is most related to the this one?\\n{docs[0]}\"\n",
    "keys = docs[1:]\n",
    "scorse = reranker.compute_score([[q, k] for k in keys], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e8d7cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.1232107581442072\n",
      "Document: Title: XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework\n",
      "Abstract: In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is https://xmusic-project.github.io.\n",
      "\n",
      "\n",
      "Score: 0.045600519994519494\n",
      "Document: Title: Text2midi: Generating Symbolic Music from Captions\n",
      "Abstract: This paper introduces text2midi, an end-to-end model to generate MIDI files from textual descriptions. Leveraging the growing popularity of multimodal generative approaches, text2midi capitalizes on the extensive availability of textual data and the success of large language models (LLMs). Our end-to-end system harnesses the power of LLMs to generate symbolic music in the form of MIDI files. Specifically, we utilize a pretrained LLM encoder to process captions, which then condition an autoregressive transformer decoder to produce MIDI sequences that accurately reflect the provided descriptions. This intuitive and user-friendly method significantly streamlines the music creation process by allowing users to generate music pieces using text prompts. We conduct comprehensive empirical evaluations, incorporating both automated and human studies, that show our model generates MIDI files of high quality that are indeed controllable by text captions that may include music theory terms such as chords, keys, and tempo. We release the code and music samples on our demo page (https://github.com/AMAAI-Lab/Text2midi) for users to interact with text2midi.\n",
      "\n",
      "\n",
      "Score: 0.02275328629521134\n",
      "Document: Title: Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation\n",
      "Abstract: The recent surge in the popularity of diffusion models for image synthesis has attracted new attention to their potential for generation tasks in other domains. However, their applications to symbolic music generation remain largely under-explored because symbolic music is typically represented as sequences of discrete events and standard diffusion models are not well-suited for discrete data. We represent symbolic music as image-like pianorolls, facilitating the use of diffusion models for the generation of symbolic music. Moreover, this study introduces a novel diffusion model that incorporates our proposed Transformer-Mamba block and learnable wavelet transform. Classifier-free guidance is utilised to generate symbolic music with target chords. Our evaluation shows that our method achieves compelling results in terms of music quality and controllability, outperforming the strong baseline in pianoroll generation. Our code is available at https://github.com/jinchengzhanggg/proffusion.\n",
      "\n",
      "\n",
      "Score: 0.01315356645979164\n",
      "Document: Title: Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation\n",
      "Abstract: Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of \"musical thoughts\", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models.   Our samples are available at https://MusiCoT.github.io/.\n",
      "\n",
      "\n",
      "Score: 0.011026510901414936\n",
      "Document: Title: Efficient Fine-Grained Guidance for Diffusion-Based Symbolic Music Generation\n",
      "Abstract: Developing generative models to create or conditionally create symbolic music presents unique challenges due to the combination of limited data availability and the need for high precision in note pitch. To address these challenges, we introduce an efficient Fine-Grained Guidance (FGG) approach within diffusion models. FGG guides the diffusion models to generate music that aligns more closely with the control and intent of expert composers, which is critical to improve the accuracy, listenability, and quality of generated music. This approach empowers diffusion models to excel in advanced applications such as improvisation, and interactive music creation. We derive theoretical characterizations for both the challenges in symbolic music generation and the effects of the FGG approach. We provide numerical experiments and subjective evaluation to demonstrate the effectiveness of our approach. We have published a demo page to showcase performances, as one of the first in the symbolic music literature's demo pages that enables real-time interactive generation.\n",
      "\n",
      "\n",
      "Score: 0.005598965377631223\n",
      "Document: Title: NOTA: Multimodal Music Notation Understanding for Visual Large Language Model\n",
      "Abstract: Symbolic music is represented in two distinct forms: two-dimensional, visually intuitive score images, and one-dimensional, standardized text annotation sequences. While large language models have shown extraordinary potential in music, current research has primarily focused on unimodal symbol sequence text. Existing general-domain visual language models still lack the ability of music notation understanding. Recognizing this gap, we propose NOTA, the first large-scale comprehensive multimodal music notation dataset. It consists of 1,019,237 records, from 3 regions of the world, and contains 3 tasks. Based on the dataset, we trained NotaGPT, a music notation visual large language model. Specifically, we involve a pre-alignment training phase for cross-modal alignment between the musical notes depicted in music score images and their textual representation in ABC notation. Subsequent training phases focus on foundational music information extraction, followed by training on music notation analysis. Experimental results demonstrate that our NotaGPT-7B achieves significant improvement on music understanding, showcasing the effectiveness of NOTA and the training pipeline. Our datasets are open-sourced at https://huggingface.co/datasets/MYTH-Lab/NOTA-dataset.\n",
      "\n",
      "\n",
      "Score: 0.005296076432254571\n",
      "Document: Title: YNote: A Novel Music Notation for Fine-Tuning LLMs in Music Generation\n",
      "Abstract: The field of music generation using Large Language Models (LLMs) is evolving rapidly, yet existing music notation systems, such as MIDI, ABC Notation, and MusicXML, remain too complex for effective fine-tuning of LLMs. These formats are difficult for both machines and humans to interpret due to their variability and intricate structure. To address these challenges, we introduce YNote, a simplified music notation system that uses only four characters to represent a note and its pitch. YNote's fixed format ensures consistency, making it easy to read and more suitable for fine-tuning LLMs. In our experiments, we fine-tuned GPT-2 (124M) on a YNote-encoded dataset and achieved BLEU and ROUGE scores of 0.883 and 0.766, respectively. With just two notes as prompts, the model was able to generate coherent and stylistically relevant music. We believe YNote offers a practical alternative to existing music notations for machine learning applications and has the potential to significantly enhance the quality of music generation using LLMs.\n",
      "\n",
      "\n",
      "Score: 0.004022734168410528\n",
      "Document: Title: MIDI-GPT: A Controllable Generative Model for Computer-Assisted Multitrack Music Composition\n",
      "Abstract: We present and release MIDI-GPT, a generative system based on the Transformer architecture that is designed for computer-assisted music composition workflows. MIDI-GPT supports the infilling of musical material at the track and bar level, and can condition generation on attributes including: instrument type, musical style, note density, polyphony level, and note duration. In order to integrate these features, we employ an alternative representation for musical material, creating a time-ordered sequence of musical events for each track and concatenating several tracks into a single sequence, rather than using a single time-ordered sequence where the musical events corresponding to different tracks are interleaved. We also propose a variation of our representation allowing for expressiveness. We present experimental results that demonstrate that MIDI-GPT is able to consistently avoid duplicating the musical material it was trained on, generate music that is stylistically similar to the training dataset, and that attribute controls allow enforcing various constraints on the generated material. We also outline several real-world applications of MIDI-GPT, including collaborations with industry partners that explore the integration and evaluation of MIDI-GPT into commercial products, as well as several artistic works produced using it.\n",
      "\n",
      "\n",
      "Score: 0.0035258384022085793\n",
      "Document: Title: Generation of Musical Timbres using a Text-Guided Diffusion Model\n",
      "Abstract: In recent years, text-to-audio systems have achieved remarkable success, enabling the generation of complete audio segments directly from text descriptions. While these systems also facilitate music creation, the element of human creativity and deliberate expression is often limited. In contrast, the present work allows composers, arrangers, and performers to create the basic building blocks for music creation: audio of individual musical notes for use in electronic instruments and DAWs. Through text prompts, the user can specify the timbre characteristics of the audio. We introduce a system that combines a latent diffusion model and multi-modal contrastive learning to generate musical timbres conditioned on text descriptions. By jointly generating the magnitude and phase of the spectrogram, our method eliminates the need for subsequently running a phase retrieval algorithm, as related methods do.   Audio examples, source code, and a web app are available at https://wxuanyuan.github.io/Musical-Note-Generation/\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = [{'score': score, 'doc': doc} for score, doc in zip(scorse, keys)]\n",
    "results = sorted(results, key=lambda x: x['score'], reverse=True)\n",
    "for result in results:\n",
    "    print(f\"Score: {result['score']}\")\n",
    "    print(f\"Document: {result['doc']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d8165dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.650123596191406]\n",
      "[0.0035047555373109926]\n",
      "[-8.18381404876709, 5.265047073364258]\n",
      "[0.00027905737696341354, 0.9948574330052771]\n"
     ]
    }
   ],
   "source": [
    "score = reranker.compute_score(['query', 'passage'])\n",
    "print(score) # -5.65234375\n",
    "\n",
    "# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\n",
    "score = reranker.compute_score(['query', 'passage'], normalize=True)\n",
    "print(score) # 0.003497010252573502\n",
    "\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\n",
    "print(scores) # [-8.1875, 5.26171875]\n",
    "\n",
    "# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], normalize=True)\n",
    "print(scores) # [0.00027803096387751553, 0.9948403768236574]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
