"""
ç¤ºä¾‹ï¼šåœ¨æ‘˜è¦ç”Ÿæˆæµç¨‹ä¸­ä½¿ç”¨ pdf_extractor

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•å°† pdf_extractor é›†æˆåˆ°ç°æœ‰çš„æ‘˜è¦ç”Ÿæˆæµç¨‹ä¸­
"""
from pathlib import Path
from loguru import logger
import polars as pl

# å‡è®¾æˆ‘ä»¬æœ‰æ¨èçš„è®ºæ–‡æ•°æ®
def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®æ¨¡æ‹Ÿæ¨èç»“æœ"""
    return pl.DataFrame({
        'id': ['2509.04027', '2509.18405', '2509.20138'],
        'title': [
            'CoT-Space: A Theoretical Framework',
            'Another Paper Title',
            'Third Paper Title'
        ],
        'show': [1, 1, 1]
    })


def main():
    """ä¸»æµç¨‹ï¼šä» PDF/LaTeX æå–åˆ°æ‘˜è¦ç”Ÿæˆ"""
    
    # æ­¥éª¤ 1: è·å–æ¨èçš„è®ºæ–‡
    logger.info("Step 1: Load recommended papers")
    recommended_df = create_sample_data()
    logger.info(f"  Found {len(recommended_df)} papers to process")
    
    # æ­¥éª¤ 2: æå–è®ºæ–‡å†…å®¹ï¼ˆä¼˜å…ˆ LaTeXï¼Œå¤±è´¥æ—¶ç”¨ PDFï¼‰
    logger.info("\nStep 2: Extract paper content")
    logger.info("  Using LaTeX extraction (fast) with PDF fallback (slow)")
    
    from pdf_extractor import PaperExtractor
    
    extractor = PaperExtractor(
        latex_dir=Path("arxiv/latex"),
        json_dir=Path("arxiv/json"),
        markdown_dir=Path("arxiv/markdown"),
        pdf_dir=Path("pdfs")
    )
    
    arxiv_ids = recommended_df['id'].to_list()
    markdowns = extractor.extract_batch(arxiv_ids)
    
    logger.success(f"  âœ“ Successfully extracted {len(markdowns)}/{len(arxiv_ids)} papers")
    
    # æ­¥éª¤ 3: ç”Ÿæˆæ‘˜è¦ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œä¸å®é™…è°ƒç”¨ APIï¼‰
    logger.info("\nStep 3: Generate summaries")
    logger.info("  (Skipped in this demo - would call OpenAI/Gemini API)")
    
    summaries = []
    for arxiv_id, markdown_content in markdowns.items():
        logger.info(f"  - {arxiv_id}: {len(markdown_content)} chars")
        
        # å®é™…ä½¿ç”¨æ—¶ï¼Œè¿™é‡Œä¼šè°ƒç”¨æ‘˜è¦ API
        # summary = summarize_with_api(markdown_content, ...)
        # summaries.append(summary)
    
    # æ­¥éª¤ 4: ä¿å­˜ç»“æœ
    logger.info("\nStep 4: Save results")
    logger.info("  (Skipped in this demo - would save to JSON/database)")
    
    # æ­¥éª¤ 5: æ€»ç»“ç»Ÿè®¡
    logger.info("\n" + "=" * 80)
    logger.info("Processing Summary")
    logger.info("=" * 80)
    
    total_requested = len(arxiv_ids)
    total_extracted = len(markdowns)
    extraction_rate = (total_extracted / total_requested) * 100
    
    logger.info(f"Requested papers: {total_requested}")
    logger.info(f"Successfully extracted: {total_extracted}")
    logger.info(f"Extraction rate: {extraction_rate:.1f}%")
    
    if total_extracted < total_requested:
        failed_ids = set(arxiv_ids) - set(markdowns.keys())
        logger.warning(f"Failed papers: {failed_ids}")
        logger.info("Tip: Check network connection and PDF availability")
    
    logger.info("\nğŸ’¡ Performance Tips:")
    logger.info("  1. LaTeX extraction is 100x faster than PDF")
    logger.info("  2. Enable caching to avoid reprocessing")
    logger.info("  3. Use batch processing for multiple papers")
    logger.info("  4. Consider running PDF fallback in parallel if needed")


if __name__ == "__main__":
    main()
