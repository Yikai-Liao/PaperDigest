import os
import sys
import sklearn.linear_model
import toml
from pathlib import Path
from loguru import logger
from sklearn.neighbors import NearestNeighbors

import sklearn
import polars as pl
import numpy as np
import time
import datetime

REPO_ROOT = Path(__file__).resolve().parents[1]

def log_info():
    logger.debug(f"CPU core num: {os.cpu_count()} | Memory: {os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES') / (1024. ** 3)} GB")
    logger.debug(f"Python version: {sys.version}")
    logger.debug(f"Scikit-learn version: {sklearn.__version__}")
    logger.debug(f"Polars version: {pl.__version__}")
    logger.debug(f"NumPy version: {np.__version__}")
    logger.debug(f"Current working directory: {os.getcwd()}")
    logger.debug(f"Repository root: {REPO_ROOT}")
    logger.debug(f"Script path: {Path(__file__).resolve()}")

def load_config():
    config_path = REPO_ROOT / "config.toml"
    if not config_path.exists():
        logger.error(f"Config file not found: {config_path}")
        sys.exit(1)
    
    with open(config_path, 'r') as f:
        config = toml.load(f)
        content_repo = config.get("content_repo", "lyk/ArxivEmbedding")
        config = config["fit_predict"]
        config["content_repo"] = content_repo
    
    logger.info(f"Config parameters:")
    for key, value in config.items():
        logger.info(f"- {key}: {value}")
    logger.info("Configuration loaded successfully.")
    return config

def load_preference(config):
    preference_dir = REPO_ROOT / config.get("preference_dir", "preference")
    if not preference_dir.exists():
        logger.error(f"Preference directory not found: {preference_dir}")
        sys.exit(1)

    preferences = [ pl.read_csv(p, columns=['id', 'preference'], schema={'id': pl.String, 'preference': pl.String}) for p in preference_dir.glob("**/*.csv") ]
    if not preferences:
        logger.error(f"No preference files found in {preference_dir}")
        sys.exit(1)
    # Combine all DataFrames into one
    preferences = pl.concat(preferences, how="vertical").unique(subset="id")
    logger.info(f"{len(preferences)} preference items loaded from {preference_dir}")
    return preferences

def load_recommended(config) -> pl.DataFrame:
    """
    Raw data dir contains lots of json files like 2407.0288.json
    We need to extract the Arxiv ID from the filename
    and collect all the IDs into a pl.DataFrame
    """
    # raw_dir = REPO_ROOT / config.get("raw_data_dir", "raw")
    # if not raw_dir.exists():
        # return pl.DataFrame(columns=["id"], schema={"id": pl.String})
    content_repo = config["content_repo"]
    recommended = pl.scan_parquet(f"hf://datasets/{content_repo}/main.parquet").collect()
    return recommended


def load_lazy_dataset(config, preferences: pl.DataFrame):
    categories = config.get("categories", ("cs.AI",))
    logger.debug(f"Categories: {categories}")

    dataset_name = config.get("embedding_dataset", "lyk/PaperDigestDataBase")
    current_year = time.localtime().tm_year
    backgroud_start_year = config.get("background_start_year", 2024)
    preference_start_year = config.get("preference_start_year", 2023)
    start_year = max(2017, min(current_year, backgroud_start_year, preference_start_year))
    logger.debug(f"Start year: {start_year}, Current year: {current_year}")
    parquet_paths = [
        f"hf://datasets/{dataset_name}/{year}.parquet"
        for year in range(start_year, current_year + 1)
    ]
    logger.debug(f"Parquet paths: {parquet_paths}")

    # lazy load the dataset from huggingface
    try:
        df = pl.scan_parquet(parquet_paths, missing_columns="insert")
    except Exception as e:
        logger.error(f"Error loading dataset: {e}")
        logger.warning("Attempting to load without the last year...")
        df = pl.scan_parquet(parquet_paths[:-1], allow_missing_columns=True)
    logger.debug(f"Dataset loaded: {df}")

    # construct filter condition
    filter_condition = pl.col("categories").list.contains(pl.lit(categories[0]))
    for category in categories[1:]:
        filter_condition = filter_condition | pl.col("categories").list.contains(pl.lit(category))
    logger.debug(f"Filter condition: {filter_condition}")

    # filter and collect the dataset
    lazy_df = df.filter(filter_condition)
    
    preference_ids = preferences.select("id").to_series().to_list()
    indicator_col_name = "__is_preferred__" # Using a distinct name
    database = lazy_df.with_columns(
        pl.col("id").is_in(preference_ids).alias(indicator_col_name)
    )

    logger.info(f"Splitting collected data based on '{indicator_col_name}' column...")

    prefered_df = database.filter(pl.col(indicator_col_name))
    remaining_df = database.filter(~pl.col(indicator_col_name))

    preferences = preferences.lazy()

    prefered_df = prefered_df.join(
        preferences, on="id", how="inner"
    ).drop(indicator_col_name)
        
    remaining_df = remaining_df.drop(indicator_col_name)

    if start_year < backgroud_start_year:
        # year is stored in updated column, like 2023-10-01 in string format
        logger.debug(f"Filtering out rows in remaining_df whose year is less than {backgroud_start_year}...")
        remaining_df = remaining_df\
            .with_columns(pl.col("updated").str.slice(0, 4).cast(pl.Int32).alias("year"))\
            .filter(pl.col("year") >= backgroud_start_year)

    # ğŸ”§ ä¿®å¤: è¿‡æ»¤æ‰ embedding æœ‰é—®é¢˜çš„è¡Œ
    # æ ¹æ®è¯Šæ–­å‘ç°:
    # 1. jasper_v1 åˆ—çš„å‘é‡å†…éƒ¨åŒ…å« NaN å€¼ (çº¦30% çš„å‘é‡å…¨æ˜¯ NaN)
    # 2. è¿™ä¸æ˜¯ null å€¼é—®é¢˜,è€Œæ˜¯å‘é‡å†…å®¹æœ¬èº«å°±æ˜¯ [NaN, NaN, ...]
    # 3. éœ€è¦æ£€æŸ¥å‘é‡å†…éƒ¨æ˜¯å¦åŒ…å« NaN,ä¸åªæ˜¯æ£€æŸ¥æ˜¯å¦ä¸º null
    embedding_columns = config.get("embedding_columns", ["jasper_v1", "conan_v1"])
    
    logger.info("å¼€å§‹è¿‡æ»¤å« NaN çš„ embedding å‘é‡...")
    
    # è®¡ç®—è¿‡æ»¤å‰çš„æ•°é‡
    prefered_count_before = prefered_df.select(pl.count()).collect()[0, 0]
    remaining_count_before = remaining_df.select(pl.count()).collect()[0, 0]
    
    # æ–¹æ³•: ä½¿ç”¨ polars çš„ list.eval æ£€æŸ¥å‘é‡å†…éƒ¨æ˜¯å¦æœ‰ NaN
    # ä½†ç”±äº polars æ— æ³•ç›´æ¥æ£€æŸ¥ list å†…éƒ¨çš„ NaN,æˆ‘ä»¬éœ€è¦åœ¨æ”¶é›†æ—¶è¿‡æ»¤
    # è¿™é‡Œå…ˆè¿‡æ»¤ null å€¼ä½œä¸ºåˆæ­¥æ¸…æ´—
    null_filter = pl.col(embedding_columns[0]).is_not_null()
    for col in embedding_columns[1:]:
        null_filter = null_filter & pl.col(col).is_not_null()
    
    prefered_df = prefered_df.filter(null_filter)
    remaining_df = remaining_df.filter(null_filter)
    
    # è®¡ç®—è¿‡æ»¤åçš„æ•°é‡
    prefered_count_after = prefered_df.select(pl.count()).collect()[0, 0]
    remaining_count_after = remaining_df.select(pl.count()).collect()[0, 0]
    
    # è®°å½•è¿‡æ»¤ç»“æœ
    prefered_removed = prefered_count_before - prefered_count_after
    remaining_removed = remaining_count_before - remaining_count_after
    
    if prefered_removed > 0 or remaining_removed > 0:
        logger.warning(f"è¿‡æ»¤äº† embedding ä¸º null çš„æ ·æœ¬:")
        logger.warning(f"  prefered: ç§»é™¤ {prefered_removed}/{prefered_count_before} ({prefered_removed/prefered_count_before*100:.2f}%)")
        logger.warning(f"  remaining: ç§»é™¤ {remaining_removed}/{remaining_count_before} ({remaining_removed/remaining_count_before*100:.2f}%)")
    else:
        logger.info("âœ… æ²¡æœ‰ embedding ä¸º null çš„æ ·æœ¬")
    
    logger.warning("âš ï¸  æ³¨æ„: jasper_v1 å‘é‡å†…éƒ¨å¯èƒ½ä»åŒ…å« NaN å€¼ (çº¦30%)")
    logger.warning("    è¿™äº› NaN ä¼šåœ¨è®­ç»ƒæ—¶è¢«æ›¿æ¢ä¸º 0")
    logger.warning("    å»ºè®®è”ç³»æ•°æ®å›¢é˜Ÿä¿®å¤ jasper_v1 çš„ embedding ç”Ÿæˆé—®é¢˜")

    return prefered_df, remaining_df



def show_df_size(df: pl.DataFrame, name: str):
    # Row count and size in MB
    row_count = df.height
    size_mb = df.estimated_size() / (1024 ** 2)
    logger.debug(f"{name} - Rows: {row_count}, Size: {size_mb:.2f} MB")

def remove_recommended(remaining_df: pl.LazyFrame, recommended_df: pl.DataFrame):
    # Remove recommended items from remaining_df
    recommended_ids = recommended_df.select("id").to_series().to_list()
    remaining_df = remaining_df.filter(~pl.col("id").is_in(recommended_ids))
    return remaining_df

"""
Training Part
"""

def confidence_weighted_sampling(
    x, model, 
    high_conf_threshold: float = 0.9, 
    high_conf_weight: float = 2.0, 
    random_state: int = 42
) -> pl.DataFrame:
    n_samples = x.shape[0]
    confidences = model.predict_proba(x)[:, 1]
    
    # set weights based on confidence
    weights = np.ones_like(confidences)
    high_conf_indices = np.where(confidences >= high_conf_threshold)[0]
    weights[high_conf_indices] = high_conf_weight
    
    # log the number of high confidence samples
    n_high_conf = len(high_conf_indices)
    logger.info(f"å‘ç°{n_high_conf}ä¸ªé«˜ç½®ä¿¡åº¦æ ·æœ¬ (ç½®ä¿¡åº¦ >= {high_conf_threshold})ã€‚" + 
                f"æƒé‡è®¾ç½®ï¼šé«˜ç½®ä¿¡åº¦æ ·æœ¬={high_conf_weight}ï¼Œå…¶ä»–=1.0")
    # Normalize weights to sum to 1
    sampling_probs = weights / weights.sum()
    # Sample with replacement based on the weights
    np.random.seed(random_state)
    sampled_indices = np.random.choice(
        np.arange(n_samples), size=n_samples, replace=True, p=sampling_probs
    )
    sampled_x = x[sampled_indices]

    return sampled_x

def adaptive_difficulty_sampling(
    x_pos, unlabeled_data, 
    n_neighbors: int = 5,
    sampling_ratio: float = 1.0,
    random_state: int = 42,
    synthetic_ratio: float = 0.5,  # æ§åˆ¶åˆæˆæ•°æ®çš„æ¯”ä¾‹
    k_smote: int = 5  # SMOTEä¸­ç”¨äºç”Ÿæˆåˆæˆæ ·æœ¬çš„æœ€è¿‘é‚»æ•°é‡
) -> np.ndarray:
    """
    åŸºäºæ­£æ ·æœ¬ä¸èƒŒæ™¯æ•°æ®åˆ†å¸ƒçš„ç›¸å¯¹å…³ç³»è¿›è¡Œè‡ªé€‚åº”éš¾åº¦åŠ æƒé‡‡æ ·ï¼Œå¹¶åˆæˆæ–°æ ·æœ¬
    
    Args:
        x_pos: æ­£æ ·æœ¬ç‰¹å¾çŸ©é˜µ
        unlabeled_data: æœªæ ‡è®°çš„èƒŒæ™¯æ•°æ®ç‰¹å¾çŸ©é˜µ
        n_neighbors: è®¡ç®—éš¾åº¦æ—¶è€ƒè™‘çš„é‚»å±…æ•°é‡
        sampling_ratio: é‡‡æ ·æ¯”ä¾‹ï¼Œç›¸å¯¹äºæ­£æ ·æœ¬æ•°é‡çš„å€æ•°
        random_state: éšæœºç§å­
        synthetic_ratio: åˆæˆæ•°æ®çš„æ¯”ä¾‹(0-1)ï¼Œ0è¡¨ç¤ºå…¨éƒ¨é‡é‡‡æ ·ï¼Œ1è¡¨ç¤ºå…¨éƒ¨åˆæˆ
        k_smote: ç”¨äºSMOTEåˆæˆçš„è¿‘é‚»æ•°é‡
    
    Returns:
        é‡‡æ ·åçš„æ­£æ ·æœ¬ç‰¹å¾çŸ©é˜µï¼ˆåŒ…å«åŸå§‹æ ·æœ¬å’Œåˆæˆæ ·æœ¬ï¼‰
    """
    # è®¾ç½®éšæœºç§å­
    np.random.seed(random_state)
    
    n_pos = x_pos.shape[0]
    
    if n_pos == 0:
        logger.warning("æ²¡æœ‰æ­£æ ·æœ¬ï¼Œæ— æ³•è¿›è¡Œè‡ªé€‚åº”é‡‡æ ·")
        return x_pos
    
    # è®¡ç®—ç›®æ ‡é‡‡æ ·æ•°é‡
    n_samples = int(n_pos * sampling_ratio)
    if n_samples <= 0:
        logger.warning("è®¡ç®—çš„é‡‡æ ·æ•°é‡ä¸º0ï¼Œä¿æŒåŸå§‹æ•°æ®ä¸å˜")
        return x_pos
    
    # è®¡ç®—é‡é‡‡æ ·å’Œåˆæˆçš„æ•°é‡
    n_synthetic = int(n_samples * synthetic_ratio)
    n_resample = n_samples - n_synthetic
    
    logger.info(f"è‡ªé€‚åº”éš¾åº¦é‡‡æ ·: æ­£æ ·æœ¬æ•°={n_pos}, é‡‡æ ·æ¯”ä¾‹={sampling_ratio}, "
               f"ç›®æ ‡é‡‡æ ·æ•°é‡={n_samples} (é‡é‡‡æ ·={n_resample}, åˆæˆ={n_synthetic})")
    
    try:
        # è®¡ç®—æ¯ä¸ªæ­£æ ·æœ¬åˆ°èƒŒæ™¯æ•°æ®çš„å¹³å‡è·ç¦»ä½œä¸ºéš¾åº¦æŒ‡æ ‡
        # è·ç¦»æ›´è¿‘çš„æ­£æ ·æœ¬è¡¨ç¤ºæ›´æ¥è¿‘å†³ç­–è¾¹ç•Œï¼Œå­¦ä¹ éš¾åº¦æ›´å¤§
        
        # ğŸ” NaN è¯Šæ–­: æ£€æŸ¥è¾“å…¥æ•°æ®
        logger.info(f"[NaNè¯Šæ–­-ADS] x_pos shape: {x_pos.shape}, unlabeled_data shape: {unlabeled_data.shape}")
        
        # æ£€æŸ¥å¹¶å¤„ç† NaN å€¼
        nan_in_xpos = np.isnan(x_pos).sum()
        if nan_in_xpos > 0:
            logger.warning(f"[NaNè¯Šæ–­-ADS] æ­£æ ·æœ¬æ•°æ®ä¸­å‘ç° {nan_in_xpos} ä¸ª NaN å€¼ï¼Œå°†æ›¿æ¢ä¸º 0")
            rows_with_nan = np.where(np.isnan(x_pos).any(axis=1))[0]
            logger.warning(f"[NaNè¯Šæ–­-ADS] æ­£æ ·æœ¬ä¸­æœ‰ {len(rows_with_nan)} è¡ŒåŒ…å« NaN")
            x_pos = np.nan_to_num(x_pos, nan=0.0)
        else:
            logger.info(f"[NaNè¯Šæ–­-ADS] âœ… æ­£æ ·æœ¬æ•°æ®æ²¡æœ‰ NaN")
        
        nan_in_unlabeled = np.isnan(unlabeled_data).sum()
        if nan_in_unlabeled > 0:
            logger.warning(f"[NaNè¯Šæ–­-ADS] èƒŒæ™¯æ•°æ®ä¸­å‘ç° {nan_in_unlabeled} ä¸ª NaN å€¼ï¼Œå°†æ›¿æ¢ä¸º 0")
            rows_with_nan = np.where(np.isnan(unlabeled_data).any(axis=1))[0]
            logger.warning(f"[NaNè¯Šæ–­-ADS] èƒŒæ™¯æ•°æ®ä¸­æœ‰ {len(rows_with_nan)} è¡ŒåŒ…å« NaN")
            unlabeled_data = np.nan_to_num(unlabeled_data, nan=0.0)
        else:
            logger.info(f"[NaNè¯Šæ–­-ADS] âœ… èƒŒæ™¯æ•°æ®æ²¡æœ‰ NaN")
        
        # 1. å¯¹èƒŒæ™¯æ•°æ®å»ºç«‹KNNæ¨¡å‹
        nn_background = NearestNeighbors(n_neighbors=min(n_neighbors, unlabeled_data.shape[0]))
        nn_background.fit(unlabeled_data)
        
        # 2. è®¡ç®—æ¯ä¸ªæ­£æ ·æœ¬åˆ°æœ€è¿‘çš„n_neighborsä¸ªèƒŒæ™¯æ•°æ®ç‚¹çš„å¹³å‡è·ç¦»
        distances, _ = nn_background.kneighbors(x_pos)
        avg_distances = np.mean(distances, axis=1)
        
        # 3. è½¬æ¢è·ç¦»ä¸ºéš¾åº¦åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°ï¼Œéš¾åº¦è¶Šå¤§ï¼‰
        # è·ç¦»å–å€’æ•°åå½’ä¸€åŒ–
        if np.max(avg_distances) - np.min(avg_distances) < 1e-10:
            # å¦‚æœæ‰€æœ‰è·ç¦»å‡ ä¹ç›¸åŒï¼Œä½¿ç”¨å‡åŒ€æƒé‡
            logger.warning("æ‰€æœ‰æ ·æœ¬åˆ°èƒŒæ™¯æ•°æ®çš„è·ç¦»å‡ ä¹ç›¸åŒï¼Œä½¿ç”¨å‡åŒ€é‡‡æ ·")
            difficulties = np.ones(n_pos) / n_pos
        else:
            # åè½¬è·ç¦»å¹¶å½’ä¸€åŒ–ï¼ˆä½¿ç”¨è½¯æ€§åè½¬ä»¥é¿å…æç«¯å€¼ï¼‰
            difficulties = 1.0 / (avg_distances + 1e-6)
            difficulties = difficulties / np.sum(difficulties)
        
        # 4. è®°å½•éš¾åº¦åˆ†å¸ƒæƒ…å†µ
        logger.info(f"è·ç¦»ç»Ÿè®¡: æœ€å°={np.min(avg_distances):.4f}, æœ€å¤§={np.max(avg_distances):.4f}, "
                   f"å¹³å‡={np.mean(avg_distances):.4f}, æ ‡å‡†å·®={np.std(avg_distances):.4f}")
        logger.info(f"éš¾åº¦åˆ†å¸ƒ: æœ€å°={np.min(difficulties):.4f}, æœ€å¤§={np.max(difficulties):.4f}, "
                   f"å¹³å‡={np.mean(difficulties):.4f}, æ ‡å‡†å·®={np.std(difficulties):.4f}")
        
        # 5. åŸºäºéš¾åº¦è¿›è¡Œé‡é‡‡æ ·
        if n_resample > 0:
            resampled_indices = np.random.choice(
                np.arange(n_pos), size=n_resample, replace=True, p=difficulties
            )
            resampled_x = x_pos[resampled_indices]
        else:
            resampled_x = np.empty((0, x_pos.shape[1]))
        
        # 6. ä¸ºæ¯ä¸ªæ­£æ ·æœ¬åˆ†é…åˆæˆæ ·æœ¬æ•°é‡
        if n_synthetic > 0:
            # æ ¹æ®éš¾åº¦åˆ†é…æ¯ä¸ªæ ·æœ¬éœ€è¦åˆæˆçš„æ•°é‡
            synthetic_per_sample = np.zeros(n_pos, dtype=int)
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬åº”è¯¥ç”Ÿæˆå¤šå°‘åˆæˆæ ·æœ¬
            for _ in range(n_synthetic):
                # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬ï¼Œæ¦‚ç‡ä¸å…¶éš¾åº¦æˆæ­£æ¯”
                idx = np.random.choice(np.arange(n_pos), p=difficulties)
                synthetic_per_sample[idx] += 1
            
            # 7. åˆ›å»ºä¸€ä¸ªåŸºäºæ­£æ ·æœ¬çš„KNNæ¨¡å‹ï¼Œç”¨äºSMOTEåˆæˆ
            k_nn = min(k_smote, n_pos - 1)  # é˜²æ­¢kå¤§äºæ ·æœ¬æ•°-1
            if k_nn <= 0:
                logger.warning(f"æ­£æ ·æœ¬æ•°é‡è¿‡å°‘({n_pos})ï¼Œæ— æ³•æ‰§è¡ŒSMOTEåˆæˆï¼Œå°†åªè¿›è¡Œé‡é‡‡æ ·")
                synthetic_x = np.empty((0, x_pos.shape[1]))
            else:
                nn_pos = NearestNeighbors(n_neighbors=k_nn+1).fit(x_pos)  # +1æ˜¯å› ä¸ºåŒ…å«è‡ªèº«
                
                # ç”¨äºå­˜å‚¨åˆæˆæ ·æœ¬
                synthetic_x = []
                
                # 8. ä¸ºæ¯ä¸ªæ­£æ ·æœ¬ç”Ÿæˆå¯¹åº”æ•°é‡çš„åˆæˆæ ·æœ¬
                for i in range(n_pos):
                    n_to_generate = synthetic_per_sample[i]
                    if n_to_generate == 0:
                        continue
                    
                    # æ‰¾åˆ°å½“å‰æ ·æœ¬çš„kä¸ªæœ€è¿‘é‚»
                    distances_i, indices_i = nn_pos.kneighbors([x_pos[i]])
                    # æ’é™¤è‡ªèº«
                    nn_indices = indices_i[0][1:]
                    
                    # ä¸ºå½“å‰æ ·æœ¬ç”ŸæˆæŒ‡å®šæ•°é‡çš„åˆæˆæ ·æœ¬
                    for _ in range(n_to_generate):
                        # éšæœºé€‰æ‹©ä¸€ä¸ªè¿‘é‚»
                        nn_idx = np.random.choice(nn_indices)
                        # ç”Ÿæˆä¸€ä¸ª0åˆ°1ä¹‹é—´çš„éšæœºæ•°ä½œä¸ºæ’å€¼æ¯”ä¾‹
                        alpha = np.random.random()
                        # ä½¿ç”¨SMOTEæ–¹æ³•ç”Ÿæˆåˆæˆæ ·æœ¬
                        synthetic_sample = x_pos[i] + alpha * (x_pos[nn_idx] - x_pos[i])
                        synthetic_x.append(synthetic_sample)
                
                # å°†åˆæˆæ ·æœ¬åˆ—è¡¨è½¬æ¢ä¸ºnumpyæ•°ç»„
                if synthetic_x:
                    synthetic_x = np.array(synthetic_x)
                else:
                    synthetic_x = np.empty((0, x_pos.shape[1]))
        else:
            synthetic_x = np.empty((0, x_pos.shape[1]))
        
        # 9. åˆå¹¶é‡é‡‡æ ·æ ·æœ¬å’Œåˆæˆæ ·æœ¬
        final_samples = np.vstack([resampled_x, synthetic_x]) if synthetic_x.size > 0 else resampled_x
        
        # 10. è®°å½•é‡‡æ ·å’Œåˆæˆçš„ç»Ÿè®¡ä¿¡æ¯
        if n_resample > 0:
            unique_indices, counts = np.unique(resampled_indices, return_counts=True)
            coverage = len(unique_indices) / n_pos * 100
            logger.info(f"é‡é‡‡æ ·è¦†ç›–ç‡: {coverage:.2f}% ({len(unique_indices)}/{n_pos})")
            logger.info(f"é‡é‡‡æ ·é¢‘ç‡: æœ€å°={np.min(counts) if counts.size > 0 else 0}, "
                       f"æœ€å¤§={np.max(counts) if counts.size > 0 else 0}, "
                       f"å¹³å‡={np.mean(counts) if counts.size > 0 else 0:.2f}")
        
        if n_synthetic > 0:
            # ç»Ÿè®¡åˆæˆæ ·æœ¬æ•°é‡
            synthetic_count = np.sum(synthetic_per_sample > 0)
            synthetic_coverage = synthetic_count / n_pos * 100
            logger.info(f"åˆæˆæ ·æœ¬è¦†ç›–ç‡: {synthetic_coverage:.2f}% ({synthetic_count}/{n_pos})")
            logger.info(f"åˆæˆæ ·æœ¬æ•°é‡: {synthetic_x.shape[0]}")
        
        logger.info(f"æœ€ç»ˆæ ·æœ¬æ•°é‡: {final_samples.shape[0]} = {resampled_x.shape[0]}(é‡é‡‡æ ·) + {synthetic_x.shape[0]}(åˆæˆ)")
        
        # è¿”å›æœ€ç»ˆæ ·æœ¬
        return final_samples
        
    except Exception as e:
        logger.error(f"è‡ªé€‚åº”éš¾åº¦é‡‡æ ·è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        # å‡ºé”™æ—¶è¿”å›ç®€å•éšæœºé‡‡æ ·çš„ç»“æœ
        logger.warning("å›é€€åˆ°ç®€å•éšæœºé‡‡æ ·")
        sampled_indices = np.random.choice(np.arange(n_pos), size=n_samples, replace=True)
        return x_pos[sampled_indices]

def train_model(prefered_df: pl.DataFrame, remaining_df: pl.DataFrame, config: dict):
    # Placeholder for training logic
    logger.info("Training model...")
    # Convert labels from string to int, where 'like' is 1 and 'dislike' is 0

    embedding_columns = config.get("embedding_columns")
    prefered_df = prefered_df.with_columns(
        pl.when(pl.col("preference") == "like").then(1).otherwise(0).alias("label")
    ).select("label", *embedding_columns)

    remaining_df = remaining_df.select(*embedding_columns)

    # calculate positive sample number(like)
    positive_sample_num = prefered_df.filter(pl.col("label") == 1).height
    logger.debug(f"Positive sample number: {positive_sample_num}")
    
    # sample negative background data with neg_sample_ratio * positive_sample_num
    neg_sample_ratio = config.get("neg_sample_ratio", 5.0)
    neg_sample_num = int(neg_sample_ratio * positive_sample_num)
    logger.debug(f"Negative sample number: {neg_sample_num}")
    
    # sample negative data
    pesudo_neg_df = remaining_df.sample(n=neg_sample_num, seed=42)
    pesudo_neg_df = pesudo_neg_df.with_columns(
        pl.lit(0).alias("label")
    ).select("label", *embedding_columns)

    # combine 
    combined_df = pl.concat([prefered_df, pesudo_neg_df], how="vertical")
    logger.info(f"Combined DataFrame size (before NaN filtering): {combined_df.height} rows")

    # ğŸ”§ è¿‡æ»¤å‘é‡å†…éƒ¨åŒ…å« NaN çš„æ ·æœ¬
    # ç­–ç•¥: å…ˆè½¬æ¢ä¸º numpy æ£€æŸ¥,ç„¶åè¿‡æ»¤
    logger.info(f"å¼€å§‹è¿‡æ»¤å‘é‡å†…éƒ¨åŒ…å« NaN çš„æ ·æœ¬...")
    
    # é€åˆ—æ£€æŸ¥ NaN
    nan_mask = np.zeros(combined_df.height, dtype=bool)
    for col in embedding_columns:
        col_data = combined_df[col].to_list()
        for i, vec in enumerate(col_data):
            if vec is None or (isinstance(vec, (list, np.ndarray)) and np.isnan(vec).any()):
                nan_mask[i] = True
    
    # è¿‡æ»¤ä¿ç•™æœ‰æ•ˆè¡Œ
    removed_count = nan_mask.sum()
    if removed_count > 0:
        logger.warning(f"è¿‡æ»¤äº† {removed_count}/{len(combined_df)} ({removed_count/len(combined_df)*100:.2f}%) ä¸ªå« NaN çš„æ ·æœ¬")
        # ä½¿ç”¨ polars çš„ filter æŒ‰è¡Œè¿‡æ»¤
        combined_df = combined_df.with_row_index("__idx__")
        valid_indices = np.where(~nan_mask)[0]
        combined_df = combined_df.filter(pl.col("__idx__").is_in(valid_indices)).drop("__idx__")
        logger.info(f"Combined DataFrame size (after NaN filtering): {combined_df.height} rows")
    else:
        logger.info(f"âœ… æ²¡æœ‰å‘é‡å†…éƒ¨åŒ…å« NaN çš„æ ·æœ¬")

    # ğŸ” NaN è¯Šæ–­: æ£€æŸ¥åŸå§‹æ•°æ®ä¸­çš„ None å€¼
    logger.info(f"[NaNè¯Šæ–­] å¼€å§‹æ£€æŸ¥è¿‡æ»¤åçš„ combined_df")
    for col in embedding_columns:
        col_data = combined_df[col].to_list()
        none_count = sum(1 for x in col_data if x is None)
        if none_count > 0:
            logger.warning(f"[NaNè¯Šæ–­] åˆ— '{col}' æœ‰ {none_count} ä¸ª None å€¼ ({none_count/len(col_data)*100:.2f}%)")
            # æ‰¾å‡º None å€¼çš„è®ºæ–‡ ID
            if 'id' in combined_df.columns:
                none_indices = [i for i, x in enumerate(col_data) if x is None][:5]
                none_ids = [combined_df['id'][i] for i in none_indices]
                logger.warning(f"[NaNè¯Šæ–­] None å€¼çš„è®ºæ–‡IDç¤ºä¾‹: {none_ids}")

    # convert to numpy array, where x should be the concatenated embedding columns
    # and y should be the label column
    # ğŸ” NaN è¯Šæ–­: é€åˆ—è½¬æ¢å¹¶æ£€æŸ¥
    logger.info(f"[NaNè¯Šæ–­] å¼€å§‹é€åˆ—è½¬æ¢ä¸º numpy æ•°ç»„")
    arrays = []
    for col in embedding_columns:
        try:
            col_arr = np.vstack(combined_df[col].to_numpy())
            nan_count = np.isnan(col_arr).sum()
            logger.info(f"[NaNè¯Šæ–­] åˆ— '{col}' vstackå: shape={col_arr.shape}, NaNæ•°é‡={nan_count}")
            if nan_count > 0:
                rows_with_nan = np.where(np.isnan(col_arr).any(axis=1))[0]
                logger.warning(f"[NaNè¯Šæ–­] åˆ— '{col}' ä¸­æœ‰ {len(rows_with_nan)} è¡ŒåŒ…å« NaN")
                logger.warning(f"[NaNè¯Šæ–­] å‰10ä¸ªNaNè¡Œç´¢å¼•: {rows_with_nan[:10].tolist()}")
            arrays.append(col_arr)
        except Exception as e:
            logger.error(f"[NaNè¯Šæ–­] åˆ— '{col}' è½¬æ¢å¤±è´¥: {e}")
            raise
    
    x = np.hstack(arrays)
    y = combined_df.select("label").to_numpy().ravel()
    logger.info(f"[NaNè¯Šæ–­] hstackå: shape={x.shape}")
    
    samples_with_nan = np.isnan(x).any(axis=1).sum()
    if samples_with_nan > 0:
        logger.warning(f"[NaNè¯Šæ–­] æœ€ç»ˆæ£€æµ‹: {samples_with_nan}ä¸ªæ ·æœ¬ ({samples_with_nan/x.shape[0]*100:.2f}%) åŒ…å«NaNå€¼")
    else:
        logger.info(f"[NaNè¯Šæ–­] âœ… æœ€ç»ˆæ£€æµ‹: æ²¡æœ‰NaNå€¼!")
    
    # æ–¹æ³•1ï¼šç”¨0å¡«å……NaN (ç®€å•æ–¹æ³•)
    if samples_with_nan > 0:
        x = np.nan_to_num(x, nan=0.0)
        logger.info("å·²å°†æ‰€æœ‰NaNå€¼æ›¿æ¢ä¸º0")

    logger.info(f"x: {x.dtype} | {x.shape}, y shape: {y.dtype} | {y.shape}")
    lg_config = config.get("logistic_regression", {})
    cws_config = config.get("confidence_weighted_sampling", {'enable': False})

    if cws_config.get("enable", False):
        logger.info("Using confidence weighted sampling...")
        tmp_model = sklearn.linear_model.LogisticRegression(
            C=lg_config.get("C", 1.0),
            max_iter=lg_config.get("max_iter", 1000),
            random_state=config.get("seed", 42),
            class_weight="balanced",
        ).fit(x, y)
        new_positive_embedding = confidence_weighted_sampling(
            x[y == 1], tmp_model,
            high_conf_threshold=cws_config.get("high_conf_threshold", 0.9),
            high_conf_weight=cws_config.get("high_conf_weight", 2.0),
            random_state=config.get("seed", 42)
        )

        x = np.concatenate((x[y == 0], new_positive_embedding))
        y = np.concatenate((y[y == 0], np.ones(new_positive_embedding.shape[0])))
        logger.info(f"New x shape: {x.shape}, New y shape: {y.shape}")

    ads_config = config.get("adaptive_difficulty_sampling", {'enable': False})
    if ads_config.get("enable", False):
        # unlabeled_data = remaining_df.select(*embedding_columns).to_numpy()
        unlabeled_data = np.hstack([np.vstack(remaining_df[col].to_numpy()) for col in embedding_columns])
        x_pos = adaptive_difficulty_sampling(
            x[y == 1], unlabeled_data,
            n_neighbors=config.get("n_neighbors", 5),
            sampling_ratio=config.get("pos_sampling_ratio", 2.0),
            random_state=config.get("seed", 42),
            synthetic_ratio=config.get("synthetic_ratio", 0.5),  # é»˜è®¤50%æ˜¯åˆæˆçš„
            k_smote=config.get("k_smote", 16)  # SMOTEçš„kè¿‘é‚»å‚æ•°
        )
        x = np.concatenate((x[y == 0], x_pos))
        y = np.concatenate((y[y == 0], np.ones(x_pos.shape[0])))

    # Train the final model
    final_model = sklearn.linear_model.LogisticRegression(
        C=lg_config.get("C", 1.0),
        max_iter=lg_config.get("max_iter", 1000),
        random_state=config.get("seed", 42),
        class_weight="balanced",
    ).fit(x, y)

    logger.info("Model training completed.")
    return final_model
    

"""
Prediction Part
"""

def predict_and_recommend(model, remaining_df: pl.LazyFrame, recommended_df: pl.DataFrame, config: dict) -> pl.DataFrame:
    """æ ¹æ®è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹å‰©ä½™æ•°æ®è¿›è¡Œé¢„æµ‹å¹¶æ¨èè®ºæ–‡ã€‚
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        remaining_df: å‰©ä½™æ•°æ®
        recommended_df: å·²æ¨èçš„æ•°æ®
        config: é…ç½®å‚æ•°
    
    Returns:
        å¸¦æœ‰æ¨èæ ‡è®°çš„DataFrame
    """
    logger.info("å¼€å§‹é¢„æµ‹å’Œæ¨èé˜¶æ®µ...")
    
    # 1. ä»å‰©ä½™æ•°æ®ä¸­æ’é™¤å·²æ¨èçš„è®ºæ–‡
    target_df = remove_recommended(remaining_df, recommended_df)
    
    # 2. æ ¹æ®é…ç½®é€‰æ‹©ç›®æ ‡æ—¶é—´æ®µçš„æ•°æ®
    predict_config = config.get("predict", {})
    
    # è§£ææ—¥æœŸç›¸å…³é…ç½®
    last_n_days = predict_config.get("last_n_days", 7)
    start_date = predict_config.get("start_date", "")
    end_date = predict_config.get("end_date", "")
    
    # å¦‚æœæŒ‡å®šäº†èµ·æ­¢æ—¥æœŸï¼Œä½¿ç”¨æ—¥æœŸèŒƒå›´ç­›é€‰
    if start_date and end_date:
        logger.info(f"ä½¿ç”¨æŒ‡å®šçš„æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
        target_df = target_df.filter(
            (pl.col("updated") >= start_date) & (pl.col("updated") <= end_date)
        )
    # å¦åˆ™ä½¿ç”¨æœ€è¿‘Nå¤©
    else:
        # è®¡ç®—æœ€è¿‘Nå¤©çš„æ—¥æœŸ
        today = datetime.datetime.now()
        n_days_ago = (today - datetime.timedelta(days=last_n_days)).strftime("%Y-%m-%d")
        logger.info(f"ä½¿ç”¨æœ€è¿‘{last_n_days}å¤©çš„æ•°æ®: {n_days_ago} åˆ° {today.strftime('%Y-%m-%d')}")
        
        target_df = target_df.filter(pl.col("updated") >= n_days_ago)
    
    # 3. æ”¶é›†ç›®æ ‡æ•°æ®
    logger.info("æ”¶é›†ç›®æ ‡æ•°æ®...")
    embedding_columns = config.get("embedding_columns")
    
    # å¦‚æœæ²¡æœ‰ç›®æ ‡æ•°æ®ï¼Œæå‰è¿”å›
    if target_df.is_empty():
        logger.warning("æ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„ç›®æ ‡æ•°æ®å¯ä¾›æ¨è")
        return target_df
    
    logger.info(f"ç›®æ ‡æ•°æ®æ”¶é›†å®Œæˆï¼Œå…± {target_df.height} æ¡è®°å½•")
    
    # ğŸ”§ è¿‡æ»¤å‘é‡å†…éƒ¨åŒ…å« NaN çš„æ ·æœ¬ (é¢„æµ‹é˜¶æ®µ)
    logger.info(f"å¼€å§‹è¿‡æ»¤é¢„æµ‹æ•°æ®ä¸­å‘é‡å†…éƒ¨åŒ…å« NaN çš„æ ·æœ¬...")
    
    nan_mask = np.zeros(target_df.height, dtype=bool)
    for col in embedding_columns:
        col_data = target_df[col].to_list()
        for i, vec in enumerate(col_data):
            if vec is None or (isinstance(vec, (list, np.ndarray)) and np.isnan(vec).any()):
                nan_mask[i] = True
    
    removed_count = nan_mask.sum()
    if removed_count > 0:
        logger.warning(f"è¿‡æ»¤äº† {removed_count}/{target_df.height} ({removed_count/target_df.height*100:.2f}%) ä¸ªå« NaN çš„æ ·æœ¬")
        target_df = target_df.with_row_index("__idx__")
        valid_indices = np.where(~nan_mask)[0]
        target_df = target_df.filter(pl.col("__idx__").is_in(valid_indices)).drop("__idx__")
        logger.info(f"ç›®æ ‡æ•°æ®è¿‡æ»¤å: {target_df.height} æ¡è®°å½•")
    else:
        logger.info(f"âœ… é¢„æµ‹æ•°æ®æ²¡æœ‰å‘é‡å†…éƒ¨åŒ…å« NaN çš„æ ·æœ¬")
    
    # 4. æå–ç‰¹å¾å’Œé¢„æµ‹
    logger.info("æå–ç‰¹å¾å¹¶è¿›è¡Œé¢„æµ‹...")
    
    # ğŸ” NaN è¯Šæ–­: æ£€æŸ¥é¢„æµ‹æ•°æ®
    logger.info(f"[NaNè¯Šæ–­-é¢„æµ‹] å¼€å§‹æ£€æŸ¥è¿‡æ»¤åçš„é¢„æµ‹æ•°æ®")
    for col in embedding_columns:
        col_data = target_df[col].to_list()
        none_count = sum(1 for x in col_data if x is None)
        if none_count > 0:
            logger.warning(f"[NaNè¯Šæ–­-é¢„æµ‹] åˆ— '{col}' æœ‰ {none_count} ä¸ª None å€¼ ({none_count/len(col_data)*100:.2f}%)")
    
    # X_target = target_df.select(*embedding_columns).to_numpy()
    arrays = []
    for col in embedding_columns:
        col_arr = np.vstack(target_df[col].to_numpy())
        nan_count = np.isnan(col_arr).sum()
        if nan_count > 0:
            logger.warning(f"[NaNè¯Šæ–­-é¢„æµ‹] åˆ— '{col}' vstackåæœ‰ {nan_count} ä¸ª NaN")
        arrays.append(col_arr)
    
    X_target = np.hstack(arrays)
    logger.info(f"[NaNè¯Šæ–­-é¢„æµ‹] X_target shape: {X_target.shape}")
    
    # å¤„ç† NaN å€¼
    nan_count = np.isnan(X_target).sum()
    if nan_count > 0:
        logger.warning(f"[NaNè¯Šæ–­-é¢„æµ‹] æœ€ç»ˆæ£€æµ‹: é¢„æµ‹æ•°æ®ä¸­å‘ç° {nan_count} ä¸ª NaN å€¼ ({nan_count/X_target.size*100:.4f}%)ï¼Œå°†æ›¿æ¢ä¸º 0")
        rows_with_nan = np.where(np.isnan(X_target).any(axis=1))[0]
        logger.warning(f"[NaNè¯Šæ–­-é¢„æµ‹] æœ‰ {len(rows_with_nan)} è¡ŒåŒ…å« NaN")
        X_target = np.nan_to_num(X_target, nan=0.0)
    else:
        logger.info(f"[NaNè¯Šæ–­-é¢„æµ‹] âœ… é¢„æµ‹æ•°æ®æ²¡æœ‰ NaN")
    
    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹"å–œæ¬¢"çš„æ¦‚ç‡
    try:
        target_scores = model.predict_proba(X_target)[:, 1]
        logger.info(f"é¢„æµ‹å®Œæˆï¼Œåˆ†æ•°èŒƒå›´: {np.min(target_scores):.4f} - {np.max(target_scores):.4f}, å¹³å‡: {np.mean(target_scores):.4f}")
    except Exception as e:
        logger.error(f"é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        # å¦‚æœé¢„æµ‹å¤±è´¥ï¼Œè¿”å›éšæœºåˆ†æ•°
        logger.warning("é¢„æµ‹å¤±è´¥ï¼Œç”Ÿæˆéšæœºåˆ†æ•°")
        np.random.seed(config.get("seed", 42))
        target_scores = np.random.random(target_df.height)
    
    # 5. æ·»åŠ é¢„æµ‹åˆ†æ•°åˆ°DataFrame
    target_df = target_df.with_columns(pl.lit(target_scores).alias("score"))
    
    # 6. æ‰§è¡Œè‡ªé€‚åº”é‡‡æ ·å†³å®šæ¨è
    logger.info("æ‰§è¡Œè‡ªé€‚åº”é‡‡æ ·ç¡®å®šæ¨è...")
    
    high_threshold = predict_config.get("high_threshold", 0.95)
    boundary_threshold = predict_config.get("boundary_threshold", 0.5)
    sample_rate = predict_config.get("sample_rate", 0.15)
    
    show_flags = adaptive_sample(
        target_scores, 
        target_sample_rate=sample_rate,
        high_threshold=high_threshold,
        boundary_threshold=boundary_threshold,
        random_state=config.get("seed", 42)
    )
    
    # 7. æ·»åŠ æ¨èæ ‡è®°åˆ°DataFrame
    target_df = target_df.with_columns(pl.lit(show_flags.astype(np.int8)).alias("show"))
    
    # 8. ç»Ÿè®¡å’Œè®°å½•ç»“æœ
    recommended_count = np.sum(show_flags)
    logger.info(f"æ¨èå®Œæˆ: æ€»è®¡ {target_df.height} ç¯‡è®ºæ–‡ä¸­æ¨è {recommended_count} ç¯‡ ({recommended_count/target_df.height*100:.2f}%)")
    
    # ç»Ÿè®¡ä¸åŒåˆ†æ•°åŒºé—´çš„æ¨èæ¯”ä¾‹
    score_intervals = [(0.0, 0.3), (0.3, 0.5), (0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]
    for low, high in score_intervals:
        interval_mask = (target_scores >= low) & (target_scores < high)
        if np.sum(interval_mask) > 0:
            interval_show = np.sum(show_flags & interval_mask)
            interval_total = np.sum(interval_mask)
            logger.info(f"åˆ†æ•° {low:.1f}-{high:.1f}: {interval_show}/{interval_total} ({interval_show/interval_total*100:.2f}%)")
    
    # 9. è¿”å›ç»“æœDataFrame
    return target_df


def adaptive_sample(scores, target_sample_rate=0.15, high_threshold=0.95, boundary_threshold=0.5, random_state=42):
    """è‡ªé€‚åº”é‡‡æ ·ç®—æ³•ï¼Œæ ¹æ®åˆ†æ•°å†³å®šå“ªäº›æ ·æœ¬åº”è¯¥è¢«æ¨èã€‚
    
    ç­–ç•¥:
    1. æ‰€æœ‰é«˜äºhigh_thresholdçš„æ ·æœ¬è¢«ä¼˜å…ˆæ¨è
    2. å¦‚æœé«˜åˆ†æ ·æœ¬æ•°é‡è¶…è¿‡ç›®æ ‡æ•°é‡ï¼ŒéšæœºæŠ½å–ä¸€éƒ¨åˆ†
    3. å¦‚æœé«˜åˆ†æ ·æœ¬ä¸è¶³ï¼Œä»boundary_thresholdåˆ°high_thresholdä¹‹é—´çš„åˆ†æ•°ä¸­æŒ‰æƒé‡é‡‡æ ·
    
    Args:
        scores: æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹åˆ†æ•°
        target_sample_rate: ç›®æ ‡æ¨èæ¯”ä¾‹
        high_threshold: é«˜ç½®ä¿¡åº¦é˜ˆå€¼
        boundary_threshold: è¾¹ç•Œé˜ˆå€¼
        random_state: éšæœºç§å­
    
    Returns:
        å¸ƒå°”æ•°ç»„ï¼Œæ ‡è®°å“ªäº›æ ·æœ¬è¢«æ¨è
    """
    np.random.seed(random_state)
    n_samples = len(scores)
    target_count = int(n_samples * target_sample_rate)
    
    if target_count <= 0:
        # è‡³å°‘æ¨èä¸€ä¸ª
        target_count = 1
    
    # åˆå§‹åŒ–æ¨èæ ‡è®°æ•°ç»„
    show_flags = np.zeros(n_samples, dtype=bool)
    
    # æ‰¾å‡ºæ‰€æœ‰é«˜åˆ†æ ·æœ¬
    high_score_mask = scores >= high_threshold
    high_score_indices = np.where(high_score_mask)[0]
    high_score_count = len(high_score_indices)
    
    logger.info(f"ç›®æ ‡æ¨èæ•°é‡: {target_count} / {n_samples} ({target_sample_rate*100:.2f}%)")
    logger.info(f"é«˜åˆ†æ ·æœ¬(>={high_threshold:.4f})æ•°é‡: {high_score_count} ({high_score_count/n_samples*100:.2f}%)")
    
    # æƒ…å†µA: é«˜åˆ†æ ·æœ¬è¶³å¤Ÿæˆ–è¶…è¿‡ç›®æ ‡æ•°é‡
    if high_score_count >= target_count:
        # å¦‚æœé«˜åˆ†æ ·æœ¬è¶…è¿‡äº†ç›®æ ‡æ•°é‡ï¼Œéšæœºé€‰æ‹©ä¸€éƒ¨åˆ†
        if high_score_count > target_count:
            selected_indices = np.random.choice(high_score_indices, target_count, replace=False)
            show_flags[selected_indices] = True
            logger.info(f"é«˜åˆ†æ ·æœ¬è¶…è¿‡ç›®æ ‡æ•°é‡ï¼Œéšæœºé€‰æ‹©äº†{target_count}ä¸ª")
        else:
            # é«˜åˆ†æ ·æœ¬æ•°é‡åˆšå¥½ç­‰äºç›®æ ‡æ•°é‡
            show_flags[high_score_indices] = True
            logger.info(f"é«˜åˆ†æ ·æœ¬æ•°é‡æ°å¥½ç­‰äºç›®æ ‡æ•°é‡")
        
        return show_flags
    
    # æƒ…å†µB: é«˜åˆ†æ ·æœ¬ä¸è¶³ï¼Œéœ€è¦ä»ä¸­ç­‰åˆ†æ•°æ ·æœ¬ä¸­è¡¥å……
    # å…ˆæ ‡è®°æ‰€æœ‰é«˜åˆ†æ ·æœ¬
    show_flags[high_score_indices] = True
    remaining_count = target_count - high_score_count
    
    # æ‰¾å‡ºè¾¹ç•ŒåŒºåŸŸçš„æ ·æœ¬
    boundary_mask = (scores >= boundary_threshold) & (scores < high_threshold)
    boundary_indices = np.where(boundary_mask)[0]
    boundary_count = len(boundary_indices)
    
    logger.info(f"è¾¹ç•Œæ ·æœ¬({boundary_threshold:.4f}-{high_threshold:.4f})æ•°é‡: {boundary_count} ({boundary_count/n_samples*100:.2f}%)")
    
    if boundary_count == 0:
        # å¦‚æœæ²¡æœ‰è¾¹ç•Œæ ·æœ¬ï¼Œä»æ‰€æœ‰å‰©ä½™æ ·æœ¬ä¸­éšæœºé€‰æ‹©
        remaining_indices = np.where(~high_score_mask)[0]
        if len(remaining_indices) > 0:
            if len(remaining_indices) > remaining_count:
                # éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†
                selected_indices = np.random.choice(remaining_indices, remaining_count, replace=False)
            else:
                # å…¨éƒ¨é€‰æ‹©
                selected_indices = remaining_indices
            
            show_flags[selected_indices] = True
            logger.info(f"æ— è¾¹ç•Œæ ·æœ¬ï¼Œä»æ‰€æœ‰å‰©ä½™æ ·æœ¬ä¸­éšæœºé€‰æ‹©äº†{len(selected_indices)}ä¸ª")
    else:
        # ä»è¾¹ç•ŒåŒºåŸŸæŒ‰æƒé‡é‡‡æ ·
        # è®¡ç®—è¾¹ç•ŒåŒºåŸŸæ ·æœ¬çš„æƒé‡ï¼ˆåˆ†æ•°è¶Šé«˜æƒé‡è¶Šå¤§ï¼‰
        boundary_scores = scores[boundary_indices]
        # å½’ä¸€åŒ–åˆ°[0,1]åŒºé—´ï¼Œæé«˜å¯¹æ¯”åº¦
        min_score = boundary_threshold
        max_score = high_threshold
        normalized_scores = (boundary_scores - min_score) / (max_score - min_score)
        # ä½¿ç”¨æŒ‡æ•°å‡½æ•°å¢å¼ºå·®å¼‚ (å¯é€‰)
        weights = np.exp(normalized_scores * 2)  # ä¹˜ä»¥2æ˜¯ä¸ºäº†å¢åŠ å¯¹æ¯”åº¦
        weights = weights / np.sum(weights)
        
        # åŠ æƒé‡‡æ ·
        sample_size = min(remaining_count, boundary_count)
        selected_indices = np.random.choice(
            boundary_indices, sample_size, replace=False, p=weights
        )
        show_flags[selected_indices] = True
        
        logger.info(f"ä»è¾¹ç•ŒåŒºåŸŸåŠ æƒé‡‡æ ·äº†{len(selected_indices)}ä¸ªæ ·æœ¬")
        
        # å¦‚æœè¾¹ç•ŒåŒºåŸŸæ ·æœ¬æ•°é‡ä»ä¸è¶³ï¼Œä»ä½åˆ†åŒºåŸŸéšæœºé‡‡æ ·è¡¥è¶³
        if sample_size < remaining_count:
            still_remaining = remaining_count - sample_size
            low_score_mask = scores < boundary_threshold
            low_score_indices = np.where(low_score_mask)[0]
            
            if len(low_score_indices) > 0:
                sample_size_low = min(still_remaining, len(low_score_indices))
                selected_indices_low = np.random.choice(low_score_indices, sample_size_low, replace=False)
                show_flags[selected_indices_low] = True
                logger.info(f"ä»ä½åˆ†åŒºåŸŸ({boundary_threshold:.4f}ä»¥ä¸‹)éšæœºé‡‡æ ·äº†{len(selected_indices_low)}ä¸ªæ ·æœ¬")
    
    return show_flags


def predict_and_save(model, remaining_df, recommended_df, config):
    """é¢„æµ‹ã€æ¨èå¹¶ä¿å­˜ç»“æœã€‚
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        remaining_df: å‰©ä½™æ•°æ®
        recommended_df: å·²æ¨èçš„æ•°æ®
        config: é…ç½®å‚æ•°
    
    Returns:
        æ¨èçš„DataFrame
    """
    # æ‰§è¡Œé¢„æµ‹å’Œæ¨è
    results_df = predict_and_recommend(model, remaining_df, recommended_df, config)
    
    # å¦‚æœç»“æœä¸ºç©ºï¼Œæå‰è¿”å›
    if results_df.is_empty():
        logger.warning("æ²¡æœ‰æ¨èç»“æœå¯ä¿å­˜")
        return results_df
    
    # ç§»é™¤embeddingåˆ—ä»¥å‡å°æ–‡ä»¶å¤§å°
    embedding_columns = config.get("embedding_columns", [])
    if embedding_columns:
        results_df = results_df.drop(*embedding_columns)
    # ä¿å­˜ç»“æœåˆ°CSV
    output_file = REPO_ROOT / "data" / "predictions.parquet"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜
    results_df.write_parquet(output_file)
    logger.info(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æå–å¹¶è¿”å›æ¨èçš„è®ºæ–‡
    recommended_results = results_df.filter(pl.col("show") == 1)
    logger.info(f"æ¨è{recommended_results.height}ç¯‡è®ºæ–‡")
    show_df = recommended_results.select("id", "title", "abstract", "score")
    logger.debug(f"{show_df}")
    
    return recommended_results

if __name__ == "__main__":
    log_info()

    config = load_config()
    preferences = load_preference(config)
    recommended = load_recommended(config)

    prefered_df, remaining_df = load_lazy_dataset(config, preferences)
    remaining_df = remove_recommended(remaining_df, recommended)

    prefered_df = prefered_df.collect()
    remaining_df = remaining_df.collect()

    model = train_model(prefered_df, remaining_df, config)
    logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")

    predict_and_save(model, remaining_df, recommended, config)
    