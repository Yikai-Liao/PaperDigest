{
    "institution": [
        "Carnegie Mellon University",
        "Google"
    ],
    "problem_background": "Detailed reasoning traces generated by Large Language Models (LLMs), while powerful, have become a \"vulnerability.\" Competitors can exploit these publicly available reasoning processes to cheaply replicate powerful models through \"Model Distillation,\" leading to intellectual property leakage and potential security risks (e.g., bypassing safety constraints).",
    "method": {
        "core_idea": "Poison the reasoning traces generated by the original model (teacher model) to interfere with the distillation process, without sacrificing the teacher model's performance.",
        "implementation": "This is a sampling strategy applied during token generation:\n1.  In addition to the teacher model's original probabilities, an \"anti-distillation\" adjustment term is introduced.\n2.  This adjustment term uses a Proxy Model and the loss gradient of a downstream task to estimate which tokens are \"harmful\" to distillation (i.e., selecting them reduces distillation effectiveness).\n3.  The next token is ultimately sampled from this adjusted probability distribution.",
        "key_points": "The original teacher model is not modified; only the sampling process during inference is adjusted. The poisoning intensity is controlled to avoid impacting the teacher model's own performance."
    },
    "experiment": {
        "effectiveness": "While maintaining the teacher model's accuracy (e.g., on GSM8K, MATH datasets), text generated using anti-distillation sampling significantly degrades the student model's distillation performance (accuracy drops substantially).",
        "superiority": "Compared to simply increasing the sampling temperature (which leads to a sharp decline in teacher model performance), anti-distillation sampling offers a better trade-off between performance and resistance to distillation.",
        "overhead": "The main added cost is two forward passes through the proxy model (a smaller model) for each generated token."
    },
    "one_sentence_summary": "This paper proposes an anti-distillation sampling method that, with the help of a proxy model, dynamically adjusts the sampling distribution for each token during inference to poison the LLM's reasoning traces, thereby interfering with model distillation while maintaining the original model's performance and significantly increasing the difficulty of distilling it by others.",
    "key_words": [
        "LLM",
        "Proxy Model",
        "Distillation",
        "Sampling",
        "Reasoning"
    ],
    "slug": "antidistillation-sampling",
    "further_thoughts": "Perhaps not only small models can be used as proxy models to adjust the probability distribution. This is because reasoning data from different models exhibits varying distillation effectiveness. For example, some studies suggest that reasoning data from models like DeepSeek Coder shows stronger generalization capabilities when used for distillation across different model architectures, whereas data from models like Alibaba's Qwen series (e.g., a 32B variant) might only yield good distillation results when applied to models within the same Qwen family."
}