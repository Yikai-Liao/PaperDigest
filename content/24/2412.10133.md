---
title: "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects"
slug: "2412.10133"
description: "本文提出ExecutionAgent，一个基于LLM的自主代理，通过meta-prompting和迭代反馈机制自动设置并执行任意软件项目的测试套件，显著提高了测试执行的成功率和准确性。"
tags: ["Large Language Models", "LLM Agents", "Autonomous Software Development", "Project Setup Automation", "Test Suite Execution", "DevOps", "Software Testing", "Artificial Intelligence"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:26:30.672735+00:00
preference: "unknown"
score: 0.5306922266449793
featured: false
draft: false
---

> 本文提出ExecutionAgent，一个基于LLM的自主代理，通过meta-prompting和迭代反馈机制自动设置并执行任意软件项目的测试套件，显著提高了测试执行的成功率和准确性。

> Large Language Models, LLM Agents, Autonomous Software Development, Project Setup Automation, Test Suite Execution, DevOps, Software Testing, Artificial Intelligence 

> Islem Bouzenia, Michael Pradel

> University of Stuttgart, Germany 

## Background Problem

本工作的出发点是解决软件项目测试套件执行的实际挑战。软件测试套件执行在许多场景中至关重要，例如评估代码质量、代码覆盖率、验证开发人员或自动化工具的代码更改，以及确保与依赖项的兼容性。然而，由于不同项目使用不同的编程语言、软件生态、构建系统、测试框架和其他工具，这使得创建可靠的、通用测试执行方法变得困难。现有方法包括手动执行（耗时且不易扩展）、复用CI/CD工作流（可能依赖特定平台且不总是可用）、或使用特定语言的启发式脚本（缺乏灵活性），这些都无法有效处理任意项目的测试执行。本文的关键问题是通过自动化方式解决这些挑战，实现对任意项目的测试套件执行。

## Method

ExecutionAgent是一个基于大型语言模型（LLM）的代理，核心思想是模拟人类开发者的决策过程，通过自主执行命令和系统交互来自动构建项目并运行其测试。具体实现包括两个主要阶段：准备阶段和反馈循环。准备阶段使用meta-prompting（一种新颖的概念）向LLM查询最新技术指导，包括语言特定指导方针（如Java或Python的安装和测试步骤）、容器化技术建议（如Docker的使用）、CI/CD脚本常见位置，并结合网络搜索获取项目特定安装提示。反馈循环中，代理迭代地通过LLM选择下一个命令、执行命令（如终端操作、文件读写）、总结输出，并基于反馈更新提示，直到测试成功。关键步骤如算法1所示，涉及命令执行、输出总结和提示动态更新，确保代理在不修改原始模型的情况下高效工作。

## Experiment

实验在50个开源项目上进行，这些项目覆盖14种编程语言和多种构建及测试工具。数据集选择考虑了语言多样性、测试结果的ground truth可用性（如通过CI/CD日志获取）、项目活跃度（至少100个星标和100个提交）。实验设置全面合理，包括与基线方法（如LLM生成的通用脚本、AutoGPT和Flapy）的比较。结果显示，ExecutionAgent成功构建并执行了33/50个项目的测试套件，与ground truth的测试结果偏差平均为7.5%，其中29个项目的偏差小于10%。与最佳现有技术相比，成功率提高了6.6倍。成本分析显示，平均执行时间为74分钟，LLM使用成本为0.16美元。消融研究证实了各组件的重要性，如移除准备阶段或反馈循环会显著降低性能。整体结果符合预期，证明了方法的有效性和鲁棒性。

## Further Thoughts 

本文的meta-prompting概念特别值得关注，它允许代理动态获取最新技术指导，而非静态硬编码，这可以扩展到其他领域，如自动代码生成或程序修复中，提升LLM的适应性。迭代反馈机制也提供灵感，可与其他AI代理结合，例如在SWE-bench基准测试中用于验证代码修改的正确性；未来可能探索与强化学习集成，以更好地处理复杂依赖和错误恢复，拓宽LLM在软件工程自动化的应用。