---
id: 2505.00557
title: "Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models"
slug: triggering-hallucinations-in-llms
description: 本文提出了一种基于提示的框架，通过合成融合语义上遥远的概念来系统触发和量化大型语言模型中的幻觉，并通过跨模型实验揭示了模型特异性和语义融合的作用，从而为开发更安全的 LLMs 提供可重复的测试平台。
tags: ["Prompt Induced Hallucination","Hallucination Inducing Prompts","Hallucination Quantifying Prompts","Prompt Engineering","Conceptual Fusion","Human-AI Collaboration"]
author: grok-3-mini-latest
pubDatetime: 2025-05-04T01:18:54.974413+00:00
preference: unknown
featured: false
draft: false
---

> 本文提出了一种基于提示的框架，通过合成融合语义上遥远的概念来系统触发和量化大型语言模型中的幻觉，并通过跨模型实验揭示了模型特异性和语义融合的作用，从而为开发更安全的 LLMs 提供可重复的测试平台。

> Prompt Induced Hallucination, Hallucination Inducing Prompts, Hallucination Quantifying Prompts, Prompt Engineering, Conceptual Fusion, Human-AI Collaboration 

> Makoto Sato

> Kanazawa University 

## Background Problem

大型语言模型（LLMs）中的幻觉问题在现实应用中（如医疗、法律和教育领域）构成重大挑战，因为这些应用需要事实可靠性。尽管通过对齐和指令微调等方法取得了进展，LLMs 仍可能生成流畅但不真实的输出。研究背景是理解幻觉的认知动态：幻觉并非随机发生，而是受特定语义条件影响，如长程依赖错误、训练数据稀疏、过度自信的采样或提示表述与用户意图的不匹配。本工作的出发点是通过系统的方法触发和量化幻觉，以揭示其机制，并为开发更安全、更内省的 LLMs 提供基础。

## Method

本研究的核心方法是设计一种提示框架，包括幻觉诱导提示（HIPs）和幻觉量化提示（HQPs）。HIPs 通过合成融合语义上遥远的概念（如元素周期表和塔罗占卜）来制造误导性提示，长度控制在约 30 个标记，分为融合型（HIPc）和非融合型（HIPn）。HQPs 用于评估响应中的事实一致性、置信度和连贯性，使用独立 LLMs（如 GPT-o3）在 0-10 规模上打分，并提供理由。此外，还设计了过渡诱导提示（TIPcs）来比较语义融合的影响。实验采用无状态会话方法，确保每个提示在新鲜会话中独立进行，并使用统计分析（如 Welch's t-测试）评估差异。主要步骤包括：1. 设计 HIPs 和 HQPs；2. 在多个 LLMs 上应用提示；3. 量化并分析响应。

## Experiment

实验在六种 LLMs 上进行，包括通用模型（如 ChatGPT-4o、Gemini2.0Flash、DeepSeek）和推理导向模型（如 ChatGPT-o3、Gemini2.5Pro、DeepSeek-R1）。数据集包括 HIPc、HIPn 和 TIPcs 提示，每个提示重复多次（n=15 或 10）。实验设置合理，采用无状态会话避免历史影响，并使用 HQPs 量化幻觉分数。结果显示：HIPs 显著诱导幻觉，而非融合提示（HIPn）分数较低；推理导向模型（如 DeepSeek-R1）比通用模型更易受影响（幻觉分数更高）。例如，Gemini2.5Pro 显示出较低幻觉分数，因为它拒绝投机响应。这与预期一致，证明语义融合是幻觉的关键因素。实验全面，统计显著性（p < 0.05）支持结论，但依赖于外部评估，可能受评测模型偏差影响。

## Further Thoughts 

这项研究深刻启发了我们思考 LLMs 的认知边界，例如，将概念混合理论（Fauconnier & Turner, 2002）与神经科学相结合，可能揭示 LLMs 在处理语义冲突时的局限性；未来，可以探索内部信号分析（如语义熵，Farquhar et al., 2024）来实时检测幻觉，提供自监控机制；此外，在人机协作领域，这提示我们设计更鲁棒的提示工程，以防范潜在的安全风险，并与其他研究（如 Turpin et al., 2023 的链式推理问题）整合，构建更全面的 AI 安全框架。