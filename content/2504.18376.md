---
title: "Pushing the boundary on Natural Language Inference"
slug: "2504.18376"
description: "\u672c\u6587\u63d0\u51fa\u4f7f\u7528Group Relative Policy Optimization\u7ed3\u5408Chain-of-Thought\u5b66\u4e60\u7684\u65b9\u6cd5\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u65e0\u9700\u6807\u6ce8\u63a8\u7406\u8def\u5f84\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5728\u5bf9\u6297\u6027\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7ed3\u679c\u3002"
tags: ["Natural Language Inference", "LLMs", "GRPO", "Chain-of-Thought", "Reinforcement Learning", "LoRA", "Adversarial Benchmarks"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:30:46.264097+00:00
preference: "unknown"
score: 0.5651388284809683
featured: false
draft: false
---

> 本文提出使用Group Relative Policy Optimization结合Chain-of-Thought学习的方法提升自然语言推理任务的性能，无需标注推理路径，通过参数高效微调在对抗性基准上实现最先进结果。

> Natural Language Inference, LLMs, GRPO, Chain-of-Thought, Reinforcement Learning, LoRA, Adversarial Benchmarks 

> Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho

> Technical University of Madrid 

## Background Problem

自然语言推理（NLI）是自然语言理解的核心任务，具有事实检查、问答和信息检索等应用。尽管其重要性，但当前NLI系统主要依赖监督学习，训练数据往往包含标注偏差和人工制品，导致模型泛化能力差和实际应用受限。本文的工作起点是使用强化学习方法，特别是Group Relative Policy Optimization（GRPO）结合Chain-of-Thought（CoT）学习来处理NLI任务，消除了对标注推理路径的依赖，从而能够在更具挑战性的数据集如ANLI上进行训练，解决了现有方法在数据质量和泛化方面的关键问题。

## Method

本文的方法核心是应用Group Relative Policy Optimization（GRPO）算法进行NLI任务的Chain-of-Thought学习。具体实现包括：将NLI表述为文本到文本问题，使用特定提示模板（如SYSTEM和USER指令，强制模型生成推理步骤后给出预测）；GRPO优化目标为$$\mathcal{L}_{\text{GRPO}}(\theta) = \frac{1}{G} \sum_{i=1}^{G} \left[ \min\left(\frac{\pi_{\theta}(o_{i}|p)}{\pi_{\text{old}}(o_{i}|p)} A_{i}, \text{clip}\left(\frac{\pi_{\theta}(o_{i}|p)}{\pi_{\text{old}}(o_{i}|p)}, 1 - \epsilon, 1 + \epsilon\right) A_{i}\right) \right] - \beta \, \text{KL}(\pi_{\theta} \| \pi_{\text{ref}})$$，其中$A_i$是归一化优势函数，$\epsilon$控制剪切范围，$\beta$是KL散度正则化系数，以防止模型偏离基线分布；无需人工标注推理路径，通过在线采样和奖励函数（如准确性奖励）实现训练；采用参数高效微调技术LoRA和QLoRA，在7B、14B和32B模型上应用。

## Experiment

实验使用Qwen2.5系列模型（7B、14B、32B），采用AWQ量化（4位）和LoRA/QLoRA微调，数据集包括标准基准（如SNLI、MultiNLI）和对抗性基准（如ANLI、Counter NLI、HANS、NLI Diagnostic）。实验设置全面，涵盖模型大小、LoRA秩（8到128）、量化影响和训练动态分析。结果显示GRPO训练显著提升了模型在对抗性数据集上的性能，例如32B AWQ量化模型在11个对抗性子集中的7个上达到最先进水平，内存占用仅22GB；平均准确率提升明显（如ANLI R3从53.58%到71.75%），量化后性能损失小（平均下降2.95%），LoRA秩增加时性能趋于稳定；实验结果符合预期，证明了方法的鲁棒性和高效性，同时通过消融实验和输出分析验证了泛化能力。

## Further Thoughts 

本文的GRPO方法在NLI任务中展示了强化学习在提升推理能力方面的潜力，值得扩展到其他领域如数学推理或代码生成中，与DeepSeek-R1类似；基模型质量的重要性突出，提示使用更大规模预训练模型来避免监督学习的偏差；未来可探索与其他数据集（如WANLI或合成数据）的结合，或优化KL散度权重以平衡探索与稳定性；此外，量化技术的应用（如AWQ）提供高效部署思路，但需关注潜在信息损失对复杂推理的影响，与Kavumba et al.的工作相比，GRPO的无监督优势可能在多模态任务中更具泛化性。