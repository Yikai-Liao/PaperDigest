---
title: "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation"
slug: "2503.04606"
description: "\u672c\u6587\u63d0\u51fa LanDiff \u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002"
tags: ["Text-to-Video", "Language Model", "Diffusion Model", "Semantic Tokenization", "Video Generation"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:27:11.936100+00:00
preference: "unknown"
score: 0.5812358273589316
featured: false
draft: false
---

> 本文提出 LanDiff 框架，通过整合语言模型和扩散模型的优点，实现高效的文本到视频生成，显著提升了视频的语义一致性和视觉质量。

> Text-to-Video, Language Model, Diffusion Model, Semantic Tokenization, Video Generation 

> Aoxiong Yin, Kai Shen, Yichong Leng, Xu Tan, Xinyu Zhou, Juncheng Li, Siliang Tang

> Zhejiang University, Moonshot AI 

## Background Problem

文本到视频（T2V）生成领域最近取得了显著进展，但主要依赖两种范式：自回归语言模型和扩散模型。每种范式都有其固有局限性：语言模型在视觉质量和错误积累方面表现较差，而扩散模型缺乏语义理解和因果建模能力。本工作的出发点是整合两种范式的优势，解决这些问题。具体来说，语言模型基于离散标记化，能够明确编码高层语义并确保叙事连贯性，但由于信息压缩导致视觉保真度较低；扩散模型使用连续潜在表示来保留更多感知细节，从而实现高重建质量，但缺乏语义可解释性和因果约束，可能导致时间不一致或语义幻觉。此外，语言模型的自回归建模强化了因果依赖性，但容易出现错误传播，而扩散模型的非自回归生成减少了错误积累，但缺少显式因果约束。

## Method

* **核心思想：** LanDiff 是一个混合框架，通过粗到细的生成范式结合语言模型和扩散模型的优势。具体实现为两阶段生成过程：第一阶段使用语言模型生成高层语义标记，第二阶段使用扩散模型细化这些语义以产生高保真视频。
* **关键组件：** (1) 语义标记化器：将 3D 视觉特征压缩成紧凑的 1D 离散表示，采用 Theia 模型提取语义丰富的特征，并通过查询-based 因果标记化和视频帧分组（受 MP4 启发）减少空间和时间冗余，实现约 14,000 倍的压缩比；(2) 语言模型：基于文本生成语义标记，使用类似 LLaMA 的结构，自回归建模语义序列，并引入控制条件如帧数和运动分数；(3) 流式扩散模型：以语义标记为条件，通过块-wise 流式策略逐步去除噪声生成感知特征，支持长视频生成。
* **主要步骤：** 首先，提取文本嵌入和视频语义特征；然后，语言模型生成离散语义标记；接着，扩散模型以这些标记为条件细化生成视频潜在特征；最后，使用 VAE 解码器转换为 RGB 视频。公式包括标记化过程：$Z_Q = \text{Enc}([F; Q])$ 和重建过程：$\hat{F} = \text{Dec}([M; \hat{Z}_Q])$，损失函数为 $\mathcal{L} = \lambda_{\text{rec}} \|\hat{F} - F\|_2 + \lambda_{\text{commit}} \|\text{sg}(\hat{Z}_Q) - Z_Q\|_2$。

## Experiment

* **数据集和设置：** 使用内部数据集训练：语义标记化和语言模型使用 200M 视频-文本对，视频时长小于 6 秒，缩放至约 480x720 分辨率，帧率设为 8；扩散模型使用 3M 高质量视频-文本对。实验在 VBench T2V 基准上评估，包括短视频和长视频生成。基线模型包括 Sora、Kling、Hunyuan Video 等。
* **实验设计：** 比较 LanDiff 与其他模型在多个指标上的性能，如总分、质量分、语义分。消融实验验证了语义标记化和无分类器引导的影响。推理使用流式策略，支持长视频生成。
* **结果和分析：** LanDiff 5B 模型在 VBench 上得分 85.43，超过开源模型 Hunyuan Video (13B) 和商业模型如 Sora。长视频生成也达到最先进水平。在定性比较中，LanDiff 更好地捕捉语义一致性和动态变化，例如鱼类实体保持完整和冰雕融化过程。结果符合预期，证明了混合框架的优势：与相同数据和大小的扩散模型相比，LanDiff 在质量和语义上均有显著提升，实验设置全面合理。

## Further Thoughts 

这项工作展示了混合模型在多模态生成中的潜力，例如可以将类似方法扩展到音频或 3D 生成任务中，通过分层处理高层语义和低层细节来提高效率；此外，语义标记化的压缩策略可能启发其他领域如长序列处理或高效压缩，未来可以探索与更先进的基础模型结合，以进一步提升泛化能力和可控性。