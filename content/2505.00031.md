---
title: "Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving"
slug: "2505.00031"
description: "\u672c\u6587\u63d0\u51faLEPA\u81ea\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3LLM\u751f\u6210\u9884\u671f\u8ba1\u5212\u4f5c\u4e3a\u62bd\u8c61\u5143\u77e5\u8bc6\u6765\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"
tags: ["Large Language Model", "Self-Training", "Anticipatory Plan", "Meta-Learning", "Reasoning", "Cognitive Science"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:33:07.280106+00:00
preference: "unknown"
score: 0.7484415848142035
featured: false
draft: false
---

> 本文提出LEPA自训练算法，通过训练LLM生成预期计划作为抽象元知识来提升问题解决泛化能力，并在多个推理基准上显著优于现有方法。

> Large Language Model, Self-Training, Anticipatory Plan, Meta-Learning, Reasoning, Cognitive Science 

> Jin Zhang, Flood Sung, Zhilin Yang, Yang Gao, Chongjie Zhang

> Tsinghua University, Moonshot AI, Washington University in St. Louis 

## Background Problem

在大型语言模型（LLM）的后训练中，使用LLM自身生成的合成数据已被证明有效，但关键问题是如何选择这些数据包含的必要信息。现有自训练方法仅生成逐步问题解决方案，训练LLM最大化生成这些解决方案的似然，从而仅让LLM记忆任务特定解决方案，而未能捕获用于泛化的高层抽象元知识，导致在困难的自然语言推理任务（如Hendrycks MATH）上表现有限。本文受认知科学和元学习研究的启发，提出通过学习高层抽象元知识来解决这一问题，人类和元学习算法通过抽象简化复杂问题并提升泛化能力。

## Method

*   **核心思想:** LEPA是一种自训练算法，训练LLM在生成详细问题解决方案前创建预期计划，这些计划作为高层抽象元知识，指导解决方案生成并减少 distractions。
*   **如何实现:** 在数据生成阶段，LEPA提示LLM首先基于问题生成一个预期计划（高层次、通用、避免问题特定细节），然后生成与计划和问题一致的解决方案。如果解决方案正确，则存储计划-解决方案对；否则，通过自反省机制优化计划，LLM分析失败原因并生成新计划，直至解决方案正确或达到最大尝试次数。在模型优化阶段，使用监督微调（SFT）最小化负对数似然损失：$$\mathcal{L}_{SFT}(\theta_t, \mathcal{D}_{train}^t) = -\mathbb{E}_{\{x_i, p_i^t, y_i^t\} \sim \mathcal{D}_{train}^t} [\log p_{\theta_t}(p_i^t, y_i^t | x_i)]$$，训练LLM预测优化后的计划和解决方案。关键步骤包括计划生成、解决方案生成、自反省和SFT优化，不修改原始模型，仅在推理时调整采样过程。

## Experiment

*   **实验设置:** 使用数据集包括Hendrycks MATH（数学推理）、Hellaswag（句子完成推理）、BoolQ（段落理解推理）和PIQA（物理推理）。初始模型为Llama 3 8B Instruct，与基线方法ReST、ReST EM和STaR比较，所有方法在相同条件下（如相同尝试次数、温度参数）进行，评估收敛后的测试准确率。设计这样是为了公平比较LEPA与基线的性能，并通过消融研究验证LEPA组件的有效性（如预期计划、自反省）。
*   **为什么这样组织:** 选择这些基准覆盖不同推理类型，测试LEPA的泛化能力；基线方法仅生成逐步解决方案，而LEPA引入计划，预期能提升性能。结果显示LEPA在所有基准上显著优于基线，平均准确率提高3.1%（例如Hendrycks MATH上从28.2%提高到30.2%），学习曲线显示LEPA收敛更快。消融研究确认预期计划和自反省的必要性，额外分析显示LEPA更有效地利用推理计算资源，结果符合预期，证明LEPA通过学习抽象计划提高了LLM的泛化和推理能力。

## Further Thoughts 

LEPA展示了学习抽象计划如何提升LLM的泛化能力，未来可探索与其他优化算法如强化学习（RL）结合，以进一步提高性能；此外，将此方法扩展到决策系统或多模态任务中，可能带来新见解，例如在机器人规划或医疗诊断中利用计划减少错误；结合元学习框架，可以开发更高效的自适应模型；论文的自反省机制也可应用于其他AI领域，如自然语言生成中的一致性检查，或与Quiet-STaR等方法整合，探索更高级的元知识提取策略。