---
title: "Training Plug-n-Play Knowledge Modules with Deep Context Distillation"
slug: "2503.08727"
description: "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6df1\u5ea6\u4e0a\u4e0b\u6587\u84b8\u998f\u8bad\u7ec3\u53ef\u63d2\u62d4\u77e5\u8bc6\u6a21\u5757\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u9ad8\u6548\u6574\u5408\u6587\u6863\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4e14\u4e0e RAG \u5177\u6709\u534f\u540c\u6548\u5e94\u3002"
tags: ["LLM", "Knowledge Module", "Deep Context Distillation", "LoRA", "RAG", "Synthetic Data", "Question Answering"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:28:09.998715+00:00
preference: "unknown"
score: 0.6906043784729547
featured: false
draft: false
---

> 本文提出使用深度上下文蒸馏训练可插拔知识模块的方法，能够在低数据场景下高效整合文档知识，并通过实验证明其在问答任务中优于传统方法且与 RAG 具有协同效应。

> LLM, Knowledge Module, Deep Context Distillation, LoRA, RAG, Synthetic Data, Question Answering 

> Lucas Caccia, Alan Ansell, Edoardo Ponti, Ivan Vulić, Alessandro Sordoni

> Microsoft Research Montreal, University of Cambridge, University of Edinburgh 

## Background Problem

大型语言模型（LLM）在海量语料上预训练后，能够捕获广泛的语言和事实知识，但事后整合新信息或快速演变的信息（如私有或专业文档）在低数据场景中仍具挑战性。在上下文化习和检索增强生成（RAG）方法存在局限性，包括高推理成本、无法捕获全局文档信息，以及仅处理局部段落；此外，持续预训练使用下一 token 预测在低数据条件下效果不佳。本文的工作起点是开发一种即插即用、参数高效的方法，以在不牺牲模型泛化能力的情况下，高效编码文档知识，支持企业场景（如处理最新政策或产品细节）和科学发现（如整合前沿出版物）。

## Method

*   **核心思想：** 本文提出深度上下文蒸馏（Deep Context Distillation，DCD）方法，通过知识蒸馏训练知识模块（Knowledge Modules，KMs），这些模块使用 LoRA（Low-Rank Adaptation）参数高效适配器来压缩文档知识，使其能在推理时即插即用，而不需文档上下文。
*   **实现方式：** KMs 被参数化为 LoRA 模块，优化以匹配教师模型（具有文档上下文）的隐藏状态和输出概率。损失函数包括 KL 散度损失（匹配输出概率）和 L1 范数损失（匹配隐藏状态），公式为：
    $$\mathcal{L}_{\text{DCD}} = \min_{\theta_{\text{KM}}} \text{KL}\left( p(\mathbb{C}_{k+1}|\mathbb{C}_{k}) \parallel p(\mathbb{C}_{k+1}; \theta_{\text{KM}}) \right) + \sum_{l} \frac{1}{Z^{l}} \left\| h^{l}_{\mathbb{C}_{k+1}|\mathbb{C}_{k}} - h^{l}_{\mathbb{C}_{k+1}; \theta_{\text{KM}}} \right\|_{1}$$
    文档 DCD 使用文档自身 chunk，合成 DCD 使用从文档生成的合成数据（如摘要、问答对或 Entigraph）。此外，知识提取器（Knowledge Extractors，KE）可进一步训练以适应特定任务，通过可学习权重组合 KMs 和 KE。
*   **主要步骤：** 1. 将文档拆分成 chunk；2. 使用教师模型生成合成数据；3. 通过梯度下降优化 KMs 参数以最小化 DCD 损失；4. 在推理时加载 KMs 和可选 KE 以处理查询。

## Experiment

*   **数据集和模型：** 实验使用 QuALITY（多选问答数据集，平均文档长度约 5,000 tokens）和 NarrativeQA（问答数据集，平均文档长度约 60,000 tokens）数据集，以及 Phi-3 3B 和 Llama-3.1 8B 指令微调模型。
*   **实验设置：** 包括闭卷（无文档上下文）和开卷（有文档上下文）评估。闭卷评估比较 KMs 的不同训练方法（如 LM 损失、DCD 变体、PIT）；开卷评估结合 RAG 基线。所有 KMs 和 KE 使用 LoRA 适配器（秩 16），训练 1500 步，批量大小 8。结果使用 NarrativeQA 的 Rouge-L 和 QuALITY 的准确率评估。
*   **结果分析：** 在闭卷设置中，合成 DCD（使用摘要和问答对）显著优于 LM 和 PIT 基线，例如在 NarrativeQA 上，Phi-3 的 Rouge-L 从 15.2（LM）提升到 25.8（合成 DCD + KE）。开卷设置中，RAG 与 KMs 结合时显示协同效应，RAG + KM + KE 比 RAG + KE 改善 4.2 和 4.1 Rouge-L（NarrativeQA）和 2.4% 和 4.1% 准确率（QuALITY）。消融实验证实隐藏状态匹配和更多合成数据提升性能，实验设计全面合理，结果符合预期，证明 DCD 在低数据条件下有效。

## Further Thoughts 

这个模块化方法强调了知识注入的灵活性，可能在隐私保护和高效推理中发挥更大作用，例如与联邦学习结合以处理分布式数据，或与知识图谱（如 GraphRAG）整合以捕获更复杂的实体关系；此外，未来可以探索高效的 KM 初始化策略或与零样本路由方法的结合，实现跨文档知识动态组合，并扩展到多模态数据或实时更新场景，以进一步提升 AI 系统在科学和企业应用中的泛化能力。