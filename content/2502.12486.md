---
title: "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning"
slug: "2502.12486"
description: "\u672c\u6587\u63d0\u51faEPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e00\u4e2a\u4e13\u95e8\u7684\u6218\u7565\u63a8\u7406\u6a21\u578b\uff0c\u8f85\u52a9\u4efb\u610fLLM\u4ee3\u7406\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u957f\u671f\u76ee\u6807\u5bf9\u9f50\uff0c\u63d0\u5347\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002"
tags: ["LLM", "Strategic Reasoning", "Reinforcement Learning", "Policy Optimization", "Self-Play"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:27:01.732600+00:00
preference: "unknown"
score: 0.6029010248350041
featured: false
draft: false
---

> 本文提出EPO方法，通过强化学习优化一个专门的战略推理模型，辅助任意LLM代理在动态环境中实现长期目标对齐，提升战略推理能力。

> LLM, Strategic Reasoning, Reinforcement Learning, Policy Optimization, Self-Play 

> Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang

> University of Chinese Academy of Sciences, Tongyi Lab, Institute of Automation, Chinese Academy of Sciences 

## Background Problem

大型语言模型（LLMs）在静态问题（如数学和编码）上表现出色，但在大语言模型中处理动态、真实世界场景（如商业谈判）的战略推理能力不足，这些场景需要处理不确定性、长期目标对齐和环境适应。现有方法包括迭代提示、模仿学习（IL）或强化学习（RL）训练，以及推理路径搜索，但它们面临适应性差、泛化能力弱和计算效率低的问题，本文的工作起点是提出一种方法来提升LLMs在动态交互环境中的战略推理能力。

## Method

本文提出显式策略优化（EPO）方法，使用一个专门的语言模型（LLM^s）来提供实时策略，辅助另一个LLM代理（LLM^d）实现目标导向行为。具体实现包括：
- LLM^s根据系统提示、目标G、交互历史h_{1:t-1}、先前策略a_{1:t-1}和当前观察x_t生成策略a_t：$$a_t = LLM_s(s_{sys}, G, h_{1:t-1}, a_{1:t-1}, x_t).$$
- LLM^d基于策略生成行为y_t：$$y_t = LLM_d(d_{sys}, G, h_{1:t-1}, a_{1:t}, x_t).$$
- 通过多轮强化学习（RL）优化LLM^s的政策，使用REINFORCE算法的目标函数：$$J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)] = \mathbb{E}_{\pi_{\theta}}[\sum_{t=1}^{T} r_t],$$
  损失函数为：$$\mathcal{L}(\theta) = - \mathbb{E}_{\pi_{\theta}} \left[ \frac{1}{T} \sum_{t=1}^{T} A_t \frac{1}{|k_t|} \sum_{i=0}^{k_t} \log \pi_{\theta}(a_{t,i} | h_{1:t-1}, a_{t,1:i-1}, x_t) \right],$$
  其中A_t是优势函数，r_t是过程奖励，由过程奖励模型（PRM）评估关键策略。还引入迭代自博弈来扩展RL训练，确保策略的适应性和可转移性，而不修改LLM^d。

## Experiment

实验在社会和物理领域进行，包括SOTOPIA（社交对话）、WebShop（网页导航）和ALFWorld（具身任务）数据集。实验设置合理全面，使用零样本或一样本提示评估，指标包括目标完成度和平均奖励。结果显示EPO在保持LLM^d泛化能力的同时，通过RL优化LLM^s显著提升性能，如在SOTOPIA上超越基线方法，平均目标完成度提高；消融实验确认RL、过程奖励和自博弈组件的关键性；分析揭示了协作推理机制和新兴策略，实验结果与预期一致，证明了EPO在长期目标对齐和战略推理方面的有效性。

## Further Thoughts 

本文的EPO方法强调了战略推理模型的模块化设计，这可能扩展到多代理环境如Diplomacy游戏中，进一步提升LLMs在复杂社会互动中的表现；同时，结合更先进的奖励模型或值函数估计（如PPO算法）可能提高训练稳定性，并探索将EPO应用于真实世界AI代理，如自动谈判系统或游戏AI，以实现更强的泛化能力和人机协作潜力。