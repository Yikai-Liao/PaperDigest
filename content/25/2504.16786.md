---
title: "MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores"
slug: "2504.16786"
description: "本文提出MOOSComp方法，通过在训练中添加inter-class cosine similarity loss缓解over-smoothing问题，并在压缩中整合outlier分数保留关键token，显著提升了任务无关的长上下文压缩性能和泛化能力。"
tags: ["Large Language Models", "Long-Context Compression", "Over-Smoothing", "Outlier Detection", "BERT", "Prompt Compression"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:29:46.232827+00:00
preference: "unknown"
score: 0.7767639042152299
featured: false
draft: false
---

> 本文提出MOOSComp方法，通过在训练中添加inter-class cosine similarity loss缓解over-smoothing问题，并在压缩中整合outlier分数保留关键token，显著提升了任务无关的长上下文压缩性能和泛化能力。

> Large Language Models, Long-Context Compression, Over-Smoothing, Outlier Detection, BERT, Prompt Compression 

> Fengwei Zhou, Jiafei Song, Wenjin Jason Li, Gengjian Xue, Zhikang Zhao, Yichao Lu, Bailin Na

> OPPO CTG 

## Background Problem

大型语言模型（LLMs）在处理长上下文输入时性能显著提升，但面临推理时间延长和资源消耗增加的挑战，尤其在资源受限的环境中，如边缘设备上。现有长上下文压缩方法存在过平滑（over-smoothing）问题，导致BERT-based模型的token表示相似性过高，影响token分类准确性；同时，任务无关压缩方法可能丢弃稀有但关键的token，降低泛化能力。本工作旨在通过改进任务无关的硬提示压缩方法来解决这些问题，提高压缩效率和效果。

## Method

核心思想是通过缓解over-smoothing问题和整合outlier分数来提升BERT-based长上下文压缩器的性能。实现方式包括：训练阶段添加inter-class cosine similarity loss来惩罚token表示之间的过高相似度，从而提高token分类准确性；压缩阶段引入outlier分数（基于Z-score计算），与分类器输出概率结合，以保留稀有但重要的token。主要步骤：在训练时优化损失函数$\mathcal{L}(\varphi, \psi) = \mathcal{L}_{\text{CE}}(\varphi, \psi) + \beta \mathcal{L}_{\text{CS}}(\varphi)$，其中$\mathcal{L}_{\text{CS}}$是inter-class cosine similarity loss，公式为$\mathcal{L}_{\text{CS}}(\varphi) := S^L = \frac{1}{|I_p||I_d|} \sum_{i \in I_p, j \in I_d} \frac{h_i^L \cdot h_j^L}{||h_i^L||_2 ||h_j^L||_2}$；在压缩时计算标准化outlier分数并整合到压缩指标中，公式为$m_k = \alpha p_k^{\text{preserve}} + (1 - \alpha)s_k^{\text{norm}}$。

## Experiment

实验使用数据集包括MeetingBank（in-domain总结任务）、LongBench（长上下文理解）、GSM8K（数学推理）和BBH（语言推理），目标模型涵盖黑箱API模型（如GPT-3.5-Turbo、GPT-4o-mini）和本地模型（如Qwen2.5-7B-Instruct、Qwen2.5-3B-Instruct）。实验设置旨在验证方法的有效性、泛化能力和加速效果：in-domain实验评估压缩后在相同任务上的性能，out-of-domain实验测试不同任务和模型的鲁棒性；结果显示MOOSComp在各种压缩比下均显著优于基线方法，如在LongBench上准确率提升10%以上，同时在资源受限设备上实现最高3.3倍加速；剔除实验确认了inter-class cosine similarity loss和outlier检测机制的贡献。实验设计合理全面，匹配预期，证明了方法的改进明显。

## Further Thoughts 

本方法通过针对性优化损失函数和引入outlier检测，展示了在提升模型泛化性方面的潜力，未来可探索将其扩展到其他transformer-based模型或多模态数据压缩中；over-smoothing问题在各种深度学习任务中普遍存在，或许能与其他反over-smoothing技术（如contrastive normalization）结合；outlier检测机制可进一步动态调整以适应实时任务变化，或与其他压缩策略（如soft prompt方法）融合，以实现更高效的LLM部署。