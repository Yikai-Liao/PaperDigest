---
title: "An Empirical Study on Prompt Compression for Large Language Models"
slug: "2505.00019"
description: "本文通过对六种提示压缩方法的实证研究，全面评估了它们在LLMs上的性能影响，包括任务表现、幻觉增加和多模态适用性，并提供了开源工具包以促进进一步研究。"
tags: ["Prompt Compression", "Large Language Models", "Hallucination", "Multimodal Tasks", "Compression Ratio"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:32:47.504172+00:00
preference: "dislike"
score: 0.8469181086790144
featured: false
draft: true
---

> 本文通过对六种提示压缩方法的实证研究，全面评估了它们在LLMs上的性能影响，包括任务表现、幻觉增加和多模态适用性，并提供了开源工具包以促进进一步研究。

> Prompt Compression, Large Language Models, Hallucination, Multimodal Tasks, Compression Ratio 

> Zheng Zhang, Jinyi Li, Yihuai Lan, Xiang Wang, Hao Wang

> The Hong Kong University of Science and Technology (Guangzhou), South China University of Technology, University of Science and Technology of China 

## Background Problem

本研究的出发点是解决大型语言模型（LLMs）在提示工程中的问题。LLMs通过提示工程（如CoT、ICL和RAG）能够适应各种任务，但长提示会显著增加计算复杂性和经济成本（如API调用费用）。以往研究主要关注压缩后模型在任务性能（如准确率、BLEU、ROUGE和BERTScore）上的表现，但忽略了对其他方面的影响，例如模型的泛化能力、幻觉问题以及在多模态任务中的适用性。此外，提示压缩中哪些词可以被省略也缺乏深入探讨。因此，本文旨在通过全面分析不同提示压缩方法，探讨其对LLMs输出质量、幻觉、响应长度以及多模态任务的影响。

## Method

*   **核心思想：** 本文研究了六种提示压缩方法，旨在减少提示长度同时保留关键信息。这些方法分为三类：基于强化学习的（KiS和SCRL）、基于LLM打分的（Selective Context）、以及基于LLM标注的（LLMLingua、LongLLMLingua和LLMLingua-2）。
*   **工作原理：** KiS使用强化学习生成简化的提示，优化流畅性、显著性和简单性；SCRL通过序列标注的强化学习策略标记并删除非必要词；Selective Context计算词的自信息来修剪冗余部分；LLMLingua采用粗到细的压缩策略，包括预算控制器、令牌级迭代压缩和指令微调；LongLLMLingua针对长上下文优化，引入问题感知压缩和文档重排序；LLMLingua-2通过数据蒸馏和Transformer编码器实现任务无关的令牌分类压缩。
*   **主要步骤：** 首先，根据压缩比定义（$\rho = 1 - \frac{L_c}{L_o}$）调整方法参数；然后，对输入提示进行处理，如KiS生成候选简化版本，SCRL标注必要性，Selective Context计算信息分值，其他方法使用LLM辅助压缩；最后，输出压缩后的提示，确保语义完整性。

## Experiment

*   **实验设置：** 本文使用GPT-3.5-turbo、GPT-4o-mini和Claude-3-Haiku三个LLM，在13个数据集上评估，包括新闻（Gigaword、DUC2004）、科学文章（Arxiv）、常识QA（BBH）、数学QA（GSM8K）、长上下文QA（LongBench）和视觉问答（VQA，IconQA和OK-VQA）。实验任务包括总结、重构和QA。指标涵盖BLEU、ROUGE、BERTScore、准确率、F1分数、微观幻觉率（MiHR）和宏观幻觉率（MaHR）。这种设置旨在全面评估压缩方法在不同上下文长度、任务类型和模态下的性能，并分析压缩比、响应长度和幻觉的影响。
*   **为什么这样设计：** 实验覆盖短长上下文、多模态任务和不同方面（如幻觉），以填补以往研究的空白。使用多个模型和数据集确保结果的鲁棒性，并通过基线（如随机选择）比较方法优劣。
*   **结果分析：** (Long)LLMLingua和LLMLingua-2在高压缩比下表现最佳，尤其在总结和长上下文QA任务中；压缩会增加幻觉，主要由于信息丢失；中度压缩可能提升长上下文性能；多模态任务中方法效果不一，SCRL计算效率最高。结果与预期一致，证明了问题感知机制在长上下文中的优势，并突出了信息丢失对幻觉的影响。

## Further Thoughts 

提示压缩的研究不仅能提升LLMs的计算效率，还启发我们思考在其他AI领域如视觉或多模态模型中的应用潜力，例如结合注意力机制优化以减少信息丢失；此外，幻觉增加问题可能与模型的内部表示相关，未来可探索与模型蒸馏或知识增强技术的整合，以实现更鲁棒的压缩策略；同时，本文对词省略分析的洞见（如常见词在长上下文中的作用）可能与Transformer架构的记忆机制类似，值得进一步研究以改进提示工程。