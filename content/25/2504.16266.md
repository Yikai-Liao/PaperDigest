---
title: "TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs"
slug: "2504.16266"
description: "本文提出TeLLMe，一种能量高效的三元LLM FPGA加速器，通过表查找矩阵引擎和反向注意力优化，支持预填充和解码阶段，在7W功率下实现高达9.51 tokens/s吞吐量和低预填充延迟。"
tags: ["LLM", "FPGA", "Ternary Quantization", "Hardware Acceleration", "Prefill Optimization", "Decode Phase", "Energy Efficiency"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:29:32.742844+00:00
preference: "unknown"
score: 0.7002065414573913
featured: false
draft: false
---

> 本文提出TeLLMe，一种能量高效的三元LLM FPGA加速器，通过表查找矩阵引擎和反向注意力优化，支持预填充和解码阶段，在7W功率下实现高达9.51 tokens/s吞吐量和低预填充延迟。

> LLM, FPGA, Ternary Quantization, Hardware Acceleration, Prefill Optimization, Decode Phase, Energy Efficiency 

> Ye Qiao, Zhiheng Chen, Yifan Zhang, Yian Wang, Sitao Huang

> University of California, Irvine 

## Background Problem

大型语言模型（LLMs）的快速发展使得它们在自然语言处理任务中表现出色，但边缘部署面临巨大挑战，包括高计算需求、高内存使用和能源消耗。低位量化方法（如BitNet和DeepSeek）将权重压缩到1.58位，显著减少了模型大小和能源成本，但边缘设备（如FPGA）仍受限于芯片资源、功率预算和预填充阶段的延迟，而预填充延迟在边缘应用中是用户体验和安全的关键瓶颈。现有工作多关注模型量化或软件加速，但缺乏对极端低位宽LLMs的系统性硬件-软件协同优化，尤其是忽略了预填充阶段的需求，本文的工作起点是开发首个支持预填充和解码两个阶段的FPGA加速器，以实现高效、低延迟的边缘LLM推理。

## Method

* **核心思想：** TeLLMe是一种针对边缘FPGA的能量高效三元LLM加速器，旨在通过硬件-软件协同优化来支持预填充和解码阶段的端到端推理。核心在于最小化资源使用和数据移动，通过表查找矩阵乘法引擎、融合注意力模块和集成标准化/量化-反量化单元来实现高效计算。
* **工作原理：** 
  - **表查找矩阵乘法（TL-based Ternary MatMul）：** 利用FPGA的LUT资源，预处理权重为分组索引（每组G个三元值编码为索引），在线阶段通过表查找和并行累加计算激活和权重的乘积。例如，对于三元权重，公式为$\mathbf{A} \otimes \mathbf{W} = \mathbf{A} \otimes \mathbf{W}_0$（权重在\{-1, 0, 1\}范围内），通过分组激活和多表并行访问（如T表和Q并行度）减少重复计算和资源消耗，算法如算法1所示。
  - **融合注意力模块：** 对于预填充阶段，引入反向重排序方案（Reversed Attention），从序列末尾开始计算，避免因因果掩码导致的冗余计算，并融合Q-K乘法、Softmax和S-V乘法，减少DRAM访问。公式为：
    $$
    \begin{cases}
    m^{(1)} = s^{(1)} \\
    \ell^{(1)} = e^{s^{(1)} - m^{(1)}} \\
    \mathbf{o}^{(1)} = e^{s^{(1)} - m^{(1)}} \mathbf{v}^{(1)} \\
    m^{(2)} = \max\left(m^{(1)}, s^{(2)}\right) = m \\
    \ell^{(2)} = e^{m^{(1)} - m^{(2)}} \ell^{(1)} + e^{s^{(2)} - m^{(2)}} \\
    \mathbf{p}^{(2)} = e^{s^{(2)} - m^{(2)}} / \ell^{(2)} \\
    \mathbf{o}^{(2)} = \mathbf{o}^{(2)} / \ell^{(2)} = \mathbf{o}
    \end{cases}
    $$
    对于解码阶段，重用注意力硬件进行LM Head计算，采用解耦执行以减少资源。
  - **其他优化：** 融合RMSNorm和量化-反量化操作，减少数据移动；激活函数如SiLU与线性层融合以隐藏延迟。
* **主要步骤：** 包括权重预处理、在线表查找计算、注意力调度和功能单元融合，确保在FPGA上高效流水线执行。

## Experiment

* **实验设置：** 本文在AMD KV260 FPGA上实现TeLLMe加速器，使用Vitis HLS和Vivado 2023.1，频率250 MHz，功率预算7W。模型基于1.58位三元权重和8位激活的BitNet，测试上下文长度至1024 tokens。实验包括解码吞吐量（tokens/s）和预填充延迟（s），比较了不同提示长度（64-128 tokens）和生成长度。设置合理，考虑了FPGA资源约束和内存带宽限制。
* **结果分析：** TeLLMe在512 tokens上下文下达到9.51 tokens/s吞吐量，1024 tokens下约8 tokens/s；预填充延迟为64 tokens提示时0.55s，128 tokens时1.15s，符合预期，展示了在保持低功率下的高效性能。与基线比较（如SECDA、LlamaF）显示16.4倍改进，支持预填充是关键优势。实验全面，包含资源分解（LUT、FF、BRAM等）和与移动CPU（如Snapdragon 8 Gen 3）的对比，证明了方法的有效性和泛化性，结果与预期的能量效率和延迟优化相符。

## Further Thoughts 

这项工作突显了硬件-软件协同优化的重要性，或许可以扩展到其他硬件如ASIC或GPU上，进一步探索不同量化策略（如二元或混合量化）的硬件适应性；同时，结合其他研究（如DeepSeek的混合量化），TeLLMe的预填充优化思路可能启发更泛化的注意力机制设计，以提升长序列模型在资源受限环境中的性能，并推动边缘AI在隐私保护和实时应用中的发展。