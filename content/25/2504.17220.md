---
title: "Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?"
slug: "2504.17220"
description: "本文首次系统探索知识蒸馏技术在基于大语言模型的捆绑生成任务中的应用，通过提出一个全面的 KD 框架和实验验证，证明了在减少计算需求的同时能保持甚至提升性能。"
tags: ["Knowledge Distillation", "Large Language Models", "Bundle Generation", "Recommender Systems", "In-Context Learning", "Supervised Fine-Tuning"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:30:22.403530+00:00
preference: "unknown"
score: 0.7296874247992619
featured: false
draft: false
---

> 本文首次系统探索知识蒸馏技术在基于大语言模型的捆绑生成任务中的应用，通过提出一个全面的 KD 框架和实验验证，证明了在减少计算需求的同时能保持甚至提升性能。

> Knowledge Distillation, Large Language Models, Bundle Generation, Recommender Systems, In-Context Learning, Supervised Fine-Tuning 

> Kaidong Feng, Zhu Sun, Jie Yang, Hui Fang, Xinghua Qu, Wenyuan Liu

> Yanshan University, Singapore University of Technology and Design, Delft University of Technology, Shanghai University of Finance and Economics, Bytedance Seed 

## Background Problem

本研究针对大语言模型（LLMs）在捆绑生成任务中的应用背景进行探讨。LLMs 凭借其出色的推理能力和全面知识，在推荐系统中被广泛用于捆绑生成，但其大规模参数化导致了高计算成本问题，包括微调和推理过程中的资源消耗。知识蒸馏（KD）被视为一种有前景的解决方案，通过将知识从大型教师模型转移到更紧凑的学生模型，从而减少计算需求同时保持性能。本文提出三个关键研究问题：（1）蒸馏知识的格式如何影响捆绑生成性能？（2）蒸馏知识的数量对性能有何影响？（3）利用蒸馏知识的不同方式如何影响性能？这些问题旨在解决 LLMs 在捆绑生成中的效率挑战，同时确保生成质量。

## Method

本文提出一个全面的知识蒸馏框架，核心思想是通过渐进式知识提取、数量变化和利用方法优化来提升捆绑生成效率。主要步骤包括：（1）渐进式知识提取：从原始数据中提取 increasingly complex 的知识形式，包括频繁模式（使用 Apriori 算法挖掘项类别级别的共现模式）、形式化规则（通过教师 LLMs 的自反省方法生成显式规则）和深度思考（使用链式思考技术生成基于用户意图的推理路径）；（2）知识数量变化：通过不同采样策略（如随机、长度、 diversity 和 difficulty 基于采样）、多领域积累（聚合同一类型知识于多个领域）和多格式聚合（结合不同知识类型）来控制知识量；（3）知识利用方法：采用互补的 LLMs 适应技术，包括 in-context learning（ICL，通过知识检索增强提示）、supervised fine-tuning（SFT，通过知识增强的训练数据微调模型）和二者结合（在训练和推理阶段同时使用知识）。该框架不修改教师模型，仅在学生模型上应用 KD，以实现高效的捆绑生成。

## Experiment

实验使用三个真实世界数据集（Electronic、Clothing 和 Food，源自 SIGIR 2022），这些数据集包含用户会话、捆绑、意图和项信息。实验设置针对三个研究问题（RQ1-3）设计：（1）比较不同知识格式（原始数据、频繁模式、规则、思考）对 ICL 和 SFT 性能的影响；（2）探索知识数量变化，通过采样策略（比率 10%-70%）、多领域和多格式积累评估性能；（3）测试知识利用方法（ICL、SFT 和结合）。教师模型为 GPT-3.5-turbo，学生模型为 Llama3.1-8B，使用 Precision、Recall 和 Coverage 指标评估。结果显示：KD 显著提升 Precision 和 Coverage，但 Recall 较教师模型低；SFT 通常优于 ICL；增加知识量总体有益，ICL 更依赖高采样比，SFT 在特定比率和知识积累下表现最佳；利用方法对性能影响最大。实验设置全面合理，覆盖多种变量和数据集，结果与预期一致，验证了 KD 在减少计算成本的同时保持高生成质量的潜力。

## Further Thoughts 

本文的显式知识蒸馏框架虽然有效，但未来可探索隐式知识蒸馏（如对齐隐藏层表示）以捕获教师模型更细微的推理过程；此外，结合多模态数据（如图像）可能提升捆绑生成的鲁棒性，并与一般推荐系统（如序列推荐）整合，开发自适应知识选择机制以优化 KD 效率；同时，考虑到不同领域的数据异质性，跨领域知识转移的泛化能力值得进一步研究，以推动 AI 模型在资源受限环境下的部署。