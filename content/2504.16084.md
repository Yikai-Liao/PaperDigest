---
title: "TTRL: Test-Time Reinforcement Learning"
slug: "2504.16084"
description: "\u672c\u6587\u63d0\u51fa\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6570\u6295\u7968\u4f30\u8ba1\u5956\u52b1\uff0c\u5728\u65e0\u6807\u7b7e\u6d4b\u8bd5\u6570\u636e\u4e0a\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u6a21\u578b\u81ea\u6f14\u5316\u5e76\u663e\u8457\u63d0\u5347\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u3002"
tags: ["Test-Time Reinforcement Learning", "Majority Voting", "Reinforcement Learning", "Large Language Models", "Reasoning", "Unlabeled Data"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:30:27.467881+00:00
preference: "unknown"
score: 0.9349312337549779
featured: false
draft: false
---

> 本文提出测试时强化学习（TTRL）方法，通过多数投票估计奖励，在无标签测试数据上训练大语言模型，实现模型自演化并显著提升推理任务性能。

> Test-Time Reinforcement Learning, Majority Voting, Reinforcement Learning, Large Language Models, Reasoning, Unlabeled Data 

> Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, Bowen Zhou

> Tsinghua University, Shanghai AI Lab 

## Background Problem

本工作的出发点是解决大型语言模型（LLMs）在推理任务上进行强化学习（RL）时面临的挑战，即在测试时没有显式标签，无法获得准确的奖励信号。背景包括测试时缩放（TTS）技术的发展，表明测试时增加计算资源比预训练时更高效，但传统RL方法主要依赖训练时的数据，无法有效处理新出现的无标签测试数据。关键问题包括：如何在无标签数据上估计奖励，以及如何实现模型的自演化以适应分布偏移和新型任务，例如OpenAI o3在ARC-AGI-2上的低性能。TTRL旨在利用预训练模型的先验，通过测试时训练来提升模型性能，减少对人类标注的依赖。

## Method

核心思想是通过测试时强化学习（TTRL）在无标签数据上训练LLMs，实现模型的自演化。具体实现包括：给定输入提示x，模型通过策略πθ生成输出y；通过重复采样生成多个候选输出{y1, y2, ..., yN}，使用多数投票估计共识输出y*；基于y*计算奖励r(y, y*)，奖励函数为二值的规则-based奖励（如果输出匹配y*则奖励1，否则0）。RL目标是最大化期望奖励：
$$
\max_{\theta} \mathbb{E}_{y \sim \pi_{\theta}(\cdot|x)} [r(y, y^*)],
$$
通过梯度上升更新参数：
$$
\theta \leftarrow \theta + \eta \nabla_{\theta} \mathbb{E}_{y \sim \pi_{\theta}(\cdot|\mathbf{x})} [r(y, y^*)].
$$
主要步骤是采样、投票估计标签、计算奖励和参数更新，不修改预训练模型，只在测试时调整。

## Experiment

实验设置包括使用Qwen2.5-Math-1.5B、7B和LLaMA-3.1-8B-Instruct等模型，基准数据集为AIME 2024、AMC和MATH-500，基线包括DeepSeek-R1-Distill系列等RL模型。实验使用GRPO算法，学习率为5×10^{-7}，采样64个响应等超参数。结果显示TTRL显著提升性能，例如在AIME 2024上Qwen2.5-Math-7B的pass@1性能从13.3提升至43.3，增幅159%；平均在三个基准上提升84%。实验设计合理，覆盖不同模型规模、任务难度和RL算法（如PPO），验证了泛化性和兼容性，结果符合预期，因为TTRL利用多数投票奖励有效估计标签，即使在无标签设置下也能接近有标签训练的上限。

## Further Thoughts 

TTRL的创新在于利用自估计奖励实现无监督RL，这启发我们思考在其他领域如计算机视觉的测试时适应（Test-Time Adaptation）中应用类似机制，减少对标注数据的依赖；此外，TTRL的self-evolution概念可能与在线学习和终身学习相结合，推动AI代理在动态环境中持续改进；与相关工作如DeepSeek-R1的RL方法相比，TTRL强调测试时训练的效率，未来可探索与其他优化算法的整合，以提升鲁棒性和泛化能力。