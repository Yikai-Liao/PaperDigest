---
title: "Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation"
slug: "2505.00009"
description: "\u672c\u6587\u63d0\u51fa TA-LoRA \u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u4f4e\u79e9\u8868\u793a\u548c\u5feb\u901f-\u7f13\u6162\u6743\u91cd\u673a\u5236\u63d0\u5347\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u77e5\u8bc6\u8f6c\u79fb\u6548\u7387\uff0c\u5b9e\u73b0\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u4f18\u5f02\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53c2\u6570\u6548\u7387\u3002"
tags: ["Multi-Task Learning", "Prompt Tuning", "Low-Rank Representation", "Fast-Slow Weights", "Parameter Efficiency"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:32:05.313067+00:00
preference: "unknown"
score: 0.8785111144271905
featured: false
draft: false
---

> 本文提出 TA-LoRA 方法，通过任务自适应低秩表示和快速-缓慢权重机制提升多任务学习的知识转移效率，实现对未见任务的优异泛化性能，同时保持高参数效率。

> Multi-Task Learning, Prompt Tuning, Low-Rank Representation, Fast-Slow Weights, Parameter Efficiency 

> Xiao Zhang, Kangsheng Wang, Tianyu Hu, Huimin Ma

> University of Science and Technology Beijing 

## Background Problem

预训练语言模型（PLMs）在捕捉一般知识方面表现出色，但面对真实世界应用中的新任务时表现不佳，训练单独的模型成本高且无法有效利用跨任务知识。多任务学习（MTL）通过从源任务转移共享知识来提升对目标任务的泛化能力，而提示调优（PT）作为一种参数高效的细调方法，通过引入可学习的连续提示向量来编码任务特定知识，但由于其表示能力的限制，难以有效捕捉任务异质性（即任务间的差异），从而导致共享知识和任务特定知识混淆，阻碍了对未见任务的泛化。

## Method

*   **核心思想：** TA-LoRA 基于提示调优，引入低秩表示来捕捉任务异质性，并通过快速-缓慢权重机制分离共享知识和任务特定知识，避免训练过程中知识混杂。
*   **工作原理：** 对于多源任务集，TA-LoRA 将可适应提示向量分解为共享部分 $\theta^0$ 和任务特定部分，使用低秩矩阵 $B$ 和 $A_i$ 近似任务特定知识，其中 $B$ 是共享的缓慢权重，$A_i$ 是任务特定的快速权重，并进一步将 $A_i$ 分解为秩-1 矩阵 $u_i \otimes v_i$。训练时，$B$ 和 $A_i$ 采用不同的学习率以平衡梯度规模。同时，引入零初始化注意力机制，通过可学习门控因子 $g^l$ 控制注意力分数，避免早期训练阶段低秩矩阵干扰原始提示。具体步骤包括：(1) 初始化基模型提示；(2) 使用公式 $$\theta = \theta_0 + s \bigoplus_{i=1}^{t} B_i A_i$$ 构建低秩表示；(3) 通过正则化损失 $$\mathcal{L} = \mathbb{E}_{(x, y) \in \mathcal{T}} \left[ L_{\text{PLM}} + \lambda \sum_{i=1}^{t} \sum_{j \neq i} \|\boldsymbol{\mathcal{A}}_i^{\top} \boldsymbol{\mathcal{A}}_j - \boldsymbol{I}\|^{2}_{2} \right]$$ 优化参数，确保任务特定矩阵正交；(4) 在目标任务上微调任务特定部分。
*   **主要步骤：** 先训练单源任务的基模型，然后在多任务设置下优化低秩表示，最后针对目标任务进行少样本泛化。

## Experiment

*   **数据集和设置：** 使用 16 个 NLP 任务数据集，包括 8 个源任务（AFQMC、BAmazon、THUCNews 等）和 8 个目标任务（ChnSent、TNews 等），采用 Qwen2.5-7B 作为 backbone。实验包括未见数据和未见任务两种评估策略，涵盖全数据和少样本（k-shot）设置。未见数据评估模型捕捉异质性的能力，未见任务评估共享知识的泛化能力。
*   **实验设计：** 比较 TA-LoRA 与基线方法（如 Full Fine-Tuning、Adapter、PT、SPoT 等）的性能，参数规模从 110K 到 7B 不等。Ablation 研究验证了快速-缓慢权重机制和零初始化注意力的重要性。
*   **结果分析：** TA-LoRA 在未见数据和未见任务上均显著优于基线，平均性能提升 13.6% 和 11.4%，参数效率高（仅 1.3M 参数/任务）。少样本设置下，32-shot 性能已接近全数据水平。结果符合预期，证明了低秩表示和权重机制的有效性，Ablation 实验显示移除任一组件均导致性能下降，验证了设计合理性。

## Further Thoughts 

这项工作展示了低秩表示在多任务学习中的潜力，可能扩展到其他领域如计算机视觉的多任务细调中，与 Adapter 方法结合可能进一步提升泛化能力；此外，结合参考 [14] 和 [15]，未来可以探索低秩结构与注意力机制的更深融合，以减少对初始化敏感性和提升跨模态任务的鲁棒性。