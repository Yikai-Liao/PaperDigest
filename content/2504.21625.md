---
title: "Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability"
slug: "2504.21625"
description: "\u672c\u6587\u63d0\u51faMeeseeks\u591a\u8f6e\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u673a\u5236\u7cfb\u7edf\u8bc4\u4f30LLMs\u7684\u81ea\u7ea0\u9519\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u591a\u8f6e\u4e92\u52a8\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"
tags: ["LLM", "Instruction Following", "Benchmark", "Multi-Turn", "Self-Correction"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:31:32.711939+00:00
preference: "unknown"
score: 0.5312241231215923
featured: false
draft: false
---

> 本文提出Meeseeks多轮指令遵循基准，通过迭代反馈机制系统评估LLMs的自纠错能力，发现模型在多轮互动中性能显著提升。

> LLM, Instruction Following, Benchmark, Multi-Turn, Self-Correction 

> Jiaming Wang

> Meituan 

## Background Problem

大型语言模型（LLMs）在实际应用中需要准确遵循指令以充当可靠的代理，但现有指令遵循基准多为单轮评估，无法捕捉多轮互动中的反馈和自纠错过程，这与真实世界用户-LLM互动模式不符。本文的工作起点是填补这一空白，解决了评估LLMs多轮指令遵循能力的关键问题，包括模型的自纠错能力以及在复杂指令下的性能。

## Method

* **核心思想:** 提出Meeseeks基准，通过多轮迭代框架模拟真实人类-LLM互动，允许模型基于反馈进行自纠错，系统地评估指令遵循能力。
* **实现方式:** 采用一个自动迭代管道，包括四个关键步骤：（1）获取待评估LLM的响应；（2）使用LLM提取器（如一般提取器或代码提取器）提取待评估部分；（3）通过规则增强的LLM评估或规则-based评估检查是否满足要求；（4）准备反馈并进入下一轮。优化了评估过程，使用代码方式提取信息，提高效率和准确性，而不修改原始模型。
* **主要步骤:** 在默认配置下，最多进行三轮评估，每轮后提供具体未满足要求的反馈，基于38个能力标签在三个维度（意图识别、粒度内容验证、输出结构验证）组织评估系统。

## Experiment

* **数据集和设置:** 使用quick-start-dataset，包含700个参数化合成数据条目，覆盖28个能力标签，所有主观要求被移除以确保可靠性。实验评估了11个LLM（包括4个推理模型和7个非推理模型），采用3轮Meeseeks框架，使用Qwen2.5-32B-Instruct作为一般评估器和提取器。指标包括Utility Rate和Meeseeks Score，公式为：
  $$	ext{Utility Rate} = rac{\sum_{i=1}^{n} U_i}{n}, \text{ where } U_i = \begin{cases} 1, & \text{if response is usable} \\ 0, & \text{if response is not usable} \end{cases}$$
  $$	ext{Meeseeks Score} = \frac{\sum_{j=1}^{m} \text{Score}_{\text{tag}_j}}{m}$$
  实验设置全面合理，通过优化规则增强的LLM评估方法，减少了计算成本，提高了端到端准确性（从78.7%提升到98.4%），并使用批量处理加速评估。
* **结果和分析:** 模型在多轮中性能显著提升，例如o3-mini(high)的Utility Rate从58.3%提高到78.1%。推理模型在第一轮表现更好，但优势在后续轮次减弱；非推理模型通过反馈获得外部推理支持。实验揭示了模型在语言要求和字数控制方面的弱点，与预期一致，证明了基准的有效性和洞察力。

## Further Thoughts 

Meeseeks基准强调了多轮互动在LLM评估中的重要性，这可能启发在对话AI和人类反馈强化学习等领域的发展，例如结合强化学习 fine-tune 模型以提升自纠错能力；同时，它揭示了模型在复杂语言约束下的不足，值得探索通过集成高级自然语言理解模块或与认知科学结合来改进，未来可扩展到其他领域如医疗或金融AI代理的鲁棒性评估，与现有基准如Complexbench比较，可进一步研究反馈机制对不同模型泛化能力的影响。