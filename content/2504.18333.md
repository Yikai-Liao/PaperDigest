---
title: "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections"
slug: "2504.18333"
description: "\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u653b\u51fb\u6846\u67b6\u548c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86LLM-as-a-judge\u7cfb\u7edf\u7684prompt injection\u6f0f\u6d1e\uff0c\u5e76\u63a8\u8350\u4f7f\u7528\u591a\u6a21\u578b\u59d4\u5458\u4f1a\u7b49\u7b56\u7565\u63d0\u5347\u9c81\u68d2\u6027\u3002"
tags: ["Large Language Models", "Adversarial Attacks", "Prompt Injection", "LLM-as-a-Judge", "Evaluation Systems", "AI Safety"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:30:57.109467+00:00
preference: "unknown"
score: 0.7206529831337822
featured: false
draft: false
---

> 本文通过提出攻击框架和实验评估，揭示了LLM-as-a-judge系统的prompt injection漏洞，并推荐使用多模型委员会等策略提升鲁棒性。

> Large Language Models, Adversarial Attacks, Prompt Injection, LLM-as-a-Judge, Evaluation Systems, AI Safety 

> Narek Maloyan, Dmitry Namiot

> Independent Researchers 

## Background Problem

大型语言模型（LLMs）被越来越多地用于自动评估文本质量、代码正确性和论点强度，提供可扩展且成本效益高的替代人类评估的方法。研究显示LLM判断与人类判断高度相关，但这些系统易受adversarial attacks，尤其是prompt injection攻击的影响，可能操纵评估结果，导致不可靠性。本文从学术文献和Kaggle比赛中获取insights，调查LLM-as-a-judge系统的漏洞，旨在揭示其潜在风险并提出改进建议。

## Method

本文提出一个全面框架，用于开发和评估针对LLM-as-a-judge系统的adversarial attacks。该框架区分content-author attacks（恶意内容提交）和system-prompt attacks（评估模板被破坏），包括三个组件：framework component（确保注入与上下文融合）、separator component（创建上下文边界，如使用复杂词汇或格式中断）和disruptor component（执行恶意指令，如直接命令输出特定分数）。攻击变体有Basic Injection（简单指令注入）、Complex Word Bombardment（使用复杂词汇轰炸）、Contextual Misdirection（结合所有组件的复杂攻击）和Adaptive Search-Based Attack（使用遗传算法基于模型反馈优化攻击字符串）。实验中通过系统比较这些攻击，强调不修改模型本身，只在推理阶段注入攻击。

## Experiment

实验涉及五种模型（Gemma-3-27B-Instruct、Gemma-3-4B-Instruct、Llama-3.2-3B-Instruct、GPT-4和Claude-3-Opus）、四种评价任务（ppe human preference、search arena v1 7k、mt bench和code review），并测试了多种防御机制（如perplexity check、instruction filtering和multi-model committees）。每个条件使用n=50个prompt，采用bootstrap置信区间和t检验进行统计分析。结果显示，Adaptive Search-Based Attack成功率最高（73.8%），小型模型更易受攻击（Gemma-3-4B平均65.9%），multi-model committees能显著降低攻击成功率（7模型委员会成功率降至10.2-19.3%）。实验设置全面合理，覆盖不同模型和任务，统计方法严谨，结果符合预期，证明了攻击的有效性和防御策略的可行性。

## Further Thoughts 

论文中多模型委员会的防御策略启发我们，在其他AI应用中采用ensemble methods可以提高整体鲁棒性；攻击方法的transferability差异提示需要开发更通用的防御机制；此外，与AdvPrompter等工作的比较显示，AI安全领域的攻击与防御是动态的arms race，未来可能需要结合formal verification等方法来提升LLM的安全性，并探索其在多模态系统中的扩展。