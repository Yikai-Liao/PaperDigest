---
title: "HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization"
slug: "2504.21018"
description: "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684HYPEROFA\u65b9\u6cd5\uff0c\u7528\u4e8e\u521d\u59cb\u5316\u65b0\u8bed\u8a00\u4ee4\u724c\u5d4c\u5165\uff0c\u63d0\u9ad8PLM\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u9002\u5e94\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316\u5e76\u4e0eOFA\u65b9\u6cd5\u6301\u5e73\u6216\u66f4\u597d\u3002"
tags: ["Hypernetwork", "Embedding Initialization", "Multilingual PLMs", "Continual Pre-training", "Cross-lingual Transfer"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:33:03.377385+00:00
preference: "unknown"
score: 0.6291955916791697
featured: false
draft: false
---

> 本文提出基于超网络的HYPEROFA方法，用于初始化新语言令牌嵌入，提高PLM对低资源语言的适应性，性能优于随机初始化并与OFA方法持平或更好。

> Hypernetwork, Embedding Initialization, Multilingual PLMs, Continual Pre-training, Cross-lingual Transfer 

> Enes Özeren, Yihong Liu, Hinrich Schütze

> LMU Munich, Munich Center for Machine Learning 

## Background Problem

多语言预训练语言模型（PLMs）在中等和低资源语言上的性能 suboptimal，主要由于预训练数据中这些语言的暴露有限。一种常见策略是引入针对目标语言的新令牌，初始化它们的嵌入，并使用目标语言数据进行持续预训练。然而，嵌入初始化的方法至关重要，随机初始化无法利用现有嵌入中的知识，而OFA等方法虽基于相似性但受限于线性表达。本文提出HYPEROFA来解决这一问题，提供更具表达力的初始化策略。

## Method

*核心思想:* 使用超网络学习从外部多语言词向量空间到PLM嵌入空间的映射，以初始化新令牌嵌入。
*如何实现:* 包括三个步骤：
1. 源嵌入因子分解，使用SVD分解嵌入矩阵。
2. 匹配外部词向量，与令牌配对。
3. 训练超网络（BiLSTM架构），使用损失函数：
   - 对比损失：$$ \mathcal{L}_{\mathbf{c}} = \mathbb{E}\left[ -\log \frac{\exp(\text{sim}(\mathbf{F}_i^s, \hat{\mathbf{F}}_i^s)/\tau)}{\exp(\text{sim}(\mathbf{F}_i^s, \hat{\mathbf{F}}_i^s)/\tau) + \text{NEG}} \right] $$
   - 标准化L1损失：$$ \mathcal{L}_{\mathrm{L1}} = \mathbb{E}\left[||\boldsymbol{F}_i^s - \hat{\boldsymbol{F}}_i^s||_1\right] $$
   最终损失：$$ L(\theta) = \lambda \cdot L_c + (1 - \lambda) \cdot L_{L1} $$
新令牌初始化：复制重叠、用超网络预测、随机初始化剩余。

## Experiment

*实验设置:* 在RoBERTa和XLM-R上扩展词汇，比较HYPEROFA、OFA和随机初始化。数据集包括句子检索（SR-T、SR-B）和序列标注（NER、POS），在22种语言上评估零样本跨语言性能。实验设计合理，覆盖不同资源水平语言，并评估初始化嵌入的质量和持续预训练后的效果。
*结果:* 预持续预训练时，HYPEROFA优于随机初始化，与OFA相当；持续预训练后，表现良好，收敛更快，结果符合预期，表明方法改进明显，实验设置全面。

## Further Thoughts 

HYPEROFA的方法突显了超网络在嵌入初始化中的灵活性，可能适用于其他任务如领域适应或跨模态学习。未来可以与最新的大型语言模型结合，或探索不同超网络架构以提高效率和泛化能力，并与其他初始化技术比较，以进一步提升在低资源场景下的表现。