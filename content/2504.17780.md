---
title: "Replay to Remember: Retaining Domain Knowledge in Streaming Language Models"
slug: "2504.17780"
description: "\u672c\u6587\u901a\u8fc7\u7ed3\u5408LoRA\u548c\u8f7b\u91cf\u7ea7\u91cd\u653e\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u5728\u6d41\u5f0f\u5b66\u4e60\u6761\u4ef6\u4e0b\u5e2e\u52a9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u57df\u9002\u5e94\u3002"
tags: ["Continual Learning", "LoRA", "Replay Buffer", "Language Models", "Real-Time Adaptation", "Catastrophic Forgetting"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:30:18.900033+00:00
preference: "unknown"
score: 0.6513709448813092
featured: false
draft: false
---

> 本文通过结合LoRA和轻量级重放机制的方法，在流式学习条件下帮助大型语言模型减轻灾难性遗忘，同时实现了实时域适应。

> Continual Learning, LoRA, Replay Buffer, Language Models, Real-Time Adaptation, Catastrophic Forgetting 

> Sneh Pillai

> University of Massachusetts Dartmouth 

## Background Problem

大型语言模型（LLMs）在持续学习中面临灾难性遗忘的重大挑战，即当暴露于新数据时，先前习得的知识会迅速退化。尽管诸如重放缓冲区和低秩适应（LoRA）等技术已被提出，但鲜有研究探讨在严格的计算资源和数据流约束下的实时域适应。本文的工作起点是解决在资源受限的真实世界场景中，LLMs如何能够持续适应新知识流而不丢失先前领域知识。

## Method

* **核心思想：** 结合LoRA参数高效微调和最小重放机制，在流式学习环境中减轻LLMs的灾难性遗忘。
* **如何实现：** 使用LoRA仅更新模型注意力层的低秩矩阵，以减少计算开销；同时，引入一个轻量级重放缓冲区，按比例重新引入先前见过的样本，以维持知识保留。
* **主要步骤：** 数据以增量批次流式输入，模型在每个批次上进行LoRA微调，并周期性地应用重放缓冲区来强化先前知识。

## Experiment

* **数据集和设置：** 实验使用三个不同领域的流式数据集：医疗问答（MedQuAD）、遗传信息和法律信息。数据以小批次形式顺序输入，模拟实时域移位场景。模型在计算资源受限的硬件上运行，使用LoRA和最小重放机制，以反映真实部署环境。
* **评估指标：** 采用困惑度（衡量预测不确定性）、语义相似度（使用余弦相似度量化语义漂移）和GPT-4基于人类评价的评分（1-10分，评估答案的相关性、完整性和流畅性）来量化适应、遗忘和恢复。
* **结果：** 结果显示灾难性遗忘确实发生，但最小重放机制显著稳定了性能并部分恢复了知识。例如，在遗传领域，困惑度从2906.99上升到超过32万，但重放后有所下降；法律领域显示更稳定，语义相似度和GPT评分变化较小。实验结果与预期一致，验证了方法的有效性，同时突出了域间差异，如遗传领域遗忘更严重。

## Further Thoughts 

这项研究强调了重放机制在持续学习中的重要性，启发我们探索更先进的策略，如基于重要性的样本选择或多域适配器融合，以进一步减少遗忘；此外，结合其他领域如计算机视觉的增量学习或强化学习的经验回放，可能带来交叉启发，并考虑元学习来改善初始适应能力。