---
title: "Codenames as a Benchmark for Large Language Models"
slug: "2412.11373"
description: "\u672c\u8bba\u6587\u63d0\u51fa\u4f7f\u7528Codenames\u6e38\u620f\u4f5c\u4e3aLLMs\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e0d\u540cLLMs\u5728\u8bed\u8a00\u7406\u89e3\u3001\u6218\u7565\u63a8\u7406\u548c\u5408\u4f5c\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u7684\u72ec\u7279\u884c\u4e3a\u548c\u6cdb\u5316\u6f5c\u529b\u3002"
tags: ["LLM", "Codenames", "Benchmark", "Reasoning", "Theory of Mind", "Game Playing Agents"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:27:20.335933+00:00
preference: "like"
score: 0.771812806040723
featured: false
draft: false
---

> 本论文提出使用Codenames游戏作为LLMs推理能力的基准，通过实验评估不同LLMs在语言理解、战略推理和合作方面的表现，展示了它们的独特行为和泛化潜力。

> LLM, Codenames, Benchmark, Reasoning, Theory of Mind, Game Playing Agents 

> Matthew Stephenson, Matthew Sidji, Benoît Ronval

> Flinders University, University of Melbourne, UCLouvain 

## Background Problem

本研究的出发点是探索一种适合评估大型语言模型（LLMs）推理能力的基准测试工具。背景问题在于，LLMs在语言任务上取得了快速进展，但传统AI基准如国际象棋或围棋更侧重空间推理和战略规划，而LLMs在这些领域的表现较弱。Codenames作为一款基于语言的合作游戏，强调自然语言理解、理论心智（theory of mind）和认识论推理（epistemic reasoning），能够更好地测试LLMs的语言中心能力。该工作解决了之前Codenames AI框架的简化问题，例如早期方法依赖词嵌入技术，词汇范围有限，且在不同策略下合作性差，无法泛化到多样化队友，而LLMs可能提供更强的泛化性和人类可解释性。

## Method

本研究的方法是更新Codenames AI框架以支持完整游戏规则，并使用LLMs作为代理进行游戏。核心思想是通过特定提示设计，让LLMs扮演codemaster（给出线索）和guesser（猜测单词）的角色。具体步骤包括：
- 为LLMs提供游戏规则提示，包括团队颜色和角色；
- codemaster在给出线索时，必须提供一个单词和数字，提示不能与棋盘单词相关联；
- guesser根据线索选择单词，并决定是否继续猜测；
- 实验中比较了多种LLMs（如GPT-4o、Gemini-1.5等）和传统词向量代理（如Word2Vec、GloVe）在单队（合作）和双队（竞争/合作）游戏版本中的性能。主要创新是通过动态提示和规则约束，评估LLMs的推理和策略适应能力，而不修改模型本身。

## Experiment

实验设置包括单队版本（评估完成所有目标单词的回合数）和双队版本（评估胜率），使用随机棋盘设置，共进行100次试验。数据集基于Codenames标准规则，单词板随机生成。实验评估了多种LLMs（如o1-preview、GPT-4o、Gemini-1.5、Sonnet-3.5、Llama-3.1）和词向量代理的性能，测量指标包括平均分数、胜率、损失率、线索数字平均值等。结果显示，LLMs在保持较高准确率的同时，表现出不同的风险策略（例如Sonnet-3.5更冒险，Llama-3.1更谨慎），且LLMs比词向量代理更易与其他代理合作。方法改进明显，因为LLMs在双队版本中胜率更高，实验设置全面合理，考虑了不同角色和团队组合，成果符合预期，突出了LLMs在语言推理和理论心智方面的优势。

## Further Thoughts 

这项研究启发我们，游戏环境可以作为评估AI智能的多功能平台，不仅能测试LLMs的语言和推理能力，还可扩展到多模态任务（如图片版Codenames）或与其他基准（如MMLU）结合，以探究AI在社会互动中的局限性。例如，LLMs在理论心智方面的表现可能与人类合作游戏中的表现相关联，未来可研究如何将这些洞见应用于真实世界的人机协作场景，或与其他AI代理（如在围棋或扑克中的强化学习模型）比较，以揭示LLMs在策略泛化上的潜力。