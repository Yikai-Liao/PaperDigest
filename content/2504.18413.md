---
title: "An Empirical Study of Evaluating Long-form Question Answering"
slug: "2504.18413"
description: "\u672c\u6587\u5b9e\u8bc1\u7814\u7a76\u4e86\u957f\u5f62\u5f0f\u95ee\u9898\u56de\u7b54\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8eLLM\u7684\u6307\u6807\u5728\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u5206\u6790\u4e86\u5176\u504f\u5dee\u548c\u6539\u8fdb\u7b56\u7565\u3002"
tags: ["LLM", "Evaluation Metrics", "Question Answering", "Human Annotation", "Bias", "Robustness", "Fairness"]
author: "grok-3-mini-latest"
pubDatetime: 2025-05-04T08:31:29.169100+00:00
preference: "unknown"
score: 0.5577591474120578
featured: false
draft: false
---

> 本文实证研究了长形式问题回答的自动评估指标，证明了基于LLM的指标在准确性和稳定性上的优势，同时分析了其偏差和改进策略。

> LLM, Evaluation Metrics, Question Answering, Human Annotation, Bias, Robustness, Fairness 

> Ning Xian, Yixing Fan, Ruqing Zhang, Maarten de Rijke, Jiafeng Guo

> Institute of Computing Technology, Chinese Academy of Sciences, University of Amsterdam 

## Background Problem

长形式问题回答（LFQA）旨在为复杂问题生成长篇答案，随着大语言模型（LLM）的进步，生成能力显著提升，但评估面临挑战。现有研究多依赖基于字符串或n-gram匹配的确定性指标（如ROUGE和Exact Match），这些指标与人类判断的相关性弱，无法捕捉长答案的细微差异；同时，基于LLM的评估方法虽有潜力，但易受幻觉问题影响，其可靠性尚未充分探索。本文通过三个研究问题探讨自动指标替代人类评估的程度、指标的局限性以及改进方法，以填补LFQA评估领域的空白。

## Method

本文采用实证研究方法，核心思想是通过元评估（meta-evaluation）比较自动评估指标与人类评估的一致性。主要步骤包括：（1）收集七个LLM（如GLM-4-9B-chat、Llama系列等）在ASQA、ANTIQUE和WikiEval数据集上的回答；（2）进行人类标注，评估正确性和信息性；（3）使用Spearman和Kendall相关系数、获胜率和一致性度量分析自动指标的准确性、鲁棒性和公平性；（4）引入微扰（如提示词变化、超参数调整）测试鲁棒性，并分析偏差（如长度偏差、问题类型偏差）；（5）探索改进策略，如细粒度提示设计。

## Experiment

实验使用ASQA（歧义事实QA）、ANTIQUE（非事实开放QA）和WikiEval（事实QA）数据集，从七个LLM生成5236个答案，并对2079个答案进行人类评估（4158个评分），焦点在正确性和信息性。实验设置全面，覆盖不同QA类型和指标类别（确定性指标如ROUGE-L、Exact Match和模型-based指标如BERTScore、GPT-4o等）。结果显示LLM-based指标与人类评估相关性更高（Spearman相关系数平均42.0%），但存在偏差，如长度增加时ROUGE-L分数下降，LLM指标对自身输出偏好；鲁棒性测试表明提示词和温度变化影响评分分布。结果符合预期，突出了细粒度评估的优点，并验证了指标的局限性。

## Further Thoughts 

本文的研究强调了LLM在评估中的潜力，但也揭示了偏差问题，这可能扩展到其他NLP领域，如文本摘要或对话系统评估中，未来可结合多模态数据（如图像和文本）提升鲁棒性；此外，偏差分析（如长度和问题类型偏差）可指导LLM训练，减少幻觉，并促进公平AI设计，例如在搜索引擎或聊天机器人中应用混合评估方法，以实现更可靠的人机交互。