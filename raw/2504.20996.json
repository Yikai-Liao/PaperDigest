{
    "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
    "authors": [
        "Sicheng Mo",
        "Thao Nguyen",
        "Xun Huang",
        "Siddharth Srinivasan Iyer",
        "Yijun Li",
        "Yuchen Liu",
        "Abhishek Tandon",
        "Eli Shechtman",
        "Krishna Kumar Singh",
        "Yong Jae Lee",
        "Bolei Zhou",
        "Yuheng Li"
    ],
    "institution": [
        "University of California, Los Angeles",
        "University of Wisconsin–Madison",
        "Adobe Research"
    ],
    "problem_background": "研究背景是大型語言模型（LLMs）在語言處理任務上取得了空前成功，但人類溝通涉及多種模態，如視覺，因此需要一個真正通用的AI模型能夠理解、推理和生成文本及視覺信息。現有方法要麼從零訓練統一模型，計算資源需求巨大，要麼微調預訓練LLMs引入視覺能力，但可能導致語言能力下降。本文的工作起點是重用預訓練LLMs，高效引入新模態，同時保留其語言能力。解決的關鍵問題包括：如何在不犧牲語言性能的情況下，實現視覺理解和生成，降低計算成本，並提供更高效的多模態整合策略。",
    "method": "*   **核心思想：** X-Fusion框架通過雙塔設計，將預訓練LLMs的語言塔參數凍結，引入可訓練的視覺塔，實現模態特定權重的處理，保留語言能力同時適應視覺任務。\n*   **如何實現：** 每個Transformer層包含凍結的文本塊和可訓練的視覺塊。輸入嵌入處理後，輸出根據模態選擇組合。具體步驟包括：\n    - 令牌化：文本使用LLM的令牌化器，圖像通過預訓練視覺編碼器（如VAE）得到潛在表示，然後轉換為視覺令牌。\n    - 雙塔處理：文本塔和視覺塔獨立處理輸入，輸出為$$\\mathbf{H}^{\\text{out}} = \\{ \\mathbf{h}_i \\} $$，其中$$ \\mathbf{h}_i = \\begin{cases} \\mathbf{h}_i^{\\text{txt}}, & \\text{if } x_i \\in \\mathbf{x}^{\\text{txt}} \\\\ \\mathbf{h}_i^{\\text{img}}, & \\text{if } x_i \\in \\mathbf{x}^{\\text{img}} \\end{cases} $$。\n    - 可選X-Fuse操作：融合兩個塔的特征，如$$ \\alpha \\times \\mathbf{h}_i^{\\text{txt-txt}} + \\beta \\times \\mathbf{h}_i^{\\text{txt-img}} $$，以提升性能。\n    - 訓練目標：結合自迴歸損失和擴散損失，公式為$$ \\mathcal{L} = \\lambda_{\\rm AR} \\cdot \\mathcal{L}_{\\rm AR} + \\lambda_{\\rm DM} \\cdot \\mathcal{L}_{\\rm DM} $$，其中$\\mathcal{L}_{\\rm AR}$是語言建模損失，$\\mathcal{L}_{\\rm DM}$是圖像去噪損失。",
    "experiment": "*   **實驗設置：** 實驗包括架構比較（單塔、門控塔、雙投影、雙塔）、數據比例、噪聲水平和特征對齊的影響。數據集：MS-COCO用於評估，內部授權數據用於訓練，包含圖像-文字對。指標：FID評估圖像生成，BLIP評估圖像理解，MMLU評估語言能力。\n*   **為何這樣設計：** 通過系統性消融研究，驗證不同因素對性能的影響，例如增加理解數據提升生成性能，減少圖像噪聲改善整體效果，特征對齊加速小模型收斂。\n*   **結果：** 雙塔設計在保持語言準確率（MMLU不變）的情況下，FID降低23%，BLIP得分提高；清潔圖像和2:1數據比例優化性能；結果符合預期，證明方法改進明顯，實驗設置全面合理。",
    "one_sentence_summary": "本文提出X-Fusion框架，通過凍結LLM參數並添加雙塔結構，高效實現多模態理解和生成，同時保留原始語言能力。",
    "slug": "x-fusion-introducing-new-modality",
    "keywords": [
        "Large Language Models",
        "Multimodal Learning",
        "Vision Tower",
        "Dual Tower Architecture",
        "Diffusion Models",
        "Image Generation",
        "Image Understanding"
    ],
    "further_thoughts": "X-Fusion的模塊化設計不僅適用於視覺模態，還可擴展到音頻或視頻，促進更通用的多模態系統發展；與LMFusion等並行工作比較，X-Fusion的獨立塔設計提供更大靈活性，可能在跨模態推理中表現更優；論文對數據質量的強調（如噪聲控制）啟發了其他領域，如醫療影像分析中的數據增強策略；未來，結合強化學習或聯邦學習，可能提升模型在動態環境中的適應性，並減少對大型數據的需求。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.20996",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:31:11.458711+00:00",
    "score": 0.6651735362670572,
    "abstract": "We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.",
    "categories": [
        "cs.CV"
    ],
    "created": "2025-04-29",
    "updated": "2025-04-30"
}