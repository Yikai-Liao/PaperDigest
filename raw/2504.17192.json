{
    "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning",
    "authors": [
        "Minju Seo",
        "Jinheon Baek",
        "Seongyun Lee",
        "Sung Ju Hwang"
    ],
    "institution": [
        "KAIST",
        "DeepAuto.ai"
    ],
    "problem_background": "机器学习研究迅速发展，但许多论文缺少对应的代码实现，导致研究者难以复现结果并在此基础上开展工作，耗时耗力。论文指出，2024年顶级机器学习会议接受论文中仅有21.23%提供了代码实现，这大大阻碍了科学创新。同时，虽然大型语言模型（LLMs）在理解科学文档和生成高质量代码方面表现出色，但现有方法通常依赖预先存在的代码片段或API，无法从论文本身直接生成完整实现。因此，本工作的出发点是利用LLMs自动生成代码仓库，以提升研究的可复现性和效率。",
    "method": "本研究提出PaperCoder框架，这是一个基于多代理LLM的系统，用于从机器学习论文自动生成可执行代码仓库。核心思想是将代码生成过程分解为三个阶段：规划（planning）、分析（analysis）和生成（coding）。在规划阶段，构建高层路线图、设计系统架构（包括类图和序列图）、识别文件依赖关系并生成配置文件；在分析阶段，对每个文件进行细粒度解释，包括功能需求、输入输出、与其他模块的交互以及算法约束；在生成阶段，按照预定义的执行顺序合成模块化代码。每个阶段由专门的代理模型协作完成，形式化为C = Mcode(R, P, A)，其中R是论文，P是规划输出，A是分析输出。",
    "experiment": "实验在Paper2Code基准（基于2024年NeurIPS、ICML和ICLR的90篇论文）和PaperBench基准上进行。Paper2Code基准通过OpenReview API筛选了代码仓库可用且规模适中的论文，采用模型-based评估（包括参考-based和参考-free设置）和人类评估。结果显示，PaperCoder在正确性评分上显著优于基线（如ChatDev、MetaGPT），参考-based评分平均为3.72-3.83，参考-free为4.73-4.77；人类评估中，77%的作者认为PaperCoder生成的仓库最佳，85%认为其有助于复现。实验设置合理全面，覆盖了不同会议和模型变体，结果与预期一致，证明了框架的有效性和鲁棒性。",
    "one_sentence_summary": "本文提出PaperCoder框架，通过多代理LLM的多阶段管道自动从机器学习论文生成高质量代码仓库，提升了研究的可复现性，并在基准测试中显著优于现有方法。",
    "slug": "paper2code-automating-code-generation",
    "keywords": [
        "Large Language Models",
        "Code Generation",
        "Multi-Agent Systems",
        "Reproducibility",
        "Machine Learning"
    ],
    "further_thoughts": "本论文的启发性在于，它展示了LLM在自动化科研工作流中的潜力，不仅限于代码生成，还可扩展到其他领域如生物医学或物理学的研究复现；未来可探索与自动测试框架的整合，以提升代码执行可靠性；此外，结合PaperBench等基准，可以进一步研究LLM在处理模糊论文描述时的鲁棒性，并与其他代理系统（如ResearchAgent）结合，实现更全面的科研自动化。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.17192",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:29:40.309951+00:00",
    "score": 0.5069470280639435,
    "abstract": "Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins. Code is available at: https://github.com/going-doer/Paper2Code.",
    "categories": [
        "cs.CL"
    ],
    "created": "2025-04-26",
    "updated": "2025-04-29"
}