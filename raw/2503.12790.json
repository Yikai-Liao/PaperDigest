{
    "title": "Quantum-Enhanced LLM Efficient Fine Tuning",
    "authors": [
        "Xiaofei Kong",
        "Lei Li",
        "Zhaoyun Chen",
        "Cheng Xue",
        "Xiaofan Xu",
        "Huanyu Liu",
        "Yuchun Wu",
        "Yuan Fang",
        "Han Fang",
        "Kejiang Chen",
        "Yang Yang",
        "Menghan Dou",
        "Guoping Guo"
    ],
    "institution": [
        "Origin Quantum Computing Company Limited",
        "Institute of Artificial Intelligence, Hefei Comprehensive National Science Center",
        "Anhui Engineering Research Center of Quantum Computing",
        "Laboratory of Quantum Information, University of Science and Technology of China",
        "National University of Singapore",
        "University of Science and Technology of China",
        "Anhui University"
    ],
    "problem_background": "大型语言模型（LLM）的快速发展推动了参数高效微调（PEFT）方法的创新，以减少计算开销同时保持性能。经典方法如低秩适配（LoRA）和加权分解低秩适配（DoRA）假设微调时的权重更新位于低秩子空间内，通过可训练的低秩矩阵实现高效适配。然而，这些低秩逼近方法固有地限制了特征表示的适应性，可能在复杂任务中影响收敛，并对秩选择敏感。本文的工作起点是克服这些限制，利用量子计算的优势，探索量子张量混合微调方法，以提升LLM在高秩依赖场景下的表达能力和性能。",
    "method": "QTHA方法的核心思想是将预训练权重分解为量子神经网络（QNN）和张量网络（基于矩阵乘积算子MPO）的混合表示，充分利用量子态叠加和纠缠来克服经典低秩逼近的瓶颈。具体实现包括：首先，使用MPO对权重矩阵进行张量分解，例如将权重矩阵$\\mathbf{W} \\in \\mathbb{R}^{M \\times N}$映射为高阶张量$\\mathcal{W}' \\in \\mathbb{R}^{J_1 \\times \\cdots \\times J_n \\times I_1 \\times \\cdots \\times I_n}$，并通过张量因式分解得到$\\mathcal{W}_{j_1\\dots j_n, i_1\\dots i_n} = \\text{Tr}[\\mathbf{w}_{j_1i_1}^{(1)} \\mathbf{w}_{j_2i_2}^{(2)} \\cdots \\mathbf{w}_{j_ni_n}^{(n)}]$，其中键维$D_k$控制模型的表达能力。然后，QNN通过角度嵌入门将输入向量编码为量子态$|\\psi(\\alpha)\\rangle = \\bigotimes_{i=1}^{q} R_Y(\\alpha_i)|0\\rangle$，并通过参数化幺正变换$U(\\theta)$演化，测量Pauli-Z观测值得到输出。最终，通过线性组合$\\tilde{O} = W_q O_q + W_c O_c$融合QNN和经典神经网络输出，构建QTHA架构，如图1所示，该架构动态调整特征权重，输出MPO和QNN的组合特征。",
    "experiment": "实验使用公开数据集CPsyCoun（中文心理咨询对话数据集，16K样本）、R1-Distill-SFT（K-12数学问答数据集）和CH-R1-Math（中文指令微调数据集，110K样本），评估指标包括交叉熵损失$\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{C} \\mathbf{y}_{ik} \\log(p_{ik})$、困惑度（PPL）、BLEU-4和ROUGE分数。实验设置合理，采用基准模型DeepSeek-R1-Distill-Qwen-7B和Qwen2-7B-Instruct，LoRA和QTHA秩设为4，QuanTA分解因子设为5，训练序列长度为1024，数据集规模从300到3000样本不等。结果显示，QTHA在保持或提升性能的同时，比LoRA减少76%可训练参数，训练损失降低最多17%，测试性能（如准确率、严格准确率、CTC和AC）提升最多17%。量子硬件实现使用Origin Wukong后端，展示了噪声量子设备下的鲁棒性，PPL和生成指标（如BLEU-4、ROUGE）均优于基线方法，验证了量子增强的实际可行性。",
    "one_sentence_summary": "本文提出量子张量混合适配（QTHA）方法，通过整合量子神经网络和张量网络，实现LLM的参数高效微调，显著减少参数量并提升性能，为量子增强人工智能奠定基础。",
    "slug": "quantum-enhanced-llm-efficient-fine-tuning",
    "keywords": [
        "LLM",
        "Quantum Computing",
        "Fine-Tuning",
        "QTHA",
        "MPO",
        "QNN"
    ],
    "further_thoughts": "这项工作展示了量子计算在LLM微调中的潜力，不仅验证了量子硬件的实际应用，还启发未来探索量子-经典混合架构在其他领域如计算机视觉或强化学习的扩展；例如，与经典方法如DoRA结合可能进一步优化参数效率，而噪声量子设备的鲁棒性研究可借鉴LLM的概率输出特性，提升量子AI系统的泛化能力；此外，它为量子增强AGI提供工程基础，未来可调查量子预训练方案如何捕获更高阶相关性，以推动更广泛的量子机器学习创新。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2503.12790",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:28:40.460169+00:00",
    "score": 0.7215195874992911,
    "abstract": "Low-Rank Adaptation (LoRA) enables efficient fine-tuning of pre-trained language models through low-rank matrix approximation, achieving effectiveness in many scenarios. However, its representation capacity is constrained in complex tasks or high-rank dependency settings, potentially limiting model adaptability. To overcome the expressive bottleneck in classical low-rank approximation for fine-tuning large language models (LLMs), we propose Quantum Tensor Hybrid Adaptation (QTHA), a parameter-efficient fine-tuning method that integrates a quantum neural network (QNN) with a tensor network. QTHA explores quantum tensor hybrid fine-tuning within low-rank spaces by decomposing pre-trained weights into quantum neural network and tensor network representations, leveraging quantum state superposition to overcome classical rank limitations. Experiments demonstrate that QTHA achieves performance comparable to or surpassing LoRA in parameter-efficient fine-tuning. Compared to LoRA, QTHA reduces trainable parameters by 76% while reducing training loss by up to 17% and improving test set performance by up to 17% within the same training steps. This research not only enables lightweight adaptation of quantum resources to the billion-parameter models but also validates the feasibility of quantum hardware optimization driven by LLM tasks. It establishes the first engineering-ready foundation for future quantum-enhanced Artificial General Intelligence (AGI) systems.",
    "categories": [
        "quant-ph",
        "cs.AI"
    ],
    "created": "2025-04-27",
    "updated": "2025-04-29"
}