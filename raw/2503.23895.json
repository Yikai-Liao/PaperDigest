{
    "title": "Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement",
    "authors": [
        "Yuqiao Tan",
        "Shizhu He",
        "Huanxuan Liao",
        "Jun Zhao",
        "Kang Liu"
    ],
    "institution": [
        "中国科学院自动化研究所",
        "中国科学院大学"
    ],
    "problem_background": "大型语言模型（LLMs）在知识密集型任务中表现出色，但由于无法访问外部知识和容易产生幻觉，研究者开发了知识增强方法。检索增强生成（RAG）通过从外部来源检索相关文档并将其注入输入上下文来提升LLMs的性能，但这会显著增加推理成本并导致RAG幻觉问题（即知识冲突）。参数化RAG（PRAG）通过将文档嵌入LLMs参数来减少推理成本，但其高训练和存储成本以及有限的泛化能力限制了其实际应用。本文的工作起点是解决这些挑战，即如何在减少成本的同时提高RAG系统的效率和鲁棒性，特别是缓解知识冲突和幻觉问题。",
    "method": "*   **核心思想：** 提出动态参数化RAG（DyPRAG）框架，使用一个轻量级参数翻译器模型（hypernetwork）来动态地将文档转换为参数知识，实现测试时的即插即用知识增强，而非传统的离线训练。\n*   **工作原理：** DyPRAG分为三个阶段：（1）收集文档-参数（Doc-Param）对，通过文档增强和参数化过程；（2）训练参数翻译器$F'_\\phi$，该模型是一个简单的多层感知器（MLP），输入文档的嵌入表示，输出动态的LoRA参数；（3）推理阶段，对检索到的文档进行嵌入编码，并通过$F'_\\phi$生成参数知识，然后注入LLMs中进行响应生成。具体公式包括损失函数：\n    $$\n    \\mathcal{L}_{\\text{align}} = \\mathcal{L}_{\\text{pred}} + \\lambda_1 \\mathcal{L}_{\\text{mse}} + \\lambda_2 \\mathcal{L}_{\\text{kl}}\n    $$\n    其中，$\\mathcal{L}_{\\text{pred}}$是预测损失，$\\mathcal{L}_{\\text{mse}}$是均方误差损失，$\\mathcal{L}_{\\text{kl}}$是KL散度损失，用于对齐生成参数与目标参数。\n*   **主要步骤：** 首先，离线收集Doc-Param对；然后，训练$F'_\\phi$以学习文档到参数的映射；最后，在测试时动态生成参数，减少了传统RAG的上下文长度依赖和PRAG的训练存储开销。",
    "experiment": "*   **数据集和实验设置：** 本文使用多个基准数据集评估DyPRAG，包括2WikiMultihopQA（2WQA）、HotpotQA（HQA）、PopQA（PQA）和ComplexWebQuestions（CWQ），这些数据集针对多跳推理和常识推理。评估指标为F1分数，实验使用不同规模的LLMs，如Qwen2.5-1.5B、LLaMA-3.2-1B和LLaMA-3-8B。实验设计包括IID和OOD设置，目的是验证DyPRAG在保持性能的同时减少成本和缓解RAG幻觉的能力。\n*   **为什么这样设计：** 实验设置全面，覆盖了不同模型规模和分布场景，以证明DyPRAG的泛化能力。OOD实验使用StrategyQA和IIRC数据集，引入ground-truth passages以更严格评估知识增强效果。\n*   **结果分析：** DyPRAG在多个数据集上表现出色，例如在LLaMA-3.2-1B上，DyPRAG的平均F1分数为27.57%，优于标准RAG（26.99%）和PRAG（26.51%）。结合上下文知识的DyPRAG-Combine进一步提升性能，平均改善1.08%。成本分析显示，DyPRAG显著降低了存储和推理成本，OOD性能也 superior，证明了其泛化能力。结果与预期一致，展示了DyPRAG在减少幻觉和提升知识融合方面的优势。",
    "one_sentence_summary": "本文提出动态参数化RAG框架DyPRAG，通过训练一个轻量级参数翻译器在测试时动态转换文档为参数知识，显著降低成本、提升泛化能力和缓解RAG幻觉问题。",
    "slug": "dynamic-parametric-rag",
    "keywords": [
        "检索增强生成",
        "参数化知识",
        "测试时增强",
        "大型语言模型",
        "知识融合"
    ],
    "further_thoughts": "DyPRAG的动态参数生成方法启发我们思考如何在实时应用中更高效地融合内部和外部知识，或许可以扩展到其他任务如数学推理或多模态处理中；此外，与其他RAG方法（如FLARE或DRAGIN）的结合可能进一步优化检索策略，而在知识冲突检测上，开发基于模型内部状态的工具（如perplexity改进）能够更准确评估和缓解幻觉问题。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2503.23895",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:29:07.644747+00:00",
    "score": 0.718901611194702,
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.",
    "categories": [
        "cs.CL",
        "cs.AI"
    ],
    "created": "2025-05-01",
    "updated": "2025-05-02"
}