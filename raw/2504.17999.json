{
    "title": "Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving",
    "authors": [
        "Chang Xiao",
        "Brenda Yang"
    ],
    "institution": [
        "未在提供的内容中指定"
    ],
    "problem_background": "这项工作的起点是针对大型语言模型（LLM）在云服务中的流式输出问题，LLM通常以固定的速度逐 token 流式传输输出，而忽略了用户阅读速度和内容认知负载的差异，导致资源利用低效。例如，在高峰期，快速流式传输复杂内容可能浪费计算资源，用户无法及时处理，而简单内容传输过慢则导致不必要等待。论文解决了关键问题，包括优化计算资源分配、减少等待时间和资源浪费，同时在高用户需求场景下提升系统效率和用户体验。",
    "method": "*   **核心思想：** 通过实时估计内容认知负载来动态调整 LLM 输出流式速度，实现计算资源高效分配，而不影响用户体验。\n*   **实现方式：** 使用认知负载估计方法，包括 Gunning-Fog 指数等可读性指标或通过提示 LLM 自身生成认知负载分数（如在输出中添加 <X> 标签表示分数）。然后，采用资源分配算法计算权重：$$ \\mathbf{w}_{\\delta} = \\alpha \\cdot \\frac{\\mathbf{s}_{\\delta}}{\\sum_{j=1}^{n} \\mathbf{s}_{j}} + (1 - \\alpha) \\cdot \\frac{1}{n} $$，其中 $\\alpha$ 是控制参数，$\\mathbf{s}_{\\delta}$ 是认知负载分数。最终，流式速度计算为 $$ v_{\\ell} = \\mathbf{w}_{\\ell} \\cdot \\mathbf{k} $$，以在并发请求中按内容复杂度分配速度。\n*   **主要步骤：** 1. 估计每个文本段落的认知负载；2. 归一化负载分数并插值计算权重；3. 根据权重调整流式速度，确保复杂内容减速，简单内容加速。",
    "experiment": "*   **实验设置：** 论文通过众包用户研究（使用 Prolific 平台）收集用户舒适阅读速度数据，采用 Parameter Estimation by Sequential Testing (PEST) 方法。数据集包括 GPT-4o 生成的 10 个不同主题和认知负载的英语段落（长度 150-200 词），参与者通过模拟流式接口调整速度。实验控制了数据质量，排除了不一致响应。\n*   **为什么这样设计：** 这种设置允许量化认知负载估计的准确性和适应性流式方法的效率，验证了认知负载与阅读速度的相关性，并模拟了真实云服务场景下的资源约束。\n*   **结果：** 认知负载估计方法验证显示，LLM-based 方法相关系数 r=0.955（p<0.001），Gunning-Fog 方法 r=0.828（p=0.003），表明有效性。比较实验显示，适应性流式方法在相同用户满意率下显著降低计算资源使用，例如在 95% 满意率下，LLM-based 方法节省 16.79%，Gunning-Fog 方法节省 10.33%。结果符合预期，证明了方法在资源有限场景下的优势。",
    "one_sentence_summary": "本文提出基于认知负载的适应性流式传输框架，用于优化 LLM 服务，通过动态调整输出速度减少计算资源消耗高达 16.8%，同时维持用户满意度。",
    "slug": "cognitive-load-aware-streaming-llm",
    "keywords": [
        "Cognitive Load",
        "Large Language Models",
        "Human-AI Interaction",
        "Streaming",
        "Readability"
    ],
    "further_thoughts": "论文的启发性想法包括将目光追踪技术整合到 LLM 交互中，以实现更精确的实时认知负载监测，可能扩展到多模态 AI 系统（如结合语音或视频流式传输）；此外，可以探索与推测性解码或其他资源优化技术结合，提升整体效率；还可考虑用户个性化因素，如基于历史数据或人口统计学特征的适应性调整，潜在应用场景包括教育 AI 或医疗助手，以进一步改善人机交互体验。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.17999",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:30:30.340392+00:00",
    "score": 0.6043354253345283,
    "abstract": "Generative conversational interfaces powered by large language models (LLMs) typically stream output token-by-token at a rate determined by computational budget, often neglecting actual human reading speeds and the cognitive load associated with the content. This mismatch frequently leads to inefficient use of computational resources. For example, in cloud-based services, streaming content faster than users can read appears unnecessary, resulting in wasted computational resources and potential delays for other users, particularly during peak usage periods. To address this issue, we propose an adaptive streaming method that dynamically adjusts the pacing of LLM streaming output in real-time based on inferred cognitive load. Our approach estimates the cognitive load associated with streaming content and strategically slows down the stream during complex or information-rich segments, thereby freeing computational resources for other users. Our statistical analysis of computational savings, combined with crowdsourced user studies, provides insights into the trade-offs between service efficiency and user satisfaction, demonstrating that our method can significantly reduce computational consumption up to 16.8\\%. This context-aware computational resource management strategy presents a practical framework for enhancing system efficiency in cloud-based conversational AI interfaces without compromising user experience.",
    "categories": [
        "cs.HC",
        "cs.LG"
    ],
    "created": "2025-04-25",
    "updated": "2025-04-28"
}