{
    "title": "MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores",
    "authors": [
        "Fengwei Zhou",
        "Jiafei Song",
        "Wenjin Jason Li",
        "Gengjian Xue",
        "Zhikang Zhao",
        "Yichao Lu",
        "Bailin Na"
    ],
    "institution": [
        "OPPO CTG"
    ],
    "problem_background": "大型语言模型（LLMs）在处理长上下文输入时性能显著提升，但面临推理时间延长和资源消耗增加的挑战，尤其在资源受限的环境中，如边缘设备上。现有长上下文压缩方法存在过平滑（over-smoothing）问题，导致BERT-based模型的token表示相似性过高，影响token分类准确性；同时，任务无关压缩方法可能丢弃稀有但关键的token，降低泛化能力。本工作旨在通过改进任务无关的硬提示压缩方法来解决这些问题，提高压缩效率和效果。",
    "method": "核心思想是通过缓解over-smoothing问题和整合outlier分数来提升BERT-based长上下文压缩器的性能。实现方式包括：训练阶段添加inter-class cosine similarity loss来惩罚token表示之间的过高相似度，从而提高token分类准确性；压缩阶段引入outlier分数（基于Z-score计算），与分类器输出概率结合，以保留稀有但重要的token。主要步骤：在训练时优化损失函数$\\mathcal{L}(\\varphi, \\psi) = \\mathcal{L}_{\\text{CE}}(\\varphi, \\psi) + \\beta \\mathcal{L}_{\\text{CS}}(\\varphi)$，其中$\\mathcal{L}_{\\text{CS}}$是inter-class cosine similarity loss，公式为$\\mathcal{L}_{\\text{CS}}(\\varphi) := S^L = \\frac{1}{|I_p||I_d|} \\sum_{i \\in I_p, j \\in I_d} \\frac{h_i^L \\cdot h_j^L}{||h_i^L||_2 ||h_j^L||_2}$；在压缩时计算标准化outlier分数并整合到压缩指标中，公式为$m_k = \\alpha p_k^{\\text{preserve}} + (1 - \\alpha)s_k^{\\text{norm}}$。",
    "experiment": "实验使用数据集包括MeetingBank（in-domain总结任务）、LongBench（长上下文理解）、GSM8K（数学推理）和BBH（语言推理），目标模型涵盖黑箱API模型（如GPT-3.5-Turbo、GPT-4o-mini）和本地模型（如Qwen2.5-7B-Instruct、Qwen2.5-3B-Instruct）。实验设置旨在验证方法的有效性、泛化能力和加速效果：in-domain实验评估压缩后在相同任务上的性能，out-of-domain实验测试不同任务和模型的鲁棒性；结果显示MOOSComp在各种压缩比下均显著优于基线方法，如在LongBench上准确率提升10%以上，同时在资源受限设备上实现最高3.3倍加速；剔除实验确认了inter-class cosine similarity loss和outlier检测机制的贡献。实验设计合理全面，匹配预期，证明了方法的改进明显。",
    "one_sentence_summary": "本文提出MOOSComp方法，通过在训练中添加inter-class cosine similarity loss缓解over-smoothing问题，并在压缩中整合outlier分数保留关键token，显著提升了任务无关的长上下文压缩性能和泛化能力。",
    "slug": "mooscomp-mitigating-oversmoothing-outlier-scores",
    "keywords": [
        "Large Language Models",
        "Long-Context Compression",
        "Over-Smoothing",
        "Outlier Detection",
        "BERT",
        "Prompt Compression"
    ],
    "further_thoughts": "本方法通过针对性优化损失函数和引入outlier检测，展示了在提升模型泛化性方面的潜力，未来可探索将其扩展到其他transformer-based模型或多模态数据压缩中；over-smoothing问题在各种深度学习任务中普遍存在，或许能与其他反over-smoothing技术（如contrastive normalization）结合；outlier检测机制可进一步动态调整以适应实时任务变化，或与其他压缩策略（如soft prompt方法）融合，以实现更高效的LLM部署。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.16786",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:29:46.232827+00:00",
    "score": 0.7767639042152299,
    "abstract": "Recent advances in large language models have significantly improved their ability to process long-context input, but practical applications are challenged by increased inference time and resource consumption, particularly in resource-constrained environments. To address these challenges, we propose MOOSComp, a token-classification-based long-context compression method that enhances the performance of a BERT-based compressor by mitigating the over-smoothing problem and incorporating outlier scores. In the training phase, we add an inter-class cosine similarity loss term to penalize excessively similar token representations, thereby improving the token classification accuracy. During the compression phase, we introduce outlier scores to preserve rare but critical tokens that are prone to be discarded in task-agnostic compression. These scores are integrated with the classifier's output, making the compressor more generalizable to various tasks. Superior performance is achieved at various compression ratios on long-context understanding and reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x compression ratio on a resource-constrained mobile device.",
    "categories": [
        "cs.CL",
        "cs.LG"
    ],
    "created": "2025-04-23",
    "updated": "2025-04-24"
}