{
    "title": "Block Circulant Adapter for Large Language Models",
    "authors": [
        "Xinyu Ding",
        "Meiqi Wang",
        "Siyu Liao",
        "Zhongfeng Wang"
    ],
    "institution": [
        "Sun Yat-sen University"
    ],
    "problem_background": "大型语言模型（LLMs）的微调由于模型规模庞大而面临巨大挑战，传统全参数微调计算成本高昂，而参数高效微调（PEFT）方法如适配器被广泛采用。本文的工作起点是基于傅里叶域的方法来降低微调成本，具体解决的关键问题是减少存储和计算开销，同时确保训练稳定。作者注意到现有的傅里叶域方法如FourierFT使用2D FFT计算开销大，因此提出利用循环矩阵和一维FFT的特性来优化PEFT方法。",
    "method": "本文提出了一种名为Block Circulant Adapter（BCA）的微调方法，其核心思想是利用块循环矩阵结构来表示权重变化矩阵，从而通过一维FFT操作减少存储和计算复杂度。具体实现包括：首先，通过理论证明和经验模拟分析了块循环矩阵的梯度爆炸风险（Proposition 1-3表明梯度值与块大小p成正比）；然后，设计了一种简单的学习率调整启发式方法，即将学习率α除以块大小p（$$\\alpha \\leftarrow \\alpha / p$$），以确保训练稳定；主要步骤为：冻结预训练模型参数，训练块循环矩阵形式的权重变化矩阵，使用FFT加速矩阵向量乘法（根据公式（4）：$$\\mathbf{h}_i = \\text{IFFT}(\\sum_{j=0}^{q-1} \\text{FFT}(\\mathbf{c}_{i,j}) \\circ \\text{FFT}(\\mathbf{x}_j))$$），并在推理时合并适配器以避免额外开销。",
    "experiment": "实验在多个数据集上验证了BCA的有效性，包括使用RoBERTa-base和RoBERTa-large模型在GLUE基准数据集（CoLA、SST-2、MRPC、STS-B、QNLI、RTE）上进行微调，以及使用LLaMA2-7B模型在Alpaca和GSM8K数据集上测试。此外，附录中还评估了视觉任务（如ViT-base在OxfordPets、CIFAR10等数据集上的性能）。实验设置全面合理：采用多种基线方法（FF、LoRA、VeRA、FourierFT等），进行5次独立运行报告中位数和标准差，指标包括准确率、相关系数等。结果显示，BCA在保持性能的同时显著降低了参数量和FLOPs（如与FourierFT相比，参数量相当但FLOPs减少32倍，与LoRA相比参数量减少16倍），且学习率调整启发式确保了收敛稳定性，实验结果与预期一致，证明了方法的效率和泛化能力。",
    "one_sentence_summary": "本文提出块循环适配器方法，通过利用块循环矩阵和FFT优化LLM的微调过程，显著降低存储和计算成本，同时通过学习率调整确保训练稳定。",
    "slug": "block-circulant-adapter",
    "keywords": [
        "Large Language Models",
        "Parameter-Efficient Fine-Tuning",
        "Block Circulant Matrix",
        "Fast Fourier Transform",
        "Adapter"
    ],
    "further_thoughts": "本文的块循环矩阵结构和梯度爆炸风险缓解策略具有广泛的启发性，例如可以扩展到计算机视觉或多模态模型中，如在ViT或其他Transformer变体上应用，以进一步减少计算开销；此外，学习率调整启发式可能启发其他结构化参数化方法（如低位移秩矩阵）的优化，避免类似梯度问题；同时，与FourierFT的比较提示，结合一维FFT的低成本方法可能在资源受限场景中更具优势，未来可探索与其他领域如信号处理或量子计算的交叉应用，以提升模型效率和鲁棒性。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2505.00582",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:34:25.157642+00:00",
    "score": 0.7185488873717468,
    "abstract": "Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\\times$ less number of parameters than VeRA, $16\\times$ smaller than LoRA and $32\\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks.",
    "categories": [
        "cs.CL",
        "cs.LG"
    ],
    "created": "2025-05-01",
    "updated": "2025-05-02"
}