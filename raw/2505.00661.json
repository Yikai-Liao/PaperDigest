{
    "title": "On the generalization of language models from in-context learning and finetuning: a controlled study",
    "authors": [
        "Andrew K. Lampinen",
        "Arslan Chaudhry",
        "Stephanie C. Y. Chan",
        "Cody Wild",
        "Diane Wan",
        "Alex Ku",
        "Jörg Bornschein",
        "Razvan Pascanu",
        "Murray Shanahan",
        "James L. McClelland"
    ],
    "institution": [
        "Google DeepMind",
        "Stanford University"
    ],
    "problem_background": "论文的出发点是探讨大型语言模型（LLMs）在上下文学习（ICL）和微调（FT）之间的泛化差异。背景是，LLMs虽然展示了强大的能力，但从微调中往往表现出狭隘的泛化，例如无法处理简单关系的逆转或从训练信息中推导出逻辑结论，这可能限制模型的实际应用。而ICL显示出不同的归纳偏差，在某些情况下能更好地泛化。本文通过构建控制的数据集，解决了LLMs泛化机制的关键问题，包括如何隔离预训练知识的影响、比较ICL和FT的泛化性能，以及提出方法来改善FT的泛化能力。",
    "method": "核心思想是通过控制实验比较LLMs在ICL和FT下的泛化能力，并提出数据增强方法来提升FT性能。实现方式包括：构建合成数据集（如简单逆转、syllogism和语义结构benchmark），这些数据集使用无意义词避免预训练污染；评估方法涉及将训练集置于上下文中的ICL和对模型进行FT；关键步骤是数据增强：使用ICL生成推理（如重述和逆转），然后添加到FT训练数据中；此外，还引入句子拆分技术（如独立拆分和累积拆分）来改善FT的学习信号。整个方法强调不修改预训练模型，只在推理或训练过程中调整策略。",
    "experiment": "实验使用多个合成数据集（如逆转诅咒数据集、简单无意义逆转、syllogism和语义结构benchmark），设置合理且全面，因为采用了无污染的数据、控制变量（如多重选择评分评估）和不同模型规模的消融研究。实验结果显示，ICL在许多泛化任务（如逆转和syllogism）中优于FT，数据增强后FT性能显著提升，有时甚至超过ICL，符合预期。例如，在语义结构benchmark中，ICL在逆转和syllogism上的表现更好，而增强FT能改善各种拆分的泛化；实验还验证了句子拆分和不同增强提示的有效性，证明方法改进明显且稳健。",
    "one_sentence_summary": "本文通过控制实验比较了语言模型在上下文学习和微调下的泛化能力，发现上下文学习更灵活，并提出通过数据增强方法显著改善微调的泛化性能。",
    "slug": "language-model-generalization-icl-ft",
    "keywords": [
        "LLM",
        "In-Context Learning",
        "Fine-Tuning",
        "Generalization",
        "Dataset Augmentation"
    ],
    "further_thoughts": "论文揭示了ICL和FT的诱导偏差差异，这可能启发AI系统设计中结合快速适应（类似人类ICL）和深度学习（FT）的优势；数据增强方法类似于认知科学中的'学习通过思考'，通过增加信息可访问性提升性能，未来可扩展到多模态模型或强化学习中，以减少对大规模数据的依赖；此外，探索如何在真实世界任务中应用这些发现，或与新兴模型（如o1系列）比较泛化机制，能够进一步深化对LLM学习过程的理解。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2505.00661",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:33:55.487593+00:00",
    "score": 0.8200374064237992,
    "abstract": "Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.",
    "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
    ],
    "created": "2025-05-01",
    "updated": "2025-05-02"
}