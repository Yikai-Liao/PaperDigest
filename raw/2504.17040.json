{
    "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
    "authors": [
        "Zhenhailong Wang",
        "Senthil Purushwalkam",
        "Caiming Xiong",
        "Silvio Savarese",
        "Heng Ji",
        "Ran Xu"
    ],
    "institution": [
        "University of Illinois Urbana-Champaign",
        "Salesforce Research"
    ],
    "problem_background": "本工作的起点是解决视觉语言模型（VLMs）中的计算效率问题。VLMs通常使用视觉编码器提取图像特征，并生成固定长度的视觉令牌，这些令牌数量不依赖于图像内容的复杂性，导致不必要的计算开销。例如，在处理高分辨率图像时，视觉令牌可能占主导地位，而简单图像也使用相同数量的令牌，造成资源浪费。论文解决了关键问题：如何动态调整视觉令牌的数量以适应图像复杂性，同时在不进行额外训练的情况下保持模型性能，从而减少计算负担并提高VLMs的实际应用效率。",
    "method": "* **核心思想：** DYMU框架旨在通过训练-free的方法动态减少VLMs中的视觉令牌数量，同时保持下游任务性能。具体来说，它包括动态令牌合并（DToMe）和虚拟令牌取消合并（VTU）两个组件，前者针对视觉编码器，后者针对语言模型解码器。\n* **如何实现：** DToMe基于图像复杂性合并相似的视觉令牌，使用二部图软匹配策略：首先，将令牌分为两组A和B，然后通过相似性得分$S^i[t] = (k^i[t]^T k^i[t_B])$选择得分高于阈值$\\tau^i$的边进行合并，合并公式为$x_i[t_B] \\leftarrow \\frac{x_i[t] \\cdot |\\mathbf{P}_i[t]| + x_i[t_B] \\cdot |\\mathbf{P}_i[t_B]|}{|\\mathbf{P}_i[t]| + |\\mathbf{P}_i[t_B]|}$，并更新位置集$\\mathbf{P}_i[t_B] \\leftarrow \\mathbf{P}_i[t_B] \\cup \\mathbf{P}_i[t]$。阈值$\\tau^i$通过批量图像计算，确保平均合并令牌数符合预设。VTU则在语言模型中使用RoPE，重建注意力矩阵，例如$A = CMQ_{\\text{un}}K_{\\text{un}}^{\\top}M^{\\top}C + SMQ_{\\text{un}}K_{\\text{un}}^{\\top}M^{\\top}S + SM(Q_{\\text{un}} \\times K_{\\text{un}}^{\\top})M^{\\top}C - CM(Q_{\\text{un}} \\times K_{\\text{un}}^{\\top})M^{\\top}S$，以模拟完整序列，而不实际扩展令牌。\n* **主要步骤：** 首先，使用多样图像批量计算层级阈值；推理时，对每个图像动态合并视觉令牌；然后在语言模型中应用VTU重建注意力动态，并重新合并令牌以减少计算。",
    "experiment": "* **实验设置：** 论文在多个基准上评估DYMU，包括GQA、MMBench、MME、POPE、ScienceQA、SEED-IMG、TextVQA、MMVet和LLaVA-Bench等。实验使用不同VLM架构，如LLaVA-1.5和LLaVA-OneVision，视觉编码器包括CLIP和SigLIP。DYMU的变体（low、mid、high）通过设置不同平均令牌减少率来测试，阈值基于LLaVA指令调优数据计算。实验设计全面，涵盖定量评估（如性能与令牌减少率）、消融实验（如VTU的影响、阈值数据集敏感性）和定性分析（如图像复杂性与令牌数的相关性）。\n* **结果：** DYMU在减少视觉令牌32%-85%的情况下，性能保持在基线水平的97.7%-100.4%，例如DYMU-low在LLaVA-1.5上平均性能为54.5，与完整模型的55.8相近。相比固定长度方法，DYMU在复杂图像上表现更好，消融实验显示VTU显著提升性能。结果符合预期，证明了动态调整的有效性和训练-free方法的实用性，同时实验开销低，仅增加少量计算。\n* **是否符合预期：** 是，实验结果显示方法改进明显，效率提升显著，且实验设置合理全面，验证了DYMU的鲁棒性和泛化能力。",
    "one_sentence_summary": "本文提出DYMU框架，通过动态令牌合并和虚拟取消合并的训练-free方法，显著提高了VLMs的计算效率，同时在多个基准上保持了与完整模型相似的性能。",
    "slug": "dymu-dynamic-merging-virtual-unmerging",
    "keywords": [
        "Vision Language Model",
        "Token Merging",
        "Efficiency",
        "Dynamic Compression",
        "RoPE"
    ],
    "further_thoughts": "这个方法启发了在其他模态如视频或3D数据中应用动态压缩技术，以减少冗余信息；此外，可以探索与高级视觉工具（如背景移除或物体检测）的结合，进一步提升效率；同时，针对空间敏感任务如TextVQA，未来可融入更多先验知识来最小化性能损失；这也与模型蒸馏或稀疏注意力机制相关，潜在地推动更泛化的高效多模态模型发展。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.17040",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:29:06.935062+00:00",
    "score": 0.6391008389157905,
    "abstract": "We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. Unlike previous approaches, our method dynamically adapts token compression to the content of the image and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models across diverse VLM architectures, including the recently popularized AnyRes-based visual encoders. Furthermore, through qualitative analyses, we demonstrate that DToMe effectively adapts token reduction based on image complexity and, unlike existing systems, provides users more control over computational costs. Project page: https://mikewangwzhl.github.io/dymu/.",
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "created": "2025-04-23",
    "updated": "2025-04-25"
}