{
    "title": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections",
    "authors": [
        "Narek Maloyan",
        "Dmitry Namiot"
    ],
    "institution": [
        "Independent Researchers"
    ],
    "problem_background": "大型语言模型（LLMs）被越来越多地用于自动评估文本质量、代码正确性和论点强度，提供可扩展且成本效益高的替代人类评估的方法。研究显示LLM判断与人类判断高度相关，但这些系统易受adversarial attacks，尤其是prompt injection攻击的影响，可能操纵评估结果，导致不可靠性。本文从学术文献和Kaggle比赛中获取insights，调查LLM-as-a-judge系统的漏洞，旨在揭示其潜在风险并提出改进建议。",
    "method": "本文提出一个全面框架，用于开发和评估针对LLM-as-a-judge系统的adversarial attacks。该框架区分content-author attacks（恶意内容提交）和system-prompt attacks（评估模板被破坏），包括三个组件：framework component（确保注入与上下文融合）、separator component（创建上下文边界，如使用复杂词汇或格式中断）和disruptor component（执行恶意指令，如直接命令输出特定分数）。攻击变体有Basic Injection（简单指令注入）、Complex Word Bombardment（使用复杂词汇轰炸）、Contextual Misdirection（结合所有组件的复杂攻击）和Adaptive Search-Based Attack（使用遗传算法基于模型反馈优化攻击字符串）。实验中通过系统比较这些攻击，强调不修改模型本身，只在推理阶段注入攻击。",
    "experiment": "实验涉及五种模型（Gemma-3-27B-Instruct、Gemma-3-4B-Instruct、Llama-3.2-3B-Instruct、GPT-4和Claude-3-Opus）、四种评价任务（ppe human preference、search arena v1 7k、mt bench和code review），并测试了多种防御机制（如perplexity check、instruction filtering和multi-model committees）。每个条件使用n=50个prompt，采用bootstrap置信区间和t检验进行统计分析。结果显示，Adaptive Search-Based Attack成功率最高（73.8%），小型模型更易受攻击（Gemma-3-4B平均65.9%），multi-model committees能显著降低攻击成功率（7模型委员会成功率降至10.2-19.3%）。实验设置全面合理，覆盖不同模型和任务，统计方法严谨，结果符合预期，证明了攻击的有效性和防御策略的可行性。",
    "one_sentence_summary": "本文通过提出攻击框架和实验评估，揭示了LLM-as-a-judge系统的prompt injection漏洞，并推荐使用多模型委员会等策略提升鲁棒性。",
    "slug": "adversarial-attacks-llm-judge-systems",
    "keywords": [
        "Large Language Models",
        "Adversarial Attacks",
        "Prompt Injection",
        "LLM-as-a-Judge",
        "Evaluation Systems",
        "AI Safety"
    ],
    "further_thoughts": "论文中多模型委员会的防御策略启发我们，在其他AI应用中采用ensemble methods可以提高整体鲁棒性；攻击方法的transferability差异提示需要开发更通用的防御机制；此外，与AdvPrompter等工作的比较显示，AI安全领域的攻击与防御是动态的arms race，未来可能需要结合formal verification等方法来提升LLM的安全性，并探索其在多模态系统中的扩展。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.18333",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:30:57.109467+00:00",
    "score": 0.7206529831337822,
    "abstract": "LLM as judge systems used to assess text quality code correctness and argument strength are vulnerable to prompt injection attacks. We introduce a framework that separates content author attacks from system prompt attacks and evaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3 Opus on four tasks with various defenses using fifty prompts per condition. Attacks achieved up to seventy three point eight percent success smaller models proved more vulnerable and transferability ranged from fifty point five to sixty two point six percent. Our results contrast with Universal Prompt Injection and AdvPrompter We recommend multi model committees and comparative scoring and release all code and datasets",
    "categories": [
        "cs.CR",
        "cs.CL"
    ],
    "created": "2025-04-25",
    "updated": "2025-04-28"
}