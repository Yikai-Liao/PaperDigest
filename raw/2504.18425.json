{
    "title": "Kimi-Audio Technical Report",
    "authors": [
        "KimiTeam",
        "Ding Ding",
        "Zeqian Ju",
        "Yichong Leng",
        "Songxiang Liu",
        "Tong Liu",
        "Zeyu Shang",
        "Kai Shen",
        "Wei Song",
        "Xu Tan",
        "Heyi Tang",
        "Zhengtao Wang",
        "Chu Wei",
        "Yifei Xin",
        "Xinran Xu",
        "Jianwei Yu",
        "Yutao Zhang",
        "Xinyu Zhou",
        "Y. Charles",
        "Jun Chen",
        "Yanru Chen",
        "Yulun Du",
        "Weiran He",
        "Zhenxing Hu",
        "Guokun Lai",
        "Qingcheng Li",
        "Yangyang Liu",
        "Weidong Sun",
        "Jianzhou Wang",
        "Yuzhi Wang",
        "Yuefeng Wu",
        "Yuxin Wu",
        "Dongchao Yang",
        "Hao Yang",
        "Ying Yang",
        "Zhilin Yang",
        "Aoxiong Yin",
        "Ruibin Yuan",
        "Yutong Zhang",
        "Zaida Zhou"
    ],
    "institution": [
        "Moonshot AI"
    ],
    "problem_background": "传统音频建模受限于人工智能的发展，每个音频处理任务（如语音识别、情感识别、声音事件检测和语音对话）都是单独处理的，这导致了模型的局限性。然而，音频是自然顺序的，语音与文本有严格的对应关系，因此适合利用大型语言模型（LLMs）的快速发展来进行音频建模。本文的工作起点是构建一个通用的音频基础模型，能够处理音频理解、生成和对话等多种任务。关键问题包括：之前的工作往往不通用，仅专注于特定任务（如音频理解或生成）、预训练不足（仅进行下游任务微调）、代码和模型不可访问（限制了社区的发展）。本文解决了这些问题，通过提出一个开源的、全面的音频基础模型来实现音频任务的统一处理。",
    "method": "核心思想是通过统一的架构设计一个音频基础模型，包括音频分词器、音频LLM和音频逆分词器。具体实现包括：\n- **音频分词器**：使用12.5Hz的音频分词器，将输入音频转换为离散语义令牌（通过Whisper编码器的向量量化）和连续声学向量（通过Whisper特征提取器下采样到12.5Hz），以增强感知能力。\n- **音频LLM**：基于预训练的文本LLM（如Qwen2.5 7B）初始化，采用共享Transformer层处理多模态输入，然后分支为文本头和音频头，分别生成文本令牌和离散音频语义令牌。\n- **音频逆分词器**：基于流匹配的方法，将离散语义令牌转换为音频波形，采用块状流式框架和前瞻机制（look-ahead）来减少延迟。\n数据处理管道包括语音增强、说话人二值化、转录和过滤等步骤，以构建高质量的数据集。训练方法分为预训练和监督微调：预训练任务包括单模态预训练（文本和音频）、音频-文本映射（ASR和TTS风格任务）和音频-文本交错预训练；监督微调使用任务特定的指令数据。整个过程不修改原始模型，仅在推理时调整采样。",
    "experiment": "实验使用大规模数据集，包括预训练数据集（超过1300万小时音频数据，覆盖语音、声音和音乐等模态）和SFT数据集（约30万小时，包含ASR、音频理解、语音对话等任务的开源和内部数据）。实验设置合理且全面，涉及多种基准测试，如LibriSpeech（WER为1.28和2.42）、MMAU（音乐、声音、语音得分分别为61.68、73.27、60.66）、ClothoAQA（得分71.24）和语音对话主观评估（平均得分3.90）。结果与预期相符，Kimi-Audio在保持教师模型准确率的同时，显著优于基线模型（如Qwen2-Audio和Baichuan-Audio），展示了方法改进的明显效果。实验还包括消融研究和公平比较工具，确保结果的可靠性和泛化能力。",
    "one_sentence_summary": "本文提出Kimi-Audio，一个开源的音频基础模型，通过结合音频分词、LLM处理和逆分词的统一架构，以及大规模多模态训练，实现了音频理解、生成和对话的多任务SOTA性能。",
    "slug": "kimi-audio-technical-report",
    "keywords": [
        "Audio Foundation Model",
        "LLM",
        "Speech Recognition",
        "Audio Generation",
        "Multimodal Training"
    ],
    "further_thoughts": "论文中提到的挑战，如从音频转录向音频描述的转变、开发更全面的音频表示和减少对ASR/TTS的依赖，启发我们思考音频AI的未来发展：例如，将音频模型与视觉或文本模态深度融合，可能实现更智能的多模态交互系统；此外，探索无监督或自监督学习方法来处理原生音频数据，能够超越当前基于转录的限制，提升模型的泛化能力和真实世界应用潜力。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.18425",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:32:25.991121+00:00",
    "score": 0.646987778672481,
    "abstract": "We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage a 12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous features as input and discrete tokens as output, and develop a chunk-wise streaming detokenizer based on flow matching. We curate a pre-training dataset that consists of more than 13 million hours of audio data covering a wide range of modalities including speech, sound, and music, and build a pipeline to construct high-quality and diverse post-training data. Initialized from a pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text data with several carefully designed tasks, and then fine-tuned to support a diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio achieves state-of-the-art performance on a range of audio benchmarks including speech recognition, audio understanding, audio question answering, and speech conversation. We release the codes, model checkpoints, as well as the evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.",
    "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM",
        "cs.SD"
    ],
    "created": "2025-04-25",
    "updated": "2025-04-28"
}