{
    "title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning",
    "authors": [
        "Song Wang",
        "Xiaolu Liu",
        "Lingdong Kong",
        "Jianyun Xu",
        "Chunyong Hu",
        "Gongfan Fang",
        "Wentong Li",
        "Jianke Zhu",
        "Xinchao Wang"
    ],
    "institution": [
        "Zhejiang University",
        "National University of Singapore",
        "AD Lab, CaiNiao, Alibaba",
        "Nanjing University of Aeronautics and Astronautics"
    ],
    "problem_background": "点云学习的自监督表示学习已被证明能提升预训练模型在各种任务中的性能，但随着模型复杂度的增加，全量微调需要大量计算和存储资源，可能破坏预训练知识并导致泛化能力下降。此外，现有的参数高效微调（PEFT）方法虽能缓解这些问题，但多依赖复杂的适配器和提示机制，导致参数增加或性能不佳。本文的工作起点是提出一种简单有效的PEFT方法，针对点云模型的微调需求，解决高效捕获全局和局部特征的问题，同时减少可训练参数。",
    "method": "* **核心思想：** PointLoRA结合低秩适配（LoRA）和多尺度令牌选择，旨在参数高效地微调点云模型。LoRA通过注入低秩矩阵捕获全局特征，而多尺度令牌选择则提取并整合局部几何信息。\n* **工作原理：** 在点云Transformer的qkv投影和FFN层中，注入LoRA层，即更新权重为$W_{\\text{update}} = W_p + \\Delta W = W_p + W_u \\cdot W_d$，其中$W_u \\in \\mathbb{R}^{d \\times r}$和$W_d \\in \\mathbb{R}^{r \\times d}$是低秩矩阵，r是秩。多尺度令牌选择使用最远点采样（FPS）和k近邻（k-NN）生成不同尺度的中心点和邻域点，然后通过Mini-PointNet嵌入为令牌。掩码预测器（Mask Predictor）计算重要性分数$s^m = \\text{Sigmoid}(\\text{MLP}(T_p^m))$，选择Top-K令牌，并通过共享的Prompt MLP整合这些局部特征与LoRA输出，形成更新输出$O_{\\text{update}} = \\text{Prompt MLP}(T_{\\text{input}}, S_p) + \\Delta W \\cdot (T_{\\text{input}}, S_p)$。\n* **主要步骤：** (1) 令牌化点云数据；(2) 注入LoRA层捕获全局信息；(3) 通过多尺度采样和掩码预测选择局部令牌；(4) 使用Prompt MLP融合全局和局部特征；(5) 在微调过程中仅更新低秩矩阵和相关参数。",
    "experiment": "* **数据集和设置：** 本文使用ScanObjectNN、ModelNet40和ShapeNetPart三个公共数据集，涵盖分类、少样本学习和分割任务。实验基于预训练模型如Point-MAE和ReCon，仅微调3.43%的参数。设置合理，包括数据增强、学习率调度（如余弦退火）和不同任务的损失函数（如分类的交叉熵损失和分割的mIoU损失），并引入掩码预测器的正则化损失$\\mathcal{L}_{\\text{mask}} = -\\frac{1}{N_{\\text{total}}} \\sum_{i=1}^{N_{\\text{total}}} (s_i \\log(s_i + \\epsilon) + (1 - s_i) \\log(1 - s_i + \\epsilon))$以监督令牌选择。\n* **结果分析：** 在ScanObjectNN的PB-T50-RS变体上，PointLoRA达到85.53%的准确率，优于全量微调和其它PEFT方法；ModelNet40少样本学习中表现最佳或次佳；ShapeNetPart分割任务中mIoU竞争性。改进明显，参数减少显著，实验设置全面，覆盖不同任务和模型，结果符合预期，验证了方法在保持性能的同时提升效率。",
    "one_sentence_summary": "本文提出PointLoRA方法，通过低秩适配和多尺度令牌选择，实现点云模型的参数高效微调，显著减少可训练参数同时在多个数据集上达到竞争性性能。",
    "slug": "pointlora",
    "keywords": [
        "Point Cloud",
        "Low-Rank Adaptation",
        "Token Selection",
        "Parameter-Efficient Fine-Tuning",
        "Self-Supervised Learning"
    ],
    "further_thoughts": "PointLoRA的方法启发我们思考低秩适配在其他3D任务中的潜力，例如结合视觉或文本模态的跨模态学习，以提升泛化能力。同时，点云的局部特征提取可以与图神经网络整合，进一步优化性能。未来，可以探索自适应秩选择或结合生成模型来处理噪声数据。另外，考虑到点云数据的稀疏性，应用到实时应用如自动驾驶中可能带来更高效率，但需注意隐私和偏差问题。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.16023",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:28:56.363836+00:00",
    "score": 0.5332449210473782,
    "abstract": "Self-supervised representation learning for point cloud has demonstrated effectiveness in improving pre-trained model performance across diverse tasks. However, as pre-trained models grow in complexity, fully fine-tuning them for downstream applications demands substantial computational and storage resources. Parameter-efficient fine-tuning (PEFT) methods offer a promising solution to mitigate these resource requirements, yet most current approaches rely on complex adapter and prompt mechanisms that increase tunable parameters. In this paper, we propose PointLoRA, a simple yet effective method that combines low-rank adaptation (LoRA) with multi-scale token selection to efficiently fine-tune point cloud models. Our approach embeds LoRA layers within the most parameter-intensive components of point cloud transformers, reducing the need for tunable parameters while enhancing global feature capture. Additionally, multi-scale token selection extracts critical local information to serve as prompts for downstream fine-tuning, effectively complementing the global context captured by LoRA. The experimental results across various pre-trained models and three challenging public datasets demonstrate that our approach achieves competitive performance with only 3.43% of the trainable parameters, making it highly effective for resource-constrained applications. Source code is available at: https://github.com/songw-zju/PointLoRA.",
    "categories": [
        "cs.CV"
    ],
    "created": "2025-04-22",
    "updated": "2025-04-23"
}