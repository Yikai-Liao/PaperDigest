{
    "title": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs",
    "authors": [
        "Tiancheng Gu",
        "Kaicheng Yang",
        "Ziyong Feng",
        "Xingjun Wang",
        "Yanzhao Zhang",
        "Dingkun Long",
        "Yingda Chen",
        "Weidong Cai",
        "Jiankang Deng"
    ],
    "institution": [
        "The University of Sydney",
        "DeepGlint",
        "Tongyi Lab, Alibaba Group",
        "Imperial College London"
    ],
    "problem_background": "大型语言模型和多模态模型的快速发展使得多模态表示学习变得至关重要。CLIP框架虽然在图像文本检索中表现出色，但存在三个关键局限性：（1）文本标记长度限制为77个标记，阻碍处理详细描述；（2）独立图像文本编码架构，削弱了在指令敏感任务中的效果；（3）缺乏组合性和袋-of-words行为，导致语言理解不足。同时，多模态大语言模型（MLLMs）在视觉语言理解上取得了显著进展，但其在学习可转移的多模态表示方面的潜力尚未被充分探索。本工作旨在通过MLLMs学习通用的多模态嵌入，解决这些问题，提高下游任务的判别性和组合能力。",
    "method": "UniME框架采用两阶段方法来学习通用的多模态嵌入。第一阶段是文本判别知识蒸馏，使用强大的LLM教师模型（如NV-Embed V2）通过Kullback-Leibler（KL）散度最小化来增强MLLM的语言组件。具体而言，从MLLM的语言模型部分提取文本嵌入，并与教师模型的嵌入对齐，损失函数为：\n$$ \\mathcal{L}_{KL} = \\sum_{i=1}^{n} \\text{KL}\\left( \\frac{\\exp\\left( e_{s_i}^{\\top} e_{s_i} / \\tau \\right)}{\\sum_{j=1}^{n} \\exp\\left( e_{s_j}^{\\top} e_{s_i} / \\tau \\right)} \\ \\middle\\| \\ \\frac{\\exp\\left( e_{t_i}^{\\top} e_{t_i} / \\tau \\right)}{\\sum_{j=1}^{n} \\exp\\left( e_{t_j}^{\\top} e_{t_i} / \\tau \\right)} \\right) $$\n第二阶段是硬负例增强指令微调，首先通过相似性阈值过滤假负例，然后采样多个硬负例，使用Noise Contrastive Estimation（InfoNCE）损失来优化模型。损失函数为：\n$$ \\mathcal{L} = -\\log \\frac{\\exp(\\cos(e_q, e_c^+) / \\tau)}{\\exp(\\cos(e_q, e_c^+) / \\tau) + \\sum_{l=1}^{k} \\exp(\\cos(e_q, e_{c_j^-}) / \\tau)} $$\n这种方法不修改原始MLLM模型，仅通过高效的训练策略提高其嵌入能力。",
    "experiment": "实验在MMEB基准（包括20个in-distribution和16个out-of-distribution数据集）和多个检索任务（如Flickr30K、MS-COCO、ShareGPT4V、Urban1K和SugarCrepe）上进行，使用Phi3.5-V和LLaVA-1.6作为基础模型。训练设置合理：第一阶段使用NLI数据集进行知识蒸馏，第二阶段使用MMEB的662k对数据进行指令微调，采用QLoRA和GradCache优化计算效率。结果显示UniME在所有任务上均显著优于基线模型，例如在MMEB上平均精度提高了4.1%，在长标题检索任务中比EVA-CLIP提升14.8%，组合检索任务中也取得了9.1%的改善。这些结果符合预期，证明了UniME框架在提升判别性和组合能力方面的有效性，实验设计全面，覆盖了不同模态和任务类型。",
    "one_sentence_summary": "本文提出UniME框架，通过文本判别知识蒸馏和硬负例增强指令微调，利用多模态大语言模型学习通用的多模态嵌入，提高了下游任务的判别性和组合能力。",
    "slug": "unime-universal-multimodal-embedding",
    "keywords": [
        "Multimodal LLM",
        "Universal Embedding",
        "Knowledge Distillation",
        "Hard Negative Sampling",
        "Contrastive Learning"
    ],
    "further_thoughts": "这项工作展示了知识蒸馏和硬负例采样的有效性，可以扩展到其他模态如音频或视频的融合中，或与其他技术结合以提升泛化能力。例如，结合联邦学习可能提高数据隐私保护；同时，硬负例策略可启发推荐系统或异常检测领域中处理困难样本的优化。另外，考虑到不同MLLM（如Qwen2-VL和LLaVA-OneVision）的特性，未来可探索模型架构的适应性以进一步提升嵌入质量。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.17432",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:29:41.228190+00:00",
    "score": 0.6980954536482764,
    "abstract": "The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored.In this work, we present UniME (Universal Multimodal Embedding), a novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLMś language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short and long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities.",
    "categories": [
        "cs.CV"
    ],
    "created": "2025-04-24",
    "updated": "2025-04-25"
}