{
    "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness",
    "authors": [
        "Junsheng Huang",
        "Zhitao He",
        "Sandeep Polisetty",
        "Qingyun Wang",
        "May Fung"
    ],
    "institution": [
        "Hong Kong University of Science and Technology",
        "University of Illinois",
        "UMass Amherst"
    ],
    "problem_background": "大型语言模型（LLMs）在知识密集型任务中广泛应用，但它们经常生成不存在的事实，称为幻觉问题，这会降低其可靠性。之前的相关研究主要关注单问题设置，即模型一次处理一个问题，而多问题设置（一个输入包含多个子问题，需要模型同时提取和回答）尚未得到充分探索。这种设置更具挑战性，可能导致上下文遮蔽和推理混乱，尤其是在涉及共享上下文的场景中（如任务指令或示例），因此本文旨在通过增强LLMs对知识边界的意识来改善多问题设置下的置信度估计，从而减少幻觉。",
    "method": "本文提出了一种名为Multiple Answers and Confidence Stepwise Tuning（MAC-Tuning）的方法，其核心思想是将答案预测和置信度估计的学习过程分开，以增强LLMs在多问题设置下的知识边界意识。具体实现包括几个关键步骤：首先，通过随机组合多个单问题构建多问题数据集，并利用该数据集与模型输出比较ground-truth答案来识别知识边界，标记每个问题的置信度标签（如\"I am sure\"或\"I am unsure\"）。然后，创建两个数据集：DMultQA用于答案预测对，输入为多个问题，输出为对应答案；DMultQA,C用于QA-置信度对，输入包括问题-答案对和置信度表达指令，输出为置信度水平。训练过程采用两步监督微调：第一步优化答案预测，目标函数为\n$$\n\\max_{\\Theta_0} \\sum_{(Q,A)\\in D_{MultiQA}} \\log P(A|Q;\\Theta_0)\n$$\n第二步优化置信度预测，目标函数为\n$$\n\\max_{\\Theta_1} \\sum_{(Q,A,C)\\in D_{MultiQA,C}} \\log P(C|Q,A;\\Theta_1)\n$$\n其中Q、A和C分别表示多个问题、答案和置信度集，\\Theta_0和\\Theta_1是模型参数。这种分步方法确保了模型在不牺牲答案准确性的前提下，提高了置信度校准能力。",
    "experiment": "实验设计全面且合理，旨在验证MAC-Tuning方法的有效性。使用多种数据集，包括独立设置（如CoQA、ParaRel、GSM、MMLU）和顺序设置（如MTI-Bench、SQA），这些数据集覆盖了问答和多选格式。实验设置了基线模型LLaMA3及其变体（如QA-Only、Single-QA、Merge-AC），并采用平均精度（AP）、期望校准误差（ECE）和准确率作为评估指标。AP用于衡量模型在正确和错误答案上的置信度排序精度，ECE用于评估预测置信度与实际正确性的匹配度，准确率计算模型在表达确定性的问题上正确回答的比例。结果显示，MAC-Tuning在所有数据集上均表现出色，AP分数比基线提高高达25%，ECE显著降低，准确率平均提升23.7%。这种改进符合预期，因为分步学习帮助模型更好地处理多问题设置中的知识边界。消融实验进一步证明了分离学习过程的重要性，而跨域和不同问题数量的分析显示了方法的泛化能力，例如在SQA上微调后测试其他数据集时，性能仍优于基线。实验开销合理，主要使用LoRA fine-tuning，资源消耗控制在Nvidia A100 GPU上。",
    "one_sentence_summary": "本文提出MAC-Tuning方法，通过分步微调分离答案预测和置信度估计，提升LLMs在多问题设置下的知识边界意识，显著减少幻觉并改善性能。",
    "slug": "mac-tuning-llm-reasoning",
    "keywords": [
        "LLM",
        "Multi-Problem",
        "Confidence Estimation",
        "Knowledge Boundary",
        "Hallucination",
        "Fine-Tuning"
    ],
    "further_thoughts": "本文的方法强调了在多问题设置下分离学习答案和置信度的优势，这可能启发在其他复杂推理任务中应用类似策略，例如结合检索增强生成（RAG）来进一步减少幻觉，或扩展到多模态设置以处理视觉和文本混合问题。同时，考虑到提示敏感性的局限性，未来可以探索更鲁棒的提示设计或无提示方法；此外，与Zhang et al. (2024)的灵感来源类似，将此方法与自洽性检查或多代理辩论结合，可能提升模型的泛化能力，尤其在实时应用中。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.21773",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:32:18.134262+00:00",
    "score": 0.6307842686459223,
    "abstract": "With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments demonstrate that our method outperforms baselines by up to 25% in average precision.",
    "categories": [
        "cs.CL",
        "cs.AI"
    ],
    "created": "2025-04-30",
    "updated": "2025-05-01"
}