{
    "title": "TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs",
    "authors": [
        "Ye Qiao",
        "Zhiheng Chen",
        "Yifan Zhang",
        "Yian Wang",
        "Sitao Huang"
    ],
    "institution": [
        "University of California, Irvine"
    ],
    "problem_background": "大型语言模型（LLMs）的快速发展使得它们在自然语言处理任务中表现出色，但边缘部署面临巨大挑战，包括高计算需求、高内存使用和能源消耗。低位量化方法（如BitNet和DeepSeek）将权重压缩到1.58位，显著减少了模型大小和能源成本，但边缘设备（如FPGA）仍受限于芯片资源、功率预算和预填充阶段的延迟，而预填充延迟在边缘应用中是用户体验和安全的关键瓶颈。现有工作多关注模型量化或软件加速，但缺乏对极端低位宽LLMs的系统性硬件-软件协同优化，尤其是忽略了预填充阶段的需求，本文的工作起点是开发首个支持预填充和解码两个阶段的FPGA加速器，以实现高效、低延迟的边缘LLM推理。",
    "method": "* **核心思想：** TeLLMe是一种针对边缘FPGA的能量高效三元LLM加速器，旨在通过硬件-软件协同优化来支持预填充和解码阶段的端到端推理。核心在于最小化资源使用和数据移动，通过表查找矩阵乘法引擎、融合注意力模块和集成标准化/量化-反量化单元来实现高效计算。\n* **工作原理：** \n  - **表查找矩阵乘法（TL-based Ternary MatMul）：** 利用FPGA的LUT资源，预处理权重为分组索引（每组G个三元值编码为索引），在线阶段通过表查找和并行累加计算激活和权重的乘积。例如，对于三元权重，公式为$\\mathbf{A} \\otimes \\mathbf{W} = \\mathbf{A} \\otimes \\mathbf{W}_0$（权重在\\{-1, 0, 1\\}范围内），通过分组激活和多表并行访问（如T表和Q并行度）减少重复计算和资源消耗，算法如算法1所示。\n  - **融合注意力模块：** 对于预填充阶段，引入反向重排序方案（Reversed Attention），从序列末尾开始计算，避免因因果掩码导致的冗余计算，并融合Q-K乘法、Softmax和S-V乘法，减少DRAM访问。公式为：\n    $$\n    \\begin{cases}\n    m^{(1)} = s^{(1)} \\\\\n    \\ell^{(1)} = e^{s^{(1)} - m^{(1)}} \\\\\n    \\mathbf{o}^{(1)} = e^{s^{(1)} - m^{(1)}} \\mathbf{v}^{(1)} \\\\\n    m^{(2)} = \\max\\left(m^{(1)}, s^{(2)}\\right) = m \\\\\n    \\ell^{(2)} = e^{m^{(1)} - m^{(2)}} \\ell^{(1)} + e^{s^{(2)} - m^{(2)}} \\\\\n    \\mathbf{p}^{(2)} = e^{s^{(2)} - m^{(2)}} / \\ell^{(2)} \\\\\n    \\mathbf{o}^{(2)} = \\mathbf{o}^{(2)} / \\ell^{(2)} = \\mathbf{o}\n    \\end{cases}\n    $$\n    对于解码阶段，重用注意力硬件进行LM Head计算，采用解耦执行以减少资源。\n  - **其他优化：** 融合RMSNorm和量化-反量化操作，减少数据移动；激活函数如SiLU与线性层融合以隐藏延迟。\n* **主要步骤：** 包括权重预处理、在线表查找计算、注意力调度和功能单元融合，确保在FPGA上高效流水线执行。",
    "experiment": "* **实验设置：** 本文在AMD KV260 FPGA上实现TeLLMe加速器，使用Vitis HLS和Vivado 2023.1，频率250 MHz，功率预算7W。模型基于1.58位三元权重和8位激活的BitNet，测试上下文长度至1024 tokens。实验包括解码吞吐量（tokens/s）和预填充延迟（s），比较了不同提示长度（64-128 tokens）和生成长度。设置合理，考虑了FPGA资源约束和内存带宽限制。\n* **结果分析：** TeLLMe在512 tokens上下文下达到9.51 tokens/s吞吐量，1024 tokens下约8 tokens/s；预填充延迟为64 tokens提示时0.55s，128 tokens时1.15s，符合预期，展示了在保持低功率下的高效性能。与基线比较（如SECDA、LlamaF）显示16.4倍改进，支持预填充是关键优势。实验全面，包含资源分解（LUT、FF、BRAM等）和与移动CPU（如Snapdragon 8 Gen 3）的对比，证明了方法的有效性和泛化性，结果与预期的能量效率和延迟优化相符。",
    "one_sentence_summary": "本文提出TeLLMe，一种能量高效的三元LLM FPGA加速器，通过表查找矩阵引擎和反向注意力优化，支持预填充和解码阶段，在7W功率下实现高达9.51 tokens/s吞吐量和低预填充延迟。",
    "slug": "tellme-ternary-llm-accelerator",
    "keywords": [
        "LLM",
        "FPGA",
        "Ternary Quantization",
        "Hardware Acceleration",
        "Prefill Optimization",
        "Decode Phase",
        "Energy Efficiency"
    ],
    "further_thoughts": "这项工作突显了硬件-软件协同优化的重要性，或许可以扩展到其他硬件如ASIC或GPU上，进一步探索不同量化策略（如二元或混合量化）的硬件适应性；同时，结合其他研究（如DeepSeek的混合量化），TeLLMe的预填充优化思路可能启发更泛化的注意力机制设计，以提升长序列模型在资源受限环境中的性能，并推动边缘AI在隐私保护和实时应用中的发展。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.16266",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:29:32.742844+00:00",
    "score": 0.7002065414573913,
    "abstract": "Deploying large language models (LLMs) on edge platforms is challenged by their high computational and memory demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek) compress weights to as little as 1.58 bits with minimal accuracy loss, edge deployment is still constrained by limited on-chip resources, power budgets, and the often-neglected latency of the prefill phase. We present TeLLMe, the first ternary LLM accelerator for low-power FPGAs (e.g., AMD KV260) that fully supports both prefill and autoregressive decoding using 1.58-bit weights and 8-bit activations. Our contributions include: (1) a table-lookup matrix engine for ternary matmul that merges grouped activations with online precomputation to minimize resource use; (2) a fused, bandwidth-efficient attention module featuring a reversed reordering scheme to accelerate prefill; and (3) a tightly integrated normalization and quantization--dequantization unit optimized for ultra-low-bit inference. Under a 7W power budget, TeLLMe delivers up to 9 tokens/s throughput over 1,024-token contexts and prefill latencies of 0.55--1.15 s for 64--128 token prompts, marking a significant energy-efficiency advance and establishing a new edge FPGA benchmark for generative AI.",
    "categories": [
        "cs.AR",
        "cs.LG"
    ],
    "created": "2025-04-24",
    "updated": "2025-04-28"
}