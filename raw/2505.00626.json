{
    "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)",
    "authors": [
        "Zihao Wang",
        "Yibo Jiang",
        "Jiahao Yu",
        "Heqing Huang"
    ],
    "institution": [
        "University of Chicago",
        "Northwestern University",
        "ByteDance Inc."
    ],
    "problem_background": "本研究的出发点是解决大型语言模型（LLMs）在多角色输入场景中角色分离（role separation）的问题。具体背景是，LLMs 经常需要在系统中处理来自不同角色的输入（如系统指令、用户查询、外部工具输出），但现有模型可能没有真正学习到角色区分，而是依赖于捷径（如任务类型关联或文本起始位置偏置），这会导致功能失效和安全风险，例如提示注入攻击（prompt injection attacks）。本工作解决了关键问题：揭示了角色分离学习中的隐藏捷径，并证明了现有微调方法可能只是记忆模式而非真正区分角色，从而为更可靠的多角色行为提供了基础。",
    "method": "本研究的核心方法是 Position-enhanced Fine-Tuning (PFT)，其核心思想是通过操纵位置 ID 来增强角色边界的不变信号。具体实现包括：在输入编码中创建系统和用户标记之间的固定间隙（例如，通过调整位置 ID，使系统标记和用户标记之间有数值间隙 d），同时保持每个角色内部的顺序信息不变，从而帮助模型更好地区分角色而不依赖于表面的捷径。其他方法包括数据增强（如交换系统和用户内容或插入非关键信息）来缓解特定捷径，但 PFT 更注重机制层面，提供更强的泛化性。主要步骤是：(1) 在微调过程中修改位置 ID，(2) 使用 LoRA 适配器优化模型，(3) 确保内部顺序不变以保留序列信息。",
    "experiment": "实验采用受控框架，使用良性训练数据（如 dataset-initial 和 dataset-symm）和对抗性评估数据（如 Gandalf Summarization、TensorTrust 攻击），以隔离模式记忆和真正角色学习。实验设置合理全面，包括：(1) 训练数据生成使用 GPT-4 创建模糊用户输入，(2) 评估包括普通数据（如 Alpaca 数据集用于效用评估）和对抗攻击（如提取和劫持攻击），(3) 比较基线模型（如标准 SFT、Delimiter-enhanced SFT）和 PFT。结果显示 PFT 在保持模型准确率（如密码任务准确率稳定）和生成质量（如 Alpaca 数据集的对数似然）的同时，显著提高了对抗攻击的鲁棒性（例如，TensorTrust 攻击准确率从 33% 提升到 62%），这与预期相符，证明了 PFT 有效缓解捷径依赖，而数据增强虽有改善但易导致迭代修补。",
    "one_sentence_summary": "本文通过提出位置 ID 操纵的 PFT 方法，揭示并解决了 LLM 在角色分离学习中依赖捷径的问题，提高了模型的鲁棒性和安全性，同时保持了性能。",
    "slug": "illusion-of-role-separation-llm-shortcuts",
    "keywords": [
        "LLM",
        "Role Separation",
        "Shortcut Learning",
        "Position ID Manipulation",
        "Finetuning"
    ],
    "further_thoughts": "本研究强调了位置编码在 LLM 中的重要性，或许可以扩展到其他领域，如多代理系统中的角色冲突或长上下文学习中，通过结合注意力机制（如注意力汇现象）进一步优化角色信号；同时，与相关工作（如 Wu et al. 的角色嵌入方法）相比，PFT 可能提供更高效的泛化，因为位置 ID 是 LLM 固有组件，而未来可探索将此与提示注入防御结合，开发更通用的安全框架，以应对新兴攻击模式。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2505.00626",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:33:01.118425+00:00",
    "score": 0.7558183688958906,
    "abstract": "Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \\emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \\emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \\emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \\emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.",
    "categories": [
        "cs.CL",
        "cs.AI"
    ],
    "created": "2025-05-01",
    "updated": "2025-05-02"
}