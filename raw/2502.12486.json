{
    "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning",
    "authors": [
        "Xiaoqian Liu",
        "Ke Wang",
        "Yongbin Li",
        "Yuchuan Wu",
        "Wentao Ma",
        "Aobo Kong",
        "Fei Huang",
        "Jianbin Jiao",
        "Junge Zhang"
    ],
    "institution": [
        "University of Chinese Academy of Sciences",
        "Tongyi Lab",
        "Institute of Automation, Chinese Academy of Sciences"
    ],
    "problem_background": "大型语言模型（LLMs）在静态问题（如数学和编码）上表现出色，但在大语言模型中处理动态、真实世界场景（如商业谈判）的战略推理能力不足，这些场景需要处理不确定性、长期目标对齐和环境适应。现有方法包括迭代提示、模仿学习（IL）或强化学习（RL）训练，以及推理路径搜索，但它们面临适应性差、泛化能力弱和计算效率低的问题，本文的工作起点是提出一种方法来提升LLMs在动态交互环境中的战略推理能力。",
    "method": "本文提出显式策略优化（EPO）方法，使用一个专门的语言模型（LLM^s）来提供实时策略，辅助另一个LLM代理（LLM^d）实现目标导向行为。具体实现包括：\n- LLM^s根据系统提示、目标G、交互历史h_{1:t-1}、先前策略a_{1:t-1}和当前观察x_t生成策略a_t：$$a_t = LLM_s(s_{sys}, G, h_{1:t-1}, a_{1:t-1}, x_t).$$\n- LLM^d基于策略生成行为y_t：$$y_t = LLM_d(d_{sys}, G, h_{1:t-1}, a_{1:t}, x_t).$$\n- 通过多轮强化学习（RL）优化LLM^s的政策，使用REINFORCE算法的目标函数：$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}[\\sum_{t=1}^{T} r_t],$$\n  损失函数为：$$\\mathcal{L}(\\theta) = - \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\frac{1}{T} \\sum_{t=1}^{T} A_t \\frac{1}{|k_t|} \\sum_{i=0}^{k_t} \\log \\pi_{\\theta}(a_{t,i} | h_{1:t-1}, a_{t,1:i-1}, x_t) \\right],$$\n  其中A_t是优势函数，r_t是过程奖励，由过程奖励模型（PRM）评估关键策略。还引入迭代自博弈来扩展RL训练，确保策略的适应性和可转移性，而不修改LLM^d。",
    "experiment": "实验在社会和物理领域进行，包括SOTOPIA（社交对话）、WebShop（网页导航）和ALFWorld（具身任务）数据集。实验设置合理全面，使用零样本或一样本提示评估，指标包括目标完成度和平均奖励。结果显示EPO在保持LLM^d泛化能力的同时，通过RL优化LLM^s显著提升性能，如在SOTOPIA上超越基线方法，平均目标完成度提高；消融实验确认RL、过程奖励和自博弈组件的关键性；分析揭示了协作推理机制和新兴策略，实验结果与预期一致，证明了EPO在长期目标对齐和战略推理方面的有效性。",
    "one_sentence_summary": "本文提出EPO方法，通过强化学习优化一个专门的战略推理模型，辅助任意LLM代理在动态环境中实现长期目标对齐，提升战略推理能力。",
    "slug": "epo-explicit-policy-optimization-strategic-reasoning",
    "keywords": [
        "LLM",
        "Strategic Reasoning",
        "Reinforcement Learning",
        "Policy Optimization",
        "Self-Play"
    ],
    "further_thoughts": "本文的EPO方法强调了战略推理模型的模块化设计，这可能扩展到多代理环境如Diplomacy游戏中，进一步提升LLMs在复杂社会互动中的表现；同时，结合更先进的奖励模型或值函数估计（如PPO算法）可能提高训练稳定性，并探索将EPO应用于真实世界AI代理，如自动谈判系统或游戏AI，以实现更强的泛化能力和人机协作潜力。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2502.12486",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:27:01.732600+00:00",
    "score": 0.6029010248350041,
    "abstract": "Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. Code and data are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.",
    "categories": [
        "cs.CL"
    ],
    "created": "2025-04-25",
    "updated": "2025-04-28"
}