{
    "title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation",
    "authors": [
        "Lucas Caccia",
        "Alan Ansell",
        "Edoardo Ponti",
        "Ivan Vulić",
        "Alessandro Sordoni"
    ],
    "institution": [
        "Microsoft Research Montreal",
        "University of Cambridge",
        "University of Edinburgh"
    ],
    "problem_background": "大型语言模型（LLM）在海量语料上预训练后，能够捕获广泛的语言和事实知识，但事后整合新信息或快速演变的信息（如私有或专业文档）在低数据场景中仍具挑战性。在上下文化习和检索增强生成（RAG）方法存在局限性，包括高推理成本、无法捕获全局文档信息，以及仅处理局部段落；此外，持续预训练使用下一 token 预测在低数据条件下效果不佳。本文的工作起点是开发一种即插即用、参数高效的方法，以在不牺牲模型泛化能力的情况下，高效编码文档知识，支持企业场景（如处理最新政策或产品细节）和科学发现（如整合前沿出版物）。",
    "method": "*   **核心思想：** 本文提出深度上下文蒸馏（Deep Context Distillation，DCD）方法，通过知识蒸馏训练知识模块（Knowledge Modules，KMs），这些模块使用 LoRA（Low-Rank Adaptation）参数高效适配器来压缩文档知识，使其能在推理时即插即用，而不需文档上下文。\n*   **实现方式：** KMs 被参数化为 LoRA 模块，优化以匹配教师模型（具有文档上下文）的隐藏状态和输出概率。损失函数包括 KL 散度损失（匹配输出概率）和 L1 范数损失（匹配隐藏状态），公式为：\n    $$\\mathcal{L}_{\\text{DCD}} = \\min_{\\theta_{\\text{KM}}} \\text{KL}\\left( p(\\mathbb{C}_{k+1}|\\mathbb{C}_{k}) \\parallel p(\\mathbb{C}_{k+1}; \\theta_{\\text{KM}}) \\right) + \\sum_{l} \\frac{1}{Z^{l}} \\left\\| h^{l}_{\\mathbb{C}_{k+1}|\\mathbb{C}_{k}} - h^{l}_{\\mathbb{C}_{k+1}; \\theta_{\\text{KM}}} \\right\\|_{1}$$\n    文档 DCD 使用文档自身 chunk，合成 DCD 使用从文档生成的合成数据（如摘要、问答对或 Entigraph）。此外，知识提取器（Knowledge Extractors，KE）可进一步训练以适应特定任务，通过可学习权重组合 KMs 和 KE。\n*   **主要步骤：** 1. 将文档拆分成 chunk；2. 使用教师模型生成合成数据；3. 通过梯度下降优化 KMs 参数以最小化 DCD 损失；4. 在推理时加载 KMs 和可选 KE 以处理查询。",
    "experiment": "*   **数据集和模型：** 实验使用 QuALITY（多选问答数据集，平均文档长度约 5,000 tokens）和 NarrativeQA（问答数据集，平均文档长度约 60,000 tokens）数据集，以及 Phi-3 3B 和 Llama-3.1 8B 指令微调模型。\n*   **实验设置：** 包括闭卷（无文档上下文）和开卷（有文档上下文）评估。闭卷评估比较 KMs 的不同训练方法（如 LM 损失、DCD 变体、PIT）；开卷评估结合 RAG 基线。所有 KMs 和 KE 使用 LoRA 适配器（秩 16），训练 1500 步，批量大小 8。结果使用 NarrativeQA 的 Rouge-L 和 QuALITY 的准确率评估。\n*   **结果分析：** 在闭卷设置中，合成 DCD（使用摘要和问答对）显著优于 LM 和 PIT 基线，例如在 NarrativeQA 上，Phi-3 的 Rouge-L 从 15.2（LM）提升到 25.8（合成 DCD + KE）。开卷设置中，RAG 与 KMs 结合时显示协同效应，RAG + KM + KE 比 RAG + KE 改善 4.2 和 4.1 Rouge-L（NarrativeQA）和 2.4% 和 4.1% 准确率（QuALITY）。消融实验证实隐藏状态匹配和更多合成数据提升性能，实验设计全面合理，结果符合预期，证明 DCD 在低数据条件下有效。",
    "one_sentence_summary": "本文提出使用深度上下文蒸馏训练可插拔知识模块的方法，能够在低数据场景下高效整合文档知识，并通过实验证明其在问答任务中优于传统方法且与 RAG 具有协同效应。",
    "slug": "plug-and-play-knowledge-modules-deep-context-distillation",
    "keywords": [
        "LLM",
        "Knowledge Module",
        "Deep Context Distillation",
        "LoRA",
        "RAG",
        "Synthetic Data",
        "Question Answering"
    ],
    "further_thoughts": "这个模块化方法强调了知识注入的灵活性，可能在隐私保护和高效推理中发挥更大作用，例如与联邦学习结合以处理分布式数据，或与知识图谱（如 GraphRAG）整合以捕获更复杂的实体关系；此外，未来可以探索高效的 KM 初始化策略或与零样本路由方法的结合，实现跨文档知识动态组合，并扩展到多模态数据或实时更新场景，以进一步提升 AI 系统在科学和企业应用中的泛化能力。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2503.08727",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:28:09.998715+00:00",
    "score": 0.6906043784729547,
    "abstract": "Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and RAG.",
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "created": "2025-04-29",
    "updated": "2025-04-30"
}