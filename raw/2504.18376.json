{
    "title": "Pushing the boundary on Natural Language Inference",
    "authors": [
        "Pablo Miralles-González",
        "Javier Huertas-Tato",
        "Alejandro Martín",
        "David Camacho"
    ],
    "institution": [
        "Technical University of Madrid"
    ],
    "problem_background": "自然语言推理（NLI）是自然语言理解的核心任务，具有事实检查、问答和信息检索等应用。尽管其重要性，但当前NLI系统主要依赖监督学习，训练数据往往包含标注偏差和人工制品，导致模型泛化能力差和实际应用受限。本文的工作起点是使用强化学习方法，特别是Group Relative Policy Optimization（GRPO）结合Chain-of-Thought（CoT）学习来处理NLI任务，消除了对标注推理路径的依赖，从而能够在更具挑战性的数据集如ANLI上进行训练，解决了现有方法在数据质量和泛化方面的关键问题。",
    "method": "本文的方法核心是应用Group Relative Policy Optimization（GRPO）算法进行NLI任务的Chain-of-Thought学习。具体实现包括：将NLI表述为文本到文本问题，使用特定提示模板（如SYSTEM和USER指令，强制模型生成推理步骤后给出预测）；GRPO优化目标为$$\\mathcal{L}_{\\text{GRPO}}(\\theta) = \\frac{1}{G} \\sum_{i=1}^{G} \\left[ \\min\\left(\\frac{\\pi_{\\theta}(o_{i}|p)}{\\pi_{\\text{old}}(o_{i}|p)} A_{i}, \\text{clip}\\left(\\frac{\\pi_{\\theta}(o_{i}|p)}{\\pi_{\\text{old}}(o_{i}|p)}, 1 - \\epsilon, 1 + \\epsilon\\right) A_{i}\\right) \\right] - \\beta \\, \\text{KL}(\\pi_{\\theta} \\| \\pi_{\\text{ref}})$$，其中$A_i$是归一化优势函数，$\\epsilon$控制剪切范围，$\\beta$是KL散度正则化系数，以防止模型偏离基线分布；无需人工标注推理路径，通过在线采样和奖励函数（如准确性奖励）实现训练；采用参数高效微调技术LoRA和QLoRA，在7B、14B和32B模型上应用。",
    "experiment": "实验使用Qwen2.5系列模型（7B、14B、32B），采用AWQ量化（4位）和LoRA/QLoRA微调，数据集包括标准基准（如SNLI、MultiNLI）和对抗性基准（如ANLI、Counter NLI、HANS、NLI Diagnostic）。实验设置全面，涵盖模型大小、LoRA秩（8到128）、量化影响和训练动态分析。结果显示GRPO训练显著提升了模型在对抗性数据集上的性能，例如32B AWQ量化模型在11个对抗性子集中的7个上达到最先进水平，内存占用仅22GB；平均准确率提升明显（如ANLI R3从53.58%到71.75%），量化后性能损失小（平均下降2.95%），LoRA秩增加时性能趋于稳定；实验结果符合预期，证明了方法的鲁棒性和高效性，同时通过消融实验和输出分析验证了泛化能力。",
    "one_sentence_summary": "本文提出使用Group Relative Policy Optimization结合Chain-of-Thought学习的方法提升自然语言推理任务的性能，无需标注推理路径，通过参数高效微调在对抗性基准上实现最先进结果。",
    "slug": "pushing-boundary-natural-language-inference",
    "keywords": [
        "Natural Language Inference",
        "LLMs",
        "GRPO",
        "Chain-of-Thought",
        "Reinforcement Learning",
        "LoRA",
        "Adversarial Benchmarks"
    ],
    "further_thoughts": "本文的GRPO方法在NLI任务中展示了强化学习在提升推理能力方面的潜力，值得扩展到其他领域如数学推理或代码生成中，与DeepSeek-R1类似；基模型质量的重要性突出，提示使用更大规模预训练模型来避免监督学习的偏差；未来可探索与其他数据集（如WANLI或合成数据）的结合，或优化KL散度权重以平衡探索与稳定性；此外，量化技术的应用（如AWQ）提供高效部署思路，但需关注潜在信息损失对复杂推理的影响，与Kavumba et al.的工作相比，GRPO的无监督优势可能在多模态任务中更具泛化性。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.18376",
    "preference": "unknown",
    "summary_time": "2025-05-04T08:30:46.264097+00:00",
    "score": 0.5651388284809683,
    "abstract": "Natural Language Inference (NLI) is a central task in natural language understanding with applications in fact-checking, question answering, and information retrieval. Despite its importance, current NLI systems heavily rely on supervised learning with datasets that often contain annotation artifacts and biases, limiting generalization and real-world applicability. In this work, we apply a reinforcement learning-based approach using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the need for labeled rationales and enabling this type of training on more challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language models using parameter-efficient techniques (LoRA and QLoRA), demonstrating strong performance across standard and adversarial NLI benchmarks. Our 32B AWQ-quantized model surpasses state-of-the-art results on 7 out of 11 adversarial sets$\\unicode{x2013}$or on all of them considering our replication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust reasoning can be retained under aggressive quantization. This work provides a scalable and practical framework for building robust NLI systems without sacrificing inference quality.",
    "categories": [
        "cs.CL",
        "cs.AI"
    ],
    "created": "2025-04-25",
    "updated": "2025-04-28"
}