{
    "title": "TTRL: Test-Time Reinforcement Learning",
    "authors": [
        "Yuxin Zuo",
        "Kaiyan Zhang",
        "Shang Qu",
        "Li Sheng",
        "Xuekai Zhu",
        "Biqing Qi",
        "Youbang Sun",
        "Ganqu Cui",
        "Ning Ding",
        "Bowen Zhou"
    ],
    "institution": [
        "Tsinghua University",
        "Shanghai AI Lab"
    ],
    "problem_background": "本工作的出发点是解决大型语言模型（LLMs）在推理任务上进行强化学习（RL）时面临的挑战，即在测试时没有显式标签，无法获得准确的奖励信号。背景包括测试时缩放（TTS）技术的发展，表明测试时增加计算资源比预训练时更高效，但传统RL方法主要依赖训练时的数据，无法有效处理新出现的无标签测试数据。关键问题包括：如何在无标签数据上估计奖励，以及如何实现模型的自演化以适应分布偏移和新型任务，例如OpenAI o3在ARC-AGI-2上的低性能。TTRL旨在利用预训练模型的先验，通过测试时训练来提升模型性能，减少对人类标注的依赖。",
    "method": "核心思想是通过测试时强化学习（TTRL）在无标签数据上训练LLMs，实现模型的自演化。具体实现包括：给定输入提示x，模型通过策略πθ生成输出y；通过重复采样生成多个候选输出{y1, y2, ..., yN}，使用多数投票估计共识输出y*；基于y*计算奖励r(y, y*)，奖励函数为二值的规则-based奖励（如果输出匹配y*则奖励1，否则0）。RL目标是最大化期望奖励：\n$$\n\\max_{\\theta} \\mathbb{E}_{y \\sim \\pi_{\\theta}(\\cdot|x)} [r(y, y^*)],\n$$\n通过梯度上升更新参数：\n$$\n\\theta \\leftarrow \\theta + \\eta \\nabla_{\\theta} \\mathbb{E}_{y \\sim \\pi_{\\theta}(\\cdot|\\mathbf{x})} [r(y, y^*)].\n$$\n主要步骤是采样、投票估计标签、计算奖励和参数更新，不修改预训练模型，只在测试时调整。",
    "experiment": "实验设置包括使用Qwen2.5-Math-1.5B、7B和LLaMA-3.1-8B-Instruct等模型，基准数据集为AIME 2024、AMC和MATH-500，基线包括DeepSeek-R1-Distill系列等RL模型。实验使用GRPO算法，学习率为5×10^{-7}，采样64个响应等超参数。结果显示TTRL显著提升性能，例如在AIME 2024上Qwen2.5-Math-7B的pass@1性能从13.3提升至43.3，增幅159%；平均在三个基准上提升84%。实验设计合理，覆盖不同模型规模、任务难度和RL算法（如PPO），验证了泛化性和兼容性，结果符合预期，因为TTRL利用多数投票奖励有效估计标签，即使在无标签设置下也能接近有标签训练的上限。",
    "one_sentence_summary": "本文提出测试时强化学习（TTRL）方法，通过多数投票估计奖励，在无标签测试数据上训练大语言模型，实现模型自演化并显著提升推理任务性能。",
    "slug": "ttrl-test-time-reinforcement-learning",
    "keywords": [
        "Test-Time Reinforcement Learning",
        "Majority Voting",
        "Reinforcement Learning",
        "Large Language Models",
        "Reasoning",
        "Unlabeled Data"
    ],
    "further_thoughts": "TTRL的创新在于利用自估计奖励实现无监督RL，这启发我们思考在其他领域如计算机视觉的测试时适应（Test-Time Adaptation）中应用类似机制，减少对标注数据的依赖；此外，TTRL的self-evolution概念可能与在线学习和终身学习相结合，推动AI代理在动态环境中持续改进；与相关工作如DeepSeek-R1的RL方法相比，TTRL强调测试时训练的效率，未来可探索与其他优化算法的整合，以提升鲁棒性和泛化能力。",
    "model": "grok-3-mini-latest",
    "temperature": 0.5,
    "top_p": 0.7,
    "lang": "zh",
    "id": "2504.16084",
    "preference": "like",
    "summary_time": "2025-05-04T08:30:27.467881+00:00",
    "score": 0.9349312337549779,
    "abstract": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
    "categories": [
        "cs.CL",
        "cs.LG"
    ],
    "created": "2025-04-22",
    "updated": "2025-04-23"
}