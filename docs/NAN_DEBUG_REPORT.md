# NaN é—®é¢˜è¯Šæ–­æŠ¥å‘Š

## é—®é¢˜å®šä½

æ ¹æ®ä»£ç åˆ†æ,`fit_predict.py` ä¸­çš„ NaN æ£€æµ‹åœ¨ä»¥ä¸‹ä½ç½®è§¦å‘:

### 1. **Line 396** (ä¸»è¦æ£€æµ‹ç‚¹)
```python
x = np.hstack([np.vstack(combined_df[col].to_numpy()) for col in embedding_columns])
samples_with_nan = np.isnan(x).any(axis=1).sum()
logger.warning(f"{samples_with_nan}ä¸ªæ ·æœ¬ ({samples_with_nan/x.shape[0]*100:.2f}%) åŒ…å«NaNå€¼")
```

### 2. **Line 233-239** (adaptive_difficulty_sampling)
```python
if np.isnan(x_pos).any():
    logger.warning(f"æ­£æ ·æœ¬æ•°æ®ä¸­å‘ç° NaN å€¼ï¼Œå°†æ›¿æ¢ä¸º 0")
if np.isnan(unlabeled_data).any():
    logger.warning(f"èƒŒæ™¯æ•°æ®ä¸­å‘ç° NaN å€¼ï¼Œå°†æ›¿æ¢ä¸º 0")
```

### 3. **Line 514** (é¢„æµ‹é˜¶æ®µ)
```python
nan_count = np.isnan(X_target).sum()
if nan_count > 0:
    logger.warning(f"é¢„æµ‹æ•°æ®ä¸­å‘ç° {nan_count} ä¸ª NaN å€¼ï¼Œå°†æ›¿æ¢ä¸º 0")
```

## æ ¹æœ¬åŸå› åˆ†æ

ä½ åŒäº‹è¯´ **`lyk/ArxivEmbedding` æ•°æ®é›†æ²¡æœ‰ NaN** æ˜¯æ­£ç¡®çš„,ä½† NaN å¯èƒ½åœ¨ä»¥ä¸‹ç¯èŠ‚äº§ç”Ÿ:

### å¯èƒ½æ€§ 1: None å€¼è½¬æ¢ (æœ€å¯èƒ½)
```python
# å½“æ•°æ®ä¸­æœ‰ None æ—¶:
col_data = [vec1, vec2, None, vec3, ...]  # æŸäº›è®ºæ–‡çš„ embedding ä¸º None
arr = np.vstack(col_data)  # None ä¼šè¢«è½¬æ¢ä¸º NaN
```

### å¯èƒ½æ€§ 2: Join æ“ä½œäº§ç”Ÿç¼ºå¤±
```python
prefered_df = prefered_df.join(preferences, on="id", how="inner")
# å¦‚æœ join åæŸäº›å­—æ®µç¼ºå¤±,å†ç”¨äº vstack æ—¶ä¼šäº§ç”Ÿ NaN
```

### å¯èƒ½æ€§ 3: æ•°æ®ç±»å‹ä¸åŒ¹é…
```python
# å¦‚æœæŸäº› embedding çš„æ•°æ®ç±»å‹ä¸ä¸€è‡´ (float32 vs float64, list vs array)
# vstack æ“ä½œå¯èƒ½å¼•å…¥ NaN
```

### å¯èƒ½æ€§ 4: è¿‡æ»¤åçš„ç©ºå€¼
```python
# æŸäº›è¿‡æ»¤æ¡ä»¶å¯¼è‡´æŸäº›è¡Œçš„ embedding åˆ—ä¸ºç©º
remaining_df = remaining_df.filter(...)
# è¿‡æ»¤åå¯èƒ½äº§ç”Ÿç©º embedding
```

## è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: å¢å¼ºè¯Šæ–­ (æ¨èç«‹å³æ‰§è¡Œ)

åœ¨ `fit_predict.py` ä¸­æ·»åŠ è¯¦ç»†çš„ NaN æ¥æºè¿½è¸ª:

```python
# åœ¨ train_model å‡½æ•° Line 390 ä¹‹åæ·»åŠ :

# ğŸ” è¯Šæ–­: æ£€æŸ¥åŸå§‹æ•°æ®
logger.info(f"[NaNè¯Šæ–­] combined_df shape: {combined_df.shape}")
for col in embedding_columns:
    col_data = combined_df[col].to_list()
    none_count = sum(1 for x in col_data if x is None)
    if none_count > 0:
        logger.warning(f"[NaNè¯Šæ–­] åˆ— '{col}' æœ‰ {none_count} ä¸ª None å€¼")
        none_ids = [combined_df['id'][i] for i, x in enumerate(col_data) if x is None][:5]
        logger.warning(f"[NaNè¯Šæ–­] None å€¼çš„è®ºæ–‡IDç¤ºä¾‹: {none_ids}")

# ğŸ” è¯Šæ–­: æ£€æŸ¥ vstack æ¯ä¸€æ­¥
arrays = []
for col in embedding_columns:
    col_arr = np.vstack(combined_df[col].to_numpy())
    nan_before = np.isnan(col_arr).sum()
    logger.info(f"[NaNè¯Šæ–­] åˆ— '{col}' vstackå: shape={col_arr.shape}, NaN={nan_before}")
    arrays.append(col_arr)

x = np.hstack(arrays)
nan_after = np.isnan(x).sum()
logger.info(f"[NaNè¯Šæ–­] hstackå: shape={x.shape}, NaN={nan_after}")
```

### æ–¹æ¡ˆ 2: æ•°æ®æ¸…æ´— (æ ¹æ²»æ–¹æ¡ˆ)

å¦‚æœç¡®è®¤æ˜¯ None å€¼å¯¼è‡´çš„,åœ¨æ•°æ®åŠ è½½åç«‹å³æ¸…æ´—:

```python
# åœ¨ load_lazy_dataset å‡½æ•°è¿”å›å‰æ·»åŠ :

def clean_none_embeddings(df: pl.DataFrame, embedding_columns: list) -> pl.DataFrame:
    """æ¸…é™¤ embedding ä¸º None çš„è¡Œ"""
    original_size = df.height
    
    # åˆ›å»ºè¿‡æ»¤æ¡ä»¶: æ‰€æœ‰ embedding åˆ—éƒ½ä¸ä¸º None
    condition = pl.col(embedding_columns[0]).is_not_null()
    for col in embedding_columns[1:]:
        condition = condition & pl.col(col).is_not_null()
    
    df = df.filter(condition)
    
    removed = original_size - df.height
    if removed > 0:
        logger.warning(f"æ¸…é™¤äº† {removed} ä¸ª embedding ä¸º None çš„æ ·æœ¬")
    
    return df

# ä½¿ç”¨:
prefered_df = clean_none_embeddings(prefered_df, embedding_columns)
remaining_df = clean_none_embeddings(remaining_df, embedding_columns)
```

### æ–¹æ¡ˆ 3: æ›¿æ¢ç­–ç•¥ä¼˜åŒ–

å½“å‰ä»£ç ä½¿ç”¨ `np.nan_to_num(x, nan=0.0)` ç®€å•æ›¿æ¢ä¸º 0,ä½†è¿™å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½ã€‚
æ›´å¥½çš„æ–¹æ¡ˆ:

```python
# æ–¹æ¡ˆ A: ç§»é™¤å« NaN çš„æ ·æœ¬
nan_mask = np.isnan(x).any(axis=1)
x = x[~nan_mask]
y = y[~nan_mask]
logger.info(f"ç§»é™¤äº† {nan_mask.sum()} ä¸ªå« NaN çš„æ ·æœ¬")

# æ–¹æ¡ˆ B: ç”¨è¯¥åˆ—çš„å‡å€¼æ›¿æ¢
for i, col in enumerate(embedding_columns):
    col_start = i * embedding_dim
    col_end = (i + 1) * embedding_dim
    col_data = x[:, col_start:col_end]
    col_mean = np.nanmean(col_data, axis=0)
    nan_mask = np.isnan(col_data)
    col_data[nan_mask] = np.take(col_mean, np.where(nan_mask)[1])
    x[:, col_start:col_end] = col_data
```

## ä¸åŒäº‹åé¦ˆçš„åè°ƒ

ä½ åŒäº‹è¯´çš„ "ArxivEmbedding ä¸Šå·²ç»æ²¡æœ‰ NaN å’Œå…¨ 0 çš„ vector" å¯èƒ½æŒ‡:
1. **Parquet æ–‡ä»¶æœ¬èº«å­˜å‚¨æ—¶æ²¡æœ‰ NaN** - è¿™æ˜¯å¯¹çš„
2. ä½†æ•°æ®æœ‰ **None å€¼** (Python çš„ None â‰  NumPy çš„ NaN)
3. None åœ¨ `np.vstack()` è½¬æ¢æ—¶å˜æˆäº† NaN

## ç«‹å³è¡ŒåŠ¨

è¯·ä½ åŒäº‹æ‰§è¡Œä»¥ä¸‹ SQL æŸ¥è¯¢ (å¦‚æœæ•°æ®åœ¨æ•°æ®åº“) æˆ– Python æ£€æŸ¥:

```python
import polars as pl

# æ£€æŸ¥ None å€¼
for year in [2023, 2024, 2025]:
    df = pl.scan_parquet(f"hf://datasets/lyk/ArxivEmbedding/{year}.parquet")
    
    for col in ["jasper_v1", "conan_v1"]:
        null_count = df.filter(pl.col(col).is_null()).count().collect()[0, 0]
        print(f"{year}.parquet - {col}: {null_count} null values")
```

å¦‚æœæœ‰ null å€¼,é‚£å°±æ˜¯é—®é¢˜æ‰€åœ¨!

## å»ºè®®

1. **ç«‹å³**: åœ¨ `fit_predict.py` Line 390 åæ·»åŠ è¯Šæ–­ä»£ç  (æ–¹æ¡ˆ1)
2. **çŸ­æœŸ**: è¿è¡Œä¸€æ¬¡æŸ¥çœ‹æ—¥å¿—,ç¡®è®¤æ˜¯ None è¿˜æ˜¯å…¶ä»–åŸå› 
3. **é•¿æœŸ**: 
   - å¦‚æœæ˜¯ None å€¼: ä½¿ç”¨æ–¹æ¡ˆ2åœ¨æ•°æ®åŠ è½½æ—¶æ¸…æ´—
   - å¦‚æœæ˜¯å…¶ä»–åŸå› : æ ¹æ®è¯Šæ–­æ—¥å¿—è¿›ä¸€æ­¥åˆ†æ
   - è”ç³»åŒäº‹ç¡®è®¤æ•°æ®é›†ä¸­æ˜¯å¦æœ‰ null å€¼(ä¸åªæ˜¯ NaN)

## æµ‹è¯•æ–¹æ³•

ä¿®æ”¹åè¿è¡Œ:
```bash
python script/fit_predict.py
```

æŸ¥çœ‹æ—¥å¿—ä¸­çš„ `[NaNè¯Šæ–­]` æ ‡è®°,å°±èƒ½ç²¾ç¡®çŸ¥é“:
1. å“ªäº›è®ºæ–‡çš„ embedding æ˜¯ None/NaN
2. æ˜¯åœ¨å“ªä¸ªæ­¥éª¤äº§ç”Ÿçš„
3. å½±å“äº†å¤šå°‘æ ·æœ¬
